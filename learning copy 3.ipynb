{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de57485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Standard Libraries\n",
    "# =============================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# progress / kaggle\n",
    "from tqdm.auto import tqdm\n",
    "import kagglehub\n",
    "from IPython.display import display\n",
    "\n",
    "# =============================\n",
    "# Data Science Libraries\n",
    "# =============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "# =============================\n",
    "# Visualization\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn\n",
    "# =============================\n",
    "from sklearn import clone\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    ParameterGrid,\n",
    "    ParameterSampler,\n",
    "    KFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "import sklearn.model_selection._search as sk_search\n",
    "import sklearn.model_selection._validation as sk_validation\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    MultiLabelBinarizer,\n",
    "    Normalizer,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    get_scorer,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SequentialFeatureSelector,\n",
    "    f_regression,\n",
    "    SelectKBest,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    "    RidgeClassifier,\n",
    "    LogisticRegression,\n",
    "    RidgeCV,\n",
    "    LassoCV,\n",
    "    ElasticNetCV,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# extra joblib tools\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline as SKPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif, VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, mean_absolute_error\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Global Settings\n",
    "# =============================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "random_state = 42\n",
    "N_ROWS = 10_000\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")  # no scientific notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1cdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_eda(df, name):\n",
    "    # simple settings\n",
    "    top_k_categories = 20\n",
    "    max_corr_cols = 30\n",
    "    max_rows_to_show = 25\n",
    "\n",
    "    # make printing wide and avoid scientific notation\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\", max_rows_to_show,\n",
    "        \"display.max_columns\", None,\n",
    "        \"display.width\", 1000,\n",
    "        \"display.max_colwidth\", 200,\n",
    "        \"display.float_format\", lambda x: f\"{x:.6f}\"\n",
    "    ):\n",
    "        report_lines = []\n",
    "\n",
    "        # title\n",
    "        report_lines.append(f\"=== Robust EDA Report: {name} ===\")\n",
    "\n",
    "        # shapes and memory\n",
    "        info_df = pd.DataFrame(\n",
    "            {\n",
    "                \"rows\": [df.shape[0]],\n",
    "                \"columns\": [df.shape[1]],\n",
    "                \"memory_bytes\": [int(df.memory_usage(deep=True).sum())],\n",
    "            }\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Info ===\")\n",
    "        report_lines.append(info_df.to_string(index=False))\n",
    "\n",
    "        # dtypes\n",
    "        dtypes_df = (\n",
    "            df.dtypes.rename(\"dtype\")\n",
    "            .astype(str)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"column\"})\n",
    "            .sort_values(\"column\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Dtypes ===\")\n",
    "        report_lines.append(dtypes_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # missing values\n",
    "        total_rows = len(df)\n",
    "        missing_counts = df.isna().sum()\n",
    "        if total_rows > 0:\n",
    "            missing_percent = (missing_counts / total_rows * 100).round(2)\n",
    "        else:\n",
    "            missing_percent = pd.Series([0] * len(df.columns), index=df.columns)\n",
    "        missing_df = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": df.columns,\n",
    "                    \"missing_count\": missing_counts.values,\n",
    "                    \"missing_percent\": missing_percent.values,\n",
    "                }\n",
    "            )\n",
    "            .sort_values([\"missing_count\", \"missing_percent\"], ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Missing Values ===\")\n",
    "        report_lines.append(missing_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # duplicates (safe fallback for unhashable types)\n",
    "        try:\n",
    "            duplicate_count = int(df.duplicated().sum())\n",
    "            duplicate_index = df.index[df.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "        except TypeError:\n",
    "            df_hashable = df.astype(str)\n",
    "            duplicate_count = int(df_hashable.duplicated().sum())\n",
    "            duplicate_index = df_hashable.index[df_hashable.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "\n",
    "        duplicates_summary_df = pd.DataFrame({\"duplicate_rows\": [duplicate_count]})\n",
    "        report_lines.append(\"\\n=== Duplicates Summary ===\")\n",
    "        report_lines.append(duplicates_summary_df.to_string(index=False))\n",
    "        report_lines.append(\"\\n=== Duplicates Preview (up to 20 rows) ===\")\n",
    "        if len(duplicates_preview_df) > 0:\n",
    "            report_lines.append(duplicates_preview_df.to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"(No duplicate rows found.)\")\n",
    "\n",
    "        # column groups\n",
    "        numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        categorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "        # numeric summary\n",
    "        if len(numeric_columns) > 0:\n",
    "            percentiles = [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "            numeric_summary_df = (\n",
    "                df[numeric_columns]\n",
    "                .describe(percentiles=percentiles)\n",
    "                .T.reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Numeric Summary (5%..95%) ===\")\n",
    "            report_lines.append(numeric_summary_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # skew and kurtosis\n",
    "            skew_kurt_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": numeric_columns,\n",
    "                    \"skew\": df[numeric_columns].skew(numeric_only=True).values,\n",
    "                    \"kurtosis\": df[numeric_columns].kurtosis(numeric_only=True).values,\n",
    "                }\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Skew and Kurtosis ===\")\n",
    "            report_lines.append(skew_kurt_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # IQR outliers per column\n",
    "            q1 = df[numeric_columns].quantile(0.25)\n",
    "            q3 = df[numeric_columns].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outlier_mask = (df[numeric_columns] < (q1 - 1.5 * iqr)) | (df[numeric_columns] > (q3 + 1.5 * iqr))\n",
    "            iqr_outliers_df = (\n",
    "                outlier_mask.sum()\n",
    "                .rename(\"outlier_count\")\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== IQR Outlier Counts ===\")\n",
    "            report_lines.append(iqr_outliers_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # correlation on first N numeric columns\n",
    "            if len(numeric_columns) > 1:\n",
    "                selected_cols = numeric_columns[:max_corr_cols]\n",
    "                correlation_df = df[selected_cols].corr(method=\"pearson\", numeric_only=True)\n",
    "                correlation_df.index.name = \"column\"\n",
    "                report_lines.append(f\"\\n=== Correlation (first {max_corr_cols} numeric columns) ===\")\n",
    "                report_lines.append(correlation_df.to_string())\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No numeric columns found.)\")\n",
    "\n",
    "        # categorical value counts (top K each)\n",
    "        if len(categorical_columns) > 0:\n",
    "            cat_rows = []\n",
    "            for col in categorical_columns:\n",
    "                try:\n",
    "                    vc = df[col].value_counts(dropna=False).head(top_k_categories)\n",
    "                except TypeError:\n",
    "                    vc = df[col].astype(str).value_counts(dropna=False).head(top_k_categories)\n",
    "                for value, count in vc.items():\n",
    "                    percent = (count / total_rows * 100) if total_rows > 0 else 0\n",
    "                    cat_rows.append(\n",
    "                        {\"column\": col, \"value\": value, \"count\": int(count), \"percent\": round(percent, 2)}\n",
    "                    )\n",
    "            categorical_values_df = pd.DataFrame(cat_rows)\n",
    "            report_lines.append(f\"\\n=== Categorical Values (Top {top_k_categories} per column) ===\")\n",
    "            report_lines.append(categorical_values_df.head(max_rows_to_show).to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No categorical columns found.)\")\n",
    "\n",
    "        # unique counts per column\n",
    "        def _safe_nunique(series):\n",
    "            try:\n",
    "                return int(series.nunique(dropna=False))\n",
    "            except TypeError:\n",
    "                return np.nan\n",
    "\n",
    "        unique_counts_df = pd.DataFrame(\n",
    "            {\"column\": df.columns, \"unique_values\": [_safe_nunique(df[c]) for c in df.columns]}\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Unique Counts Per Column ===\")\n",
    "        report_lines.append(unique_counts_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # sample head\n",
    "        report_lines.append(\"\\n=== Head (10 rows) ===\")\n",
    "        report_lines.append(df.head(10).to_string(index=False))\n",
    "\n",
    "        # end\n",
    "        report_lines.append(\"\\n=== End of EDA Report ===\")\n",
    "\n",
    "        # one giant print\n",
    "        print(\"\\n\".join(report_lines))\n",
    "\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "        return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bd5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb965d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepTrainColumns(BaseEstimator, TransformerMixin):\n",
    "    # remembers training columns and reindexes any input to match\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.keep_columns_ = list(X.columns)\n",
    "        else:\n",
    "            self.keep_columns_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.keep_columns_ is None:\n",
    "            return X\n",
    "        if hasattr(X, \"reindex\"):\n",
    "            return X.reindex(columns=self.keep_columns_, fill_value=0)\n",
    "        return X\n",
    "\n",
    "\n",
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    # turn scores into 0/1 using a chosen threshold\n",
    "    import numpy as np\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        raw = model.decision_function(X)\n",
    "        raw_min, raw_max = float(raw.min()), float(raw.max())\n",
    "        scores = (raw - raw_min) / (raw_max - raw_min + 1e-9)\n",
    "    else:\n",
    "        scores = model.predict(X).astype(float)\n",
    "    return (scores >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model builder + tuner\n",
    "# =============================\n",
    "def build_and_tune_models(\n",
    "    X_train, y_train,\n",
    "    task_type,\n",
    "    num_folds,\n",
    "    num_iterations,\n",
    "    oversample=False,\n",
    "    oversample_method=\"random\"\n",
    "):\n",
    "\n",
    "    # optional oversampling tools\n",
    "    ImbPipeline = None\n",
    "    RandomOverSampler = None\n",
    "    SMOTE = None\n",
    "    if oversample and str(task_type).strip().lower() == \"classification\":\n",
    "        try:\n",
    "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "        except Exception:\n",
    "            print(\"imblearn not available. Oversampling disabled.\")\n",
    "            oversample = False\n",
    "\n",
    "    # task settings\n",
    "    task = str(task_type).strip().lower()\n",
    "    if task == \"classification\":\n",
    "        scoring = \"f1_macro\"\n",
    "        selector_score_func = f_classif\n",
    "        min_class = int(y_train.value_counts().min())\n",
    "        eff_folds = max(2, min(int(num_folds), min_class))\n",
    "        baseline_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        sampler_obj = None\n",
    "        if oversample:\n",
    "            if oversample_method == \"smote\":\n",
    "                k_neighbors_for_smote = max(1, min(5, min_class - 1))\n",
    "                if k_neighbors_for_smote < 1:\n",
    "                    print(\"SMOTE not possible (minority class too small). Using RandomOverSampler.\")\n",
    "                    sampler_obj = RandomOverSampler(random_state=42)\n",
    "                else:\n",
    "                    sampler_obj = SMOTE(random_state=42, k_neighbors=k_neighbors_for_smote)\n",
    "            else:\n",
    "                sampler_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "        class_weight_choice = None if oversample else \"balanced\"\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingClassifier(random_state=42),\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=42, class_weight=class_weight_choice, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeClassifier(random_state=42, class_weight=class_weight_choice),\n",
    "            \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"LinearSVM\": LinearSVC(max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"NaiveBayes\": GaussianNB(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"Dummy\": DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "        }\n",
    "        metric_name = \"F1_macro\"\n",
    "    elif task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "        selector_score_func = f_regression\n",
    "        eff_folds = max(2, int(num_folds))\n",
    "        baseline_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingRegressor(random_state=42),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(max_iter=5000),\n",
    "            \"Lasso\": Lasso(max_iter=5000),\n",
    "            \"ElasticNet\": ElasticNet(max_iter=5000),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"Dummy\": DummyRegressor(strategy=\"mean\"),\n",
    "        }\n",
    "        metric_name = \"CV_MAE\"\n",
    "        sampler_obj = None\n",
    "    else:\n",
    "        raise ValueError('task_type must be \"classification\" or \"regression\"')\n",
    "\n",
    "    total_features = X_train.shape[1]\n",
    "    feature_fractions = [0.10, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "    # which models need scaling and selection\n",
    "    needs_scaling = {\"LogisticRegression\", \"LinearSVM\", \"KNN\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"NaiveBayes\"}\n",
    "    skip_selection = {\"Dummy\"}\n",
    "    tree_like = {\"RandomForest\", \"DecisionTree\", \"GBT\"}\n",
    "\n",
    "    def k_from_fraction(frac, total_cols):\n",
    "        if frac >= 1.0:\n",
    "            return \"all\"\n",
    "        k = int(max(1, math.ceil(frac * total_cols)))\n",
    "        return min(k, total_cols)\n",
    "\n",
    "    # dynamic KNN neighbors cap\n",
    "    per_fold_train = int(len(X_train) * (eff_folds - 1) / eff_folds)\n",
    "    max_knn_k = max(3, min(101, per_fold_train - 1))\n",
    "    knn_ks = list(range(3, max_knn_k + 1, 2))\n",
    "\n",
    "    def logspace_list(low_exp, high_exp, num):\n",
    "        return list(np.logspace(low_exp, high_exp, num))\n",
    "\n",
    "    def linspace_list(low_val, high_val, num):\n",
    "        return list(np.linspace(low_val, high_val, num))\n",
    "\n",
    "    param_spaces_classification = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LogisticRegression\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__solver\": [\"saga\"],\n",
    "        },\n",
    "        \"LinearSVM\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        },\n",
    "        \"NaiveBayes\": {\n",
    "            \"model__var_smoothing\": list(10 ** np.linspace(-11, -7, 9))\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"most_frequent\", \"stratified\", \"uniform\"]},\n",
    "    }\n",
    "\n",
    "    param_spaces_regression = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LinearRegression\": {},\n",
    "        \"Ridge\": {\"model__alpha\": logspace_list(-3, 3, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"Lasso\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"ElasticNet\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__l1_ratio\": linspace_list(0.1, 0.9, 9), \"model__fit_intercept\": [True, False]},\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"mean\", \"median\"]},\n",
    "    }\n",
    "\n",
    "    # build a pipeline for a given k\n",
    "    def make_pipeline_for_k(model_name, model_obj, k_value):\n",
    "        # order: align -> impute -> variance -> select -> scale -> sampler -> model\n",
    "        align_step = (\"align\", KeepTrainColumns())\n",
    "        impute_step = (\"impute\", SimpleImputer(strategy=\"median\"))\n",
    "        variance_step = (\"variance\", VarianceThreshold(threshold=0.0))\n",
    "\n",
    "        if model_name in skip_selection or model_name in tree_like:\n",
    "            select_step = (\"select\", \"passthrough\")\n",
    "        else:\n",
    "            select_step = (\"select\", SelectKBest(score_func=selector_score_func, k=k_value))\n",
    "\n",
    "        scale_step = (\"scale\", StandardScaler() if model_name in needs_scaling else \"passthrough\")\n",
    "\n",
    "        steps = [align_step, impute_step, variance_step, select_step, scale_step]\n",
    "\n",
    "        if task == \"classification\" and oversample and sampler_obj is not None:\n",
    "            steps.append((\"sampler\", sampler_obj))\n",
    "\n",
    "        steps.append((\"model\", model_obj))\n",
    "\n",
    "        if oversample and ImbPipeline is not None and task == \"classification\":\n",
    "            return ImbPipeline(steps)\n",
    "        else:\n",
    "            return SKPipeline(steps)\n",
    "\n",
    "    # baseline sweep across models × k\n",
    "    rows = []\n",
    "    total_steps = len(model_space) * len(feature_fractions)\n",
    "    step = 0\n",
    "    print(\"Streaming results (each line is one model × feature count):\")\n",
    "    for model_name, model_obj in model_space.items():\n",
    "        for frac in feature_fractions:\n",
    "            step += 1\n",
    "            k_val = k_from_fraction(frac, total_features)\n",
    "            k_print = total_features if k_val == \"all\" else int(k_val)\n",
    "            pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=baseline_cv, scoring=scoring, n_jobs=1)\n",
    "            mean_score = float(np.mean(scores))\n",
    "            std_score = float(np.std(scores))\n",
    "            if task == \"regression\":\n",
    "                display_mean = -mean_score\n",
    "                display_std = float(np.std(-scores))\n",
    "            else:\n",
    "                display_mean = mean_score\n",
    "                display_std = std_score\n",
    "            rows.append({\"Model\": model_name, \"K_features\": k_print, \"MeanScore\": display_mean, \"StdDev\": display_std, \"Metric\": metric_name})\n",
    "            print(f\"[{step}/{total_steps}] {model_name} | k={k_print} | {metric_name}={display_mean:.6f} ± {display_std:.6f}\", flush=True)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    if task == \"classification\":\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    else:\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Baseline results (CV) ===\")\n",
    "    print(results_df[[\"Model\", \"K_features\", \"MeanScore\", \"StdDev\", \"Metric\"]])\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    best_model_name = str(best_row[\"Model\"])\n",
    "    best_k = int(best_row[\"K_features\"])\n",
    "    best_model_obj = model_space[best_model_name]\n",
    "    k_val_for_search = \"all\" if best_k >= total_features else best_k\n",
    "    best_pipeline = make_pipeline_for_k(best_model_name, best_model_obj, k_val_for_search)\n",
    "\n",
    "    # pick search space\n",
    "    search_space = (param_spaces_classification if task == \"classification\" else param_spaces_regression).get(best_model_name, {})\n",
    "    if len(search_space) == 0:\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        try:\n",
    "            best_pipeline.input_columns_ = list(X_train.columns)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # threshold tuning only for classification\n",
    "        if task == \"classification\":\n",
    "            try:\n",
    "                _tune_threshold_inplace(best_pipeline, X_train, y_train, search_cv)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] threshold tuning failed: {e}\")\n",
    "        print(\"\\nBest model had no tunable params. Returning fitted pipeline.\")\n",
    "        return best_pipeline\n",
    "\n",
    "    # hyperparameter search\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=best_pipeline,\n",
    "        param_distributions=search_space,\n",
    "        n_iter=int(max(1, num_iterations)),\n",
    "        scoring=scoring,\n",
    "        cv=search_cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # print tuned CV result\n",
    "    if task == \"regression\":\n",
    "        tuned_score_display = -float(search.best_score_)\n",
    "        tuned_metric_name = \"CV MAE\"\n",
    "    else:\n",
    "        tuned_score_display = float(search.best_score_)\n",
    "        tuned_metric_name = \"F1 macro\"\n",
    "\n",
    "    print(\"\\n=== Best model after randomized search ===\")\n",
    "    print(f\"Model name: {best_model_name}\")\n",
    "    print(f\"Number of features: {best_k}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best CV score ({tuned_metric_name}): {tuned_score_display:.6f}\")\n",
    "\n",
    "    # remember training columns\n",
    "    try:\n",
    "        search.best_estimator_.input_columns_ = list(X_train.columns)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # threshold tuning only for classification\n",
    "    if task == \"classification\":\n",
    "        try:\n",
    "            _tune_threshold_inplace(search.best_estimator_, X_train, y_train, search_cv)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] threshold tuning failed: {e}\")\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "def _tune_threshold_inplace(fitted_estimator, X, y, cv):\n",
    "    \"\"\"\n",
    "    Finds a good decision threshold using out-of-fold scores on the training set.\n",
    "    Stores results on the estimator as .best_threshold_ and .best_threshold_cv_f1_.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    # try probabilities first\n",
    "    scores = None\n",
    "    try:\n",
    "        proba_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"predict_proba\", n_jobs=1)  # shape (n, 2)\n",
    "        scores = proba_oof[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback to decision function\n",
    "    if scores is None:\n",
    "        try:\n",
    "            decision_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"decision_function\", n_jobs=1)\n",
    "            dec_min, dec_max = float(decision_oof.min()), float(decision_oof.max())\n",
    "            scores = (decision_oof - dec_min) / (dec_max - dec_min + 1e-9)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # if no scores available, keep default threshold\n",
    "    if scores is None:\n",
    "        fitted_estimator.best_threshold_ = 0.5\n",
    "        fitted_estimator.best_threshold_cv_f1_ = None\n",
    "        print(\"[info] model does not expose scores for thresholding. Using 0.5.\")\n",
    "        return\n",
    "\n",
    "    # sweep thresholds\n",
    "    best_threshold = 0.5\n",
    "    best_f1_macro = -1.0\n",
    "    thresholds_to_try = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    for t in thresholds_to_try:\n",
    "        y_hat = (scores >= t).astype(int)\n",
    "        f1_macro_val = float(f1_score(y, y_hat, average=\"macro\"))\n",
    "        if f1_macro_val > best_f1_macro:\n",
    "            best_f1_macro = f1_macro_val\n",
    "            best_threshold = float(t)\n",
    "\n",
    "    # show OOF result at best threshold\n",
    "    y_hat_final = (scores >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Threshold tuning (OOF on train) ===\")\n",
    "    print(f\"Best threshold: {best_threshold:.2f} | F1_macro: {best_f1_macro:.6f}\")\n",
    "    print(\"Confusion matrix at best threshold:\")\n",
    "    print(confusion_matrix(y, y_hat_final))\n",
    "\n",
    "    # store on estimator\n",
    "    fitted_estimator.best_threshold_ = best_threshold\n",
    "    fitted_estimator.best_threshold_cv_f1_ = best_f1_macro\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Holdout evaluation helper (uses tuned threshold if available)\n",
    "# =============================\n",
    "def evaluate_on_holdout(model, X_test, y_test, task_type, threshold=None):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import f1_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "    # align columns to what the model saw at fit\n",
    "    try:\n",
    "        if hasattr(model, \"input_columns_\") and hasattr(X_test, \"reindex\"):\n",
    "            X_test = X_test.reindex(columns=model.input_columns_, fill_value=0)\n",
    "        elif hasattr(model, \"named_steps\") and \"align\" in getattr(model, \"named_steps\", {}):\n",
    "            keep_cols = getattr(model.named_steps[\"align\"], \"keep_columns_\", None)\n",
    "            if keep_cols is not None and hasattr(X_test, \"reindex\"):\n",
    "                X_test = X_test.reindex(columns=list(keep_cols), fill_value=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not align columns: {e}\")\n",
    "\n",
    "    # choose prediction path\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        final_threshold = threshold\n",
    "        if final_threshold is None and hasattr(model, \"best_threshold_\"):\n",
    "            final_threshold = float(model.best_threshold_)\n",
    "        if final_threshold is not None:\n",
    "            y_pred = predict_with_threshold(model, X_test, threshold=final_threshold)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n=== Holdout (time split) ===\")\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(f\"F1 macro: {f1:.6f}\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        return f1\n",
    "    else:\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c55786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Timer + memory helpers\n",
    "# =========================\n",
    "class SimpleTimer:\n",
    "    def __init__(self, enabled=True):\n",
    "        self.enabled = enabled\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "    def tick(self, label):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        t = time.perf_counter() - self.t0\n",
    "        print(f\"[timer] {label}: {t:.2f} s\")\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "def df_mem_gb(df):\n",
    "    try:\n",
    "        return float(df.memory_usage(deep=True).sum()) / (1024**3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def show_shape_mem(label, X_train=None, X_test=None):\n",
    "    parts = [label]\n",
    "    if X_train is not None:\n",
    "        parts.append(f\"X_train shape={tuple(X_train.shape)} mem={df_mem_gb(X_train):.3f} GB\")\n",
    "    if X_test is not None:\n",
    "        parts.append(f\"X_test shape={tuple(X_test.shape)} mem={df_mem_gb(X_test):.3f} GB\")\n",
    "    print(\"[info]\", \" | \".join(parts))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Text feature helpers (fast)\n",
    "# =========================\n",
    "def clean_keyword_name(s):\n",
    "    s = str(s).lower().strip().replace(\" \", \"_\")\n",
    "    keep = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch == \"_\":\n",
    "            keep.append(ch)\n",
    "    return \"\".join(keep)[:60]\n",
    "\n",
    "def text_features_fit(X, keyword_map):\n",
    "    keyword_map = keyword_map or {}\n",
    "    new_cols = []\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        if col not in X.columns:\n",
    "            continue\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"]:\n",
    "            continue\n",
    "        if col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            safe = clean_keyword_name(kw)\n",
    "            name = f\"{col}_has_{safe}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        df_part = pd.DataFrame(part, index=X.index)\n",
    "        new_parts.append(df_part)\n",
    "        new_cols.extend(df_part.columns.tolist())\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    return {\"new_cols\": new_cols, \"keyword_map\": keyword_map}\n",
    "\n",
    "def text_features_apply(X, text_info):\n",
    "    keyword_map = text_info.get(\"keyword_map\") or {}\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        if col not in X.columns:\n",
    "            part = {\n",
    "                len_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "                wc_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "            }\n",
    "            for kw in keywords:\n",
    "                name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "                part[name] = pd.Series(0, index=X.index, dtype=np.uint8)\n",
    "            new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "            continue\n",
    "\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"] or col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    for c in text_info.get(\"new_cols\", []):\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "\n",
    "    return X\n",
    "\n",
    "# length features for text columns like title_length and description_length\n",
    "def add_text_length_features_inplace(X, exclude_cols=None):\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    obj_cols = [c for c in X.columns if str(X[c].dtype) in [\"object\", \"category\"] and c not in exclude_cols]\n",
    "    for c in obj_cols:\n",
    "        X[f\"{c}_length\"] = X[c].fillna(\"\").astype(str).str.len().astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# General helpers\n",
    "# =========================\n",
    "def datetimes_to_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        if np.issubdtype(X[c].dtype, np.datetime64):\n",
    "            mask = X[c].isna()\n",
    "            vals_int = X[c].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "            arr = vals_int.astype(\"float64\") / 1000000000.0\n",
    "            if mask.any():\n",
    "                arr[mask.values] = np.nan\n",
    "            X[c] = arr\n",
    "    return X\n",
    "\n",
    "def downcast_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        dt = X[c].dtype\n",
    "        if np.issubdtype(dt, np.floating):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "        elif np.issubdtype(dt, np.integer) and X[c].nunique(dropna=True) > 2:\n",
    "            X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "def scale_fit(method, X_train_num):\n",
    "    if method == \"standard\":\n",
    "        return StandardScaler().fit(X_train_num)\n",
    "    if method == \"minmax\":\n",
    "        return MinMaxScaler().fit(X_train_num)\n",
    "    return None\n",
    "\n",
    "def scale_numeric_only(X_train, X_test, scale_method):\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return X_train, X_test\n",
    "    scaler = scale_fit(scale_method, X_train[num_cols])\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols]).astype(\"float32\")\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols]).astype(\"float32\")\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Safer OHE with caps (no exclusions)\n",
    "# =========================\n",
    "def _auto_exclude_mask(series, max_unique=500, max_avg_len=25):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    nunq = int(s.nunique(dropna=False))\n",
    "    avg_len = float(s.map(len).mean())\n",
    "    return (nunq > max_unique) or (avg_len > max_avg_len and nunq > 50)\n",
    "\n",
    "def _cap_categories(series, top_k=100, min_freq=1, other_label=\"Other\"):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    vc = s.value_counts()\n",
    "    kept = vc[vc >= min_freq].index.tolist()\n",
    "    if top_k is not None and len(kept) > top_k:\n",
    "        kept = vc.index[:top_k].tolist()\n",
    "    mapped = s.where(s.isin(kept), other_label)\n",
    "    return mapped.astype(\"category\"), kept\n",
    "\n",
    "def ohe_fit(\n",
    "    X,\n",
    "    exclude_cols=None,\n",
    "    top_k_per_col=100,\n",
    "    min_freq_per_col=1,\n",
    "    auto_exclude=False,\n",
    "    high_card_threshold=500,\n",
    "    long_text_avglen=25,\n",
    "):\n",
    "    exclude = set(exclude_cols or [])\n",
    "    value_map = {}\n",
    "\n",
    "    # do not drop any columns\n",
    "    X_tmp = X.copy()\n",
    "\n",
    "    obj_cols = [c for c in X_tmp.select_dtypes(include=[\"object\", \"category\"]).columns]\n",
    "\n",
    "    # do not auto-exclude anything\n",
    "    excluded = list(exclude)\n",
    "\n",
    "    for c in obj_cols:\n",
    "        capped, kept = _cap_categories(X_tmp[c], top_k=top_k_per_col, min_freq=min_freq_per_col)\n",
    "        X_tmp[c] = capped\n",
    "        value_map[c] = kept\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=obj_cols, dummy_na=False)\n",
    "    schema_cols = X_ohe.columns.tolist()\n",
    "\n",
    "    return {\n",
    "        \"obj_cols\": obj_cols,\n",
    "        \"schema_cols\": schema_cols,\n",
    "        \"value_map\": value_map,\n",
    "        \"excluded\": excluded,\n",
    "        \"other_label\": \"Other\",\n",
    "    }\n",
    "\n",
    "def ohe_apply(X, ohe_info):\n",
    "    obj_cols = ohe_info[\"obj_cols\"]\n",
    "    schema_cols = ohe_info[\"schema_cols\"]\n",
    "    value_map = ohe_info[\"value_map\"]\n",
    "    other = ohe_info.get(\"other_label\", \"Other\")\n",
    "    excluded = ohe_info.get(\"excluded\", [])\n",
    "\n",
    "    X_tmp = X.drop(columns=excluded, errors=\"ignore\").copy()\n",
    "\n",
    "    for c in obj_cols:\n",
    "        if c in X_tmp.columns:\n",
    "            s = X_tmp[c].fillna(\"Unknown\").astype(str)\n",
    "            kept = set(value_map.get(c, []))\n",
    "            s = s.where(s.isin(kept), other).astype(\"category\")\n",
    "            X_tmp[c] = s\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=[c for c in obj_cols if c in X_tmp.columns], dummy_na=False)\n",
    "    X_ohe = X_ohe.reindex(columns=schema_cols, fill_value=0)\n",
    "    return X_ohe\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Outliers\n",
    "# =========================\n",
    "def outlier_bounds_fit(X_num, lower_q=0.025, upper_q=0.975, exclude_binary=True, sample_rows=200000):\n",
    "    bounds = {}\n",
    "    if lower_q is None or upper_q is None:\n",
    "        return bounds\n",
    "    X_use = X_num\n",
    "    if len(X_num) > sample_rows:\n",
    "        X_use = X_num.sample(n=sample_rows, random_state=123)\n",
    "    for c in X_use.columns:\n",
    "        vals = X_use[c].astype(\"float32\")\n",
    "        if exclude_binary and X_use[c].nunique(dropna=True) <= 2:\n",
    "            continue\n",
    "        lo = np.nanquantile(vals, lower_q)\n",
    "        hi = np.nanquantile(vals, upper_q)\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and hi >= lo:\n",
    "            bounds[c] = (float(lo), float(hi))\n",
    "    return bounds\n",
    "\n",
    "def outlier_mask(X, bounds):\n",
    "    if not bounds:\n",
    "        return pd.Series(True, index=X.index)\n",
    "    m = pd.Series(True, index=X.index)\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in X.columns:\n",
    "            col = X[c].astype(\"float32\")\n",
    "            m &= (col >= lo) & (col <= hi)\n",
    "    return m\n",
    "\n",
    "# keep rows and clip values\n",
    "def outlier_clip_inplace(X, bounds):\n",
    "    if not bounds:\n",
    "        return X\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].astype(\"float32\").clip(lo, hi)\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature selection\n",
    "# =========================\n",
    "def forward_feature_selection(X, y, model,\n",
    "                              scoring=\"neg_mean_absolute_error\",\n",
    "                              cv=5, tol=None, max_features=None, n_jobs=-1, verbose=False):\n",
    "    try:\n",
    "        feature_names = list(X.columns)\n",
    "        X_arr = X.values\n",
    "    except AttributeError:\n",
    "        X_arr = X\n",
    "        feature_names = [f\"f{i}\" for i in range(X_arr.shape[1])]\n",
    "\n",
    "    selected_idx = []\n",
    "    remaining_idx = list(range(X_arr.shape[1]))\n",
    "    best_scores = []\n",
    "    previous_score = float(\"inf\")\n",
    "    best_feature_set_idx = []\n",
    "    best_score = float(\"inf\")\n",
    "\n",
    "    while remaining_idx:\n",
    "        scores = {}\n",
    "        for idx in remaining_idx:\n",
    "            trial_idx = selected_idx + [idx]\n",
    "            cv_score = -cross_val_score(\n",
    "                model, X_arr[:, trial_idx], y,\n",
    "                scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "            ).mean()\n",
    "            scores[idx] = cv_score\n",
    "\n",
    "        best_idx = min(scores, key=scores.get)\n",
    "        current_score = scores[best_idx]\n",
    "\n",
    "        if tol is not None and previous_score - current_score < tol:\n",
    "            if verbose:\n",
    "                print(\"Stopping early (improvement < tol).\")\n",
    "            break\n",
    "\n",
    "        selected_idx.append(best_idx)\n",
    "        remaining_idx.remove(best_idx)\n",
    "        best_scores.append(current_score)\n",
    "        previous_score = current_score\n",
    "\n",
    "        if verbose:\n",
    "            name = feature_names[best_idx]\n",
    "            print(f\"Added {name} -> CV score = {current_score:.4f}\")\n",
    "\n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_feature_set_idx = selected_idx.copy()\n",
    "\n",
    "        if max_features is not None and len(selected_idx) >= max_features:\n",
    "            break\n",
    "\n",
    "    selected_features = [feature_names[i] for i in selected_idx]\n",
    "    best_feature_set = [feature_names[i] for i in best_feature_set_idx]\n",
    "\n",
    "    if not best_feature_set:\n",
    "        best_feature_set = selected_features[:]\n",
    "        best_score = best_scores[-1] if best_scores else float(\"inf\")\n",
    "\n",
    "    try:\n",
    "        index = np.argmax(np.array(selected_features) == best_feature_set[-1])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(best_scores) + 1), best_scores, marker=\".\")\n",
    "        plt.plot([index + 1], [best_score], marker=\"x\")\n",
    "        plt.xticks(range(1, len(selected_features) + 1),\n",
    "                   selected_features, rotation=60, ha=\"right\", fontsize=6)\n",
    "        plt.title(\"Forward Feature Selection and CV Scores\")\n",
    "        plt.xlabel(\"Features Added\")\n",
    "        plt.ylabel(\"CV Score (MAE)\")\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"Best Features: {best_feature_set}\")\n",
    "    print(f\"Best CV MAE Score: {best_score:.4f}\")\n",
    "    return selected_features, best_scores, best_feature_set, best_score\n",
    "\n",
    "def select_features(method, max_features, task_type, random_state, X_train, y_train):\n",
    "    if method is None or method == \"none\":\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    k = min(max_features, X_train.shape[1]) if max_features else X_train.shape[1]\n",
    "    if k < 1:\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    if method == \"tree\":\n",
    "        est = RandomForestClassifier(random_state=random_state) if task_type == \"classification\" else RandomForestRegressor(random_state=random_state)\n",
    "        est.fit(X_train, y_train)\n",
    "        imp = pd.Series(est.feature_importances_, index=X_train.columns)\n",
    "        return imp.nlargest(k).index.tolist()\n",
    "\n",
    "    if method == \"forward\":\n",
    "        if task_type != \"regression\":\n",
    "            raise ValueError(\"Forward selection is only supported for regression tasks.\")\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        _, _, best_set, _ = forward_feature_selection(\n",
    "            X=X_train, y=y_train, model=model,\n",
    "            scoring=\"neg_mean_absolute_error\", cv=3,\n",
    "            tol=None, max_features=max_features, n_jobs=-1, verbose=True\n",
    "        )\n",
    "        return best_set\n",
    "\n",
    "    if method == \"mutual_info\":\n",
    "        sel = SelectKBest(mutual_info_classif if task_type == \"classification\" else mutual_info_regression, k=k)\n",
    "        sel.fit(X_train, y_train)\n",
    "        return X_train.columns[sel.get_support()].tolist()\n",
    "\n",
    "    return X_train.columns.tolist()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Batched poly features\n",
    "# =========================\n",
    "def add_poly_features_batched(X_train, X_test, squares, pairs):\n",
    "    train_parts = {}\n",
    "    for c in squares:\n",
    "        if c in X_train.columns:\n",
    "            train_parts[f\"{c}_sq\"] = X_train[c].astype(\"float32\") ** 2\n",
    "    for a, b in pairs:\n",
    "        if (a in X_train.columns) and (b in X_train.columns):\n",
    "            name = f\"{a}_x_{b}\"\n",
    "            train_parts[name] = (X_train[a].astype(\"float32\") * X_train[b].astype(\"float32\"))\n",
    "\n",
    "    test_parts = {}\n",
    "    for name in train_parts:\n",
    "        if name.endswith(\"_sq\"):\n",
    "            c = name[:-3]\n",
    "            if c in X_test.columns:\n",
    "                test_parts[name] = X_test[c].astype(\"float32\") ** 2\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "        else:\n",
    "            a, b = name.split(\"_x_\")\n",
    "            if (a in X_test.columns) and (b in X_test.columns):\n",
    "                test_parts[name] = (X_test[a].astype(\"float32\") * X_test[b].astype(\"float32\"))\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "\n",
    "    if train_parts:\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(train_parts, index=X_train.index)], axis=1)\n",
    "    if test_parts:\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(test_parts, index=X_test.index)], axis=1)\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train.copy())\n",
    "    X_test = downcast_numeric_inplace(X_test.copy())\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main prep\n",
    "# =========================\n",
    "def prepare_data(steam_df, olist_df, sales_df, test_size, random_state,\n",
    "                 feature_selection, max_features, task_type, scale_method,\n",
    "                 tag_min_count=5, tag_top_k=200,\n",
    "                 outlier_lower_q=0.025, outlier_upper_q=0.975,\n",
    "                 verbose=False,\n",
    "                 ohe_top_k_per_col=100,\n",
    "                 ohe_min_freq_per_col=1,\n",
    "                 ohe_auto_exclude=False,\n",
    "                 ohe_high_card_threshold=500,\n",
    "                 ohe_long_text_avglen=25):\n",
    "    feature_selection = (feature_selection or \"none\").lower()\n",
    "    outputs = {}\n",
    "    timer = SimpleTimer(enabled=verbose)\n",
    "\n",
    "    # ---------- STEAM ----------\n",
    "    steam = steam_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        steam[\"target\"] = (steam[\"positive_ratio\"] >= 50).astype(int)\n",
    "    else:\n",
    "        steam[\"target\"] = steam[\"positive_ratio\"]\n",
    "    steam.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    steam.drop(columns=[\"app_id\", \"user_id\", \"review_id\", \"positive_ratio\"],\n",
    "               errors=\"ignore\", inplace=True)\n",
    "\n",
    "    if {\"date\", \"date_release\"}.issubset(steam.columns):\n",
    "        steam[\"days_since_release\"] = (steam[\"date\"] - steam[\"date_release\"]).dt.days\n",
    "\n",
    "    for col in [\"is_recommended\", \"mac\", \"linux\", \"win\", \"steam_deck\"]:\n",
    "        if col in steam.columns:\n",
    "            steam[col] = steam[col].astype(int)\n",
    "\n",
    "    if \"hours\" in steam.columns:\n",
    "        steam[\"log_hours\"] = np.log1p(steam[\"hours\"])\n",
    "    if {\"hours\", \"user_reviews\"}.issubset(steam.columns):\n",
    "        steam[\"reviews_per_hour\"] = steam[\"user_reviews\"] / (steam[\"hours\"] + 0.000000001)\n",
    "\n",
    "    X = steam.drop(columns=[\"target\", \"rating\"], errors=\"ignore\")\n",
    "    y = steam[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"steam split\")\n",
    "    show_shape_mem(\"steam post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"steam datetime to numeric\")\n",
    "\n",
    "    # add text length features\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test,  exclude_cols=[\"tags\"])\n",
    "\n",
    "    steam_kw = {\n",
    "        \"title\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"coop\", \"online\", \"free\", \"demo\", \"survival\"],\n",
    "        \"description\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"open world\", \"story\", \"puzzle\", \"horror\", \"early access\"],\n",
    "    }\n",
    "    steam_text_info = text_features_fit(X_train, steam_kw)\n",
    "    X_test = text_features_apply(X_test, steam_text_info)\n",
    "    timer.tick(\"steam text features\")\n",
    "    show_shape_mem(\"steam after text\", X_train, X_test)\n",
    "\n",
    "    if \"tags\" in X_train.columns:\n",
    "        from collections import Counter\n",
    "\n",
    "        def tag_col_name(t):\n",
    "            s = str(t).lower().strip().replace(\" \", \"_\")\n",
    "            return \"tag_\" + \"\".join(ch for ch in s if ch.isalnum() or ch == \"_\")[:60]\n",
    "\n",
    "        cnt = Counter()\n",
    "        for v in X_train[\"tags\"].fillna(\"\").values:\n",
    "            lst = v if isinstance(v, list) else []\n",
    "            for t in lst:\n",
    "                cnt[t] += 1\n",
    "\n",
    "        items = [(t, n) for t, n in cnt.items() if n >= tag_min_count]\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        vocab = [t for t, _ in items[:tag_top_k]]\n",
    "        tag_cols = [tag_col_name(t) for t in vocab]\n",
    "        if verbose:\n",
    "            print(f\"[info] steam tags unique={len(cnt)}, kept={len(vocab)} (min_count={tag_min_count}, top_k={tag_top_k})\")\n",
    "\n",
    "        def add_tag_cols_fast(df):\n",
    "            if \"tags\" in df.columns:\n",
    "                tag_lists = df[\"tags\"].apply(lambda v: v if isinstance(v, list) else [])\n",
    "            else:\n",
    "                tag_lists = pd.Series([[]] * len(df), index=df.index)\n",
    "            new_data = {}\n",
    "            for tag, col_name in zip(vocab, tag_cols):\n",
    "                new_data[col_name] = np.fromiter(\n",
    "                    (1 if tag in lst else 0 for lst in tag_lists),\n",
    "                    dtype=np.uint8,\n",
    "                    count=len(df)\n",
    "                )\n",
    "            new_df = pd.DataFrame(new_data, index=df.index)\n",
    "            return pd.concat([df.drop(columns=[\"tags\"], errors=\"ignore\"), new_df], axis=1)\n",
    "\n",
    "        X_train = add_tag_cols_fast(X_train)\n",
    "        X_test = add_tag_cols_fast(X_test)\n",
    "        timer.tick(\"steam tags multi-hot\")\n",
    "        show_shape_mem(\"steam after tags\", X_train, X_test)\n",
    "\n",
    "    # include all columns for OHE and cap top 100\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],  # no exclusions\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"steam OHE\")\n",
    "    show_shape_mem(\"steam after OHE\", X_train, X_test)\n",
    "\n",
    "    # numeric-only safety\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "    if verbose:\n",
    "        print(\"steam non-numeric after OHE:\", X_train.select_dtypes(exclude=[\"number\"]).columns.tolist())\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"steam impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    show_shape_mem(\"steam after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    steam_squares = []\n",
    "    if \"log_hours\" in X_train.columns:\n",
    "        steam_squares.append(\"log_hours\")\n",
    "    elif \"hours\" in X_train.columns:\n",
    "        steam_squares.append(\"hours\")\n",
    "    if \"discount\" in X_train.columns:\n",
    "        steam_squares.append(\"discount\")\n",
    "    if \"days_since_release\" in X_train.columns:\n",
    "        steam_squares.append(\"days_since_release\")\n",
    "\n",
    "    steam_pairs = []\n",
    "    if (\"price_final\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_final\", \"discount\"))\n",
    "    if (\"price_original\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_original\", \"discount\"))\n",
    "    if (\"days_since_release\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"days_since_release\", \"discount\"))\n",
    "    if (\"user_reviews\" in X_train.columns) and (\"reviews\" in X_train.columns):\n",
    "        steam_pairs.append((\"user_reviews\", \"reviews\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, steam_squares, steam_pairs)\n",
    "    timer.tick(\"steam poly\")\n",
    "    show_shape_mem(\"steam after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test  = outlier_clip_inplace(X_test,  bounds)\n",
    "    timer.tick(\"steam outlier clip\")\n",
    "    show_shape_mem(\"steam after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"steam scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"steam select features\")\n",
    "    show_shape_mem(\"steam after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"steam\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- OLIST ----------\n",
    "    olist = olist_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        olist[\"target\"] = (olist[\"review_score_mean_product\"] >= 2.5).astype(int)\n",
    "    else:\n",
    "        olist[\"target\"] = olist[\"review_score_mean_product\"]\n",
    "    olist.dropna(subset=[\"target\"], inplace=True)\n",
    "    olist.drop(columns=[\"order_id\", \"customer_id\", \"customer_unique_id\"],\n",
    "               errors=\"ignore\", inplace=True)\n",
    "\n",
    "    olist[\"delivery_delay\"] = (olist[\"order_estimated_delivery_date\"] - olist[\"order_purchase_timestamp\"]).dt.days\n",
    "    denom = olist[\"payment_installments_max\"].replace(0, 1)\n",
    "    olist[\"avg_installment\"] = olist[\"payment_value_total\"] / denom\n",
    "\n",
    "    if {\"product_length_cm\", \"product_width_cm\", \"product_height_cm\"}.issubset(olist.columns):\n",
    "        olist[\"product_volume_cm3\"] = (\n",
    "            olist[\"product_length_cm\"] * olist[\"product_width_cm\"] * olist[\"product_height_cm\"]\n",
    "        )\n",
    "\n",
    "    X = olist.drop(columns=[\"review_score_mean_product\", \"target\"], errors=\"ignore\")\n",
    "    y = olist[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"olist split\")\n",
    "    show_shape_mem(\"olist post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"olist datetime to numeric\")\n",
    "\n",
    "    # add text length features\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test,  exclude_cols=[\"tags\"])\n",
    "\n",
    "    olist_kw = {\n",
    "        \"order_status\": [\"delivered\", \"shipped\", \"canceled\", \"invoiced\", \"processing\"],\n",
    "        \"product_category_name\": [\"moveis\", \"auto\", \"pet\", \"perfumaria\", \"utilidades\", \"brinquedos\"]\n",
    "    }\n",
    "    olist_text_info = text_features_fit(X_train, olist_kw)\n",
    "    X_test = text_features_apply(X_test, olist_text_info)\n",
    "    timer.tick(\"olist text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],  # no exclusions\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"olist OHE\")\n",
    "    show_shape_mem(\"olist after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"olist impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    show_shape_mem(\"olist after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    olist_squares = []\n",
    "    if \"delivery_delay\" in X_train.columns:\n",
    "        olist_squares.append(\"delivery_delay\")\n",
    "    if \"price\" in X_train.columns:\n",
    "        olist_squares.append(\"price\")\n",
    "    if \"freight_value\" in X_train.columns:\n",
    "        olist_squares.append(\"freight_value\")\n",
    "\n",
    "    olist_pairs = []\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_weight_g\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_weight_g\"))\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_volume_cm3\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_volume_cm3\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"price\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"freight_value\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"freight_value\"))\n",
    "    if (\"payment_installments_max\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"payment_installments_max\", \"price\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, olist_squares, olist_pairs)\n",
    "    timer.tick(\"olist poly\")\n",
    "    show_shape_mem(\"olist after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test  = outlier_clip_inplace(X_test,  bounds)\n",
    "    timer.tick(\"olist outlier clip\")\n",
    "    show_shape_mem(\"olist after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"olist scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"olist select features\")\n",
    "    show_shape_mem(\"olist after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"olist\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- SALES ----------\n",
    "    sales = sales_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        sales[\"target\"] = (sales[\"Critic_Score\"] >= 5.0).astype(int)\n",
    "    else:\n",
    "        sales[\"target\"] = sales[\"Critic_Score\"]\n",
    "    sales.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    for c in [\"ESRB_Rating\", \"Genre\", \"Platform\", \"Publisher\", \"Developer\"]:\n",
    "        if c in sales.columns:\n",
    "            sales[c] = sales[c].fillna(\"Unknown\")\n",
    "\n",
    "    X = sales.drop(columns=[\"target\", \"Critic_Score\"], errors=\"ignore\")\n",
    "    y = sales[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"sales split\")\n",
    "    show_shape_mem(\"sales post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"sales datetime to numeric\")\n",
    "\n",
    "    # add text length features\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test,  exclude_cols=[\"tags\"])\n",
    "\n",
    "    sales_kw = {\n",
    "        \"Name\": [\"mario\", \"pokemon\", \"zelda\", \"call of duty\", \"fifa\", \"minecraft\", \"final fantasy\"],\n",
    "        \"Genre\": [\"action\", \"sports\", \"shooter\", \"racing\", \"role\", \"adventure\", \"platform\", \"puzzle\"],\n",
    "        \"Publisher\": [\"nintendo\", \"electronic arts\", \"ea\", \"activision\", \"ubisoft\", \"sony\", \"sega\"],\n",
    "        \"ESRB_Rating\": [\"e\", \"t\", \"m\"]\n",
    "    }\n",
    "    sales_text_info = text_features_fit(X_train, sales_kw)\n",
    "    X_test = text_features_apply(X_test, sales_text_info)\n",
    "    timer.tick(\"sales text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],  # no exclusions\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"sales OHE\")\n",
    "    show_shape_mem(\"sales after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"sales impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    show_shape_mem(\"sales after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    sales_squares = []\n",
    "    if \"Year\" in X_train.columns:\n",
    "        sales_squares.append(\"Year\")\n",
    "    if \"User_Score\" in X_train.columns:\n",
    "        sales_squares.append(\"User_Score\")\n",
    "\n",
    "    sales_pairs = []\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"PAL_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"PAL_Sales\"))\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"JP_Sales\"))\n",
    "    if (\"PAL_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"PAL_Sales\", \"JP_Sales\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, sales_squares, sales_pairs)\n",
    "    timer.tick(\"sales poly\")\n",
    "    show_shape_mem(\"sales after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test  = outlier_clip_inplace(X_test,  bounds)\n",
    "    timer.tick(\"sales outlier clip\")\n",
    "    show_shape_mem(\"sales after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"sales scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"sales select features\")\n",
    "    show_shape_mem(\"sales after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"sales\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- Shape summary ----------\n",
    "    for name, parts in outputs.items():\n",
    "        Xtr, Xte, ytr, yte = parts\n",
    "        print(f\"[{name}] X_train: {Xtr.shape} | X_test: {Xte.shape} | y_train: {ytr.shape} | y_test: {yte.shape}\")\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18a3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.244 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.22 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.191 sec\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 10000 of 41154794 rows in 5.901 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(10000, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: picked 10000 of 99441 rows in 0.001 sec\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (11444, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: picked 10000 of 55792 rows in 0.022 sec\n",
      "vg2019: done shape=(10000, 16)\n",
      "main: load all done in 17.277 sec (00:00:17)\n",
      "download: shapes summary\n",
      "download: steam shape = (10000, 24)\n",
      "download: olist shape = (11444, 38)\n",
      "download: sales shape = (10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Download Paths\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "\n",
    "# Load All\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# Download Shapes\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e087b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Robust EDA Report: steam ===\n",
      "\n",
      "=== Info ===\n",
      " rows  columns  memory_bytes\n",
      "10000       24       5888254\n",
      "\n",
      "=== Dtypes ===\n",
      "        column          dtype\n",
      "        app_id          int64\n",
      "          date datetime64[ns]\n",
      "  date_release datetime64[ns]\n",
      "   description         object\n",
      "      discount        float64\n",
      "         funny          int64\n",
      "       helpful          int64\n",
      "         hours        float64\n",
      "is_recommended           bool\n",
      "         linux           bool\n",
      "           mac           bool\n",
      "positive_ratio          int64\n",
      "   price_final        float64\n",
      "price_original        float64\n",
      "      products          int64\n",
      "        rating         object\n",
      "     review_id          int64\n",
      "       reviews          int64\n",
      "    steam_deck           bool\n",
      "          tags         object\n",
      "         title         object\n",
      "       user_id          int64\n",
      "  user_reviews          int64\n",
      "           win           bool\n",
      "\n",
      "=== Missing Values ===\n",
      "        column  missing_count  missing_percent\n",
      "        app_id              0         0.000000\n",
      "       helpful              0         0.000000\n",
      "         funny              0         0.000000\n",
      "          date              0         0.000000\n",
      "is_recommended              0         0.000000\n",
      "         hours              0         0.000000\n",
      "       user_id              0         0.000000\n",
      "     review_id              0         0.000000\n",
      "         title              0         0.000000\n",
      "  date_release              0         0.000000\n",
      "           win              0         0.000000\n",
      "           mac              0         0.000000\n",
      "         linux              0         0.000000\n",
      "        rating              0         0.000000\n",
      "positive_ratio              0         0.000000\n",
      "  user_reviews              0         0.000000\n",
      "   price_final              0         0.000000\n",
      "price_original              0         0.000000\n",
      "      discount              0         0.000000\n",
      "    steam_deck              0         0.000000\n",
      "   description              0         0.000000\n",
      "          tags              0         0.000000\n",
      "      products              0         0.000000\n",
      "       reviews              0         0.000000\n",
      "\n",
      "=== Duplicates Summary ===\n",
      " duplicate_rows\n",
      "              0\n",
      "\n",
      "=== Duplicates Preview (up to 20 rows) ===\n",
      "(No duplicate rows found.)\n",
      "\n",
      "=== Numeric Summary (5%..95%) ===\n",
      "        column        count            mean             std         min             5%             25%             50%             75%             95%             max\n",
      "        app_id 10000.000000   599491.220200   472179.661750   10.000000    8847.500000   252490.000000   433340.000000   916952.500000  1549180.000000  2206340.000000\n",
      "       helpful 10000.000000        3.086700       38.879842    0.000000       0.000000        0.000000        0.000000        0.000000        8.000000     3259.000000\n",
      "         funny 10000.000000        0.984800       16.931764    0.000000       0.000000        0.000000        0.000000        0.000000        2.000000     1070.000000\n",
      "         hours 10000.000000       98.613400      172.462586    0.000000       0.800000        7.700000       26.900000       97.500000      496.095000      998.800000\n",
      "       user_id 10000.000000  7469461.674500  3995032.126820 1072.000000  772104.400000  4358313.250000  7545213.000000 10984898.750000 13573287.500000 14304565.000000\n",
      "     review_id 10000.000000 20545984.688300 11869151.354655 1499.000000 2126884.450000 10165465.750000 20557149.000000 30850872.000000 39147358.100000 41152293.000000\n",
      "positive_ratio 10000.000000       86.208500       11.123498   13.000000      64.000000       82.000000       89.000000       94.000000       97.000000      100.000000\n",
      "  user_reviews 10000.000000   174295.604600   579917.401950   10.000000     653.950000    10514.750000    45796.000000   136055.000000   637341.000000  7494460.000000\n",
      "   price_final 10000.000000       18.622725       16.560762    0.000000       0.000000        4.990000       15.000000       29.990000       59.990000       70.000000\n",
      "price_original 10000.000000        7.442697       12.041573    0.000000       0.000000        0.000000        0.000000       14.990000       29.990000       79.980000\n",
      "      discount 10000.000000        3.321200       15.152227    0.000000       0.000000        0.000000        0.000000        0.000000        0.000000       90.000000\n",
      "      products 10000.000000      274.908500      674.205738    0.000000      12.000000       49.000000      116.000000      264.000000      926.000000    19774.000000\n",
      "       reviews 10000.000000       26.350200      164.819288    1.000000       1.000000        2.000000        5.000000       15.000000       75.050000     6045.000000\n",
      "\n",
      "=== Skew and Kurtosis ===\n",
      "        column      skew    kurtosis\n",
      "        app_id  0.945135    0.089466\n",
      "       helpful 62.508928 4983.159851\n",
      "         funny 44.665099 2368.319774\n",
      "         hours  2.801352    8.186668\n",
      "       user_id -0.115645   -1.089530\n",
      "     review_id  0.009927   -1.199204\n",
      "positive_ratio -1.917753    5.259712\n",
      "  user_reviews 10.241628  122.591565\n",
      "   price_final  0.902062    0.180778\n",
      "price_original  1.875139    3.697629\n",
      "      discount  4.543284   19.337049\n",
      "      products 13.023190  281.904033\n",
      "       reviews 22.010897  603.980879\n",
      "\n",
      "=== IQR Outlier Counts ===\n",
      "        column  outlier_count\n",
      "        app_id            110\n",
      "       helpful           2107\n",
      "         funny            643\n",
      "         hours           1277\n",
      "       user_id              0\n",
      "     review_id              0\n",
      "positive_ratio            498\n",
      "  user_reviews           1209\n",
      "   price_final             10\n",
      "price_original            353\n",
      "      discount            487\n",
      "      products            970\n",
      "       reviews           1173\n",
      "\n",
      "=== Correlation (first 30 numeric columns) ===\n",
      "                  app_id   helpful     funny     hours   user_id  review_id  positive_ratio  user_reviews  price_final  price_original  discount  products   reviews\n",
      "column                                                                                                                                                              \n",
      "app_id          1.000000 -0.002930 -0.007099 -0.129953 -0.038656   0.018711       -0.161827     -0.155326     0.112610        0.024748 -0.030042 -0.005167 -0.000123\n",
      "helpful        -0.002930  1.000000  0.408794  0.002932 -0.003928  -0.004807       -0.012311      0.005315     0.006903        0.002336 -0.003899  0.020429  0.008610\n",
      "funny          -0.007099  0.408794  1.000000  0.001283 -0.004810  -0.002500        0.004805      0.009511     0.007895       -0.000077 -0.003765  0.012144  0.000962\n",
      "hours          -0.129953  0.002932  0.001283  1.000000 -0.016284  -0.233563        0.019357      0.266046     0.038505       -0.157804 -0.078525 -0.067875 -0.054228\n",
      "user_id        -0.038656 -0.003928 -0.004810 -0.016284  1.000000   0.010005        0.010324     -0.029112     0.001255        0.037213  0.015539  0.050912  0.019403\n",
      "review_id       0.018711 -0.004807 -0.002500 -0.233563  0.010005   1.000000       -0.117824     -0.183874    -0.226662        0.198994  0.097526  0.089226  0.075168\n",
      "positive_ratio -0.161827 -0.012311  0.004805  0.019357  0.010324  -0.117824        1.000000      0.027136     0.020754       -0.100965  0.001847 -0.042591 -0.059781\n",
      "user_reviews   -0.155326  0.005315  0.009511  0.266046 -0.029112  -0.183874        0.027136      1.000000    -0.053319       -0.161077 -0.054650 -0.056194 -0.030664\n",
      "price_final     0.112610  0.006903  0.007895  0.038505  0.001255  -0.226662        0.020754     -0.053319     1.000000        0.207524 -0.171852 -0.031359 -0.052080\n",
      "price_original  0.024748  0.002336 -0.000077 -0.157804  0.037213   0.198994       -0.100965     -0.161077     0.207524        1.000000  0.246890  0.086113  0.007127\n",
      "discount       -0.030042 -0.003899 -0.003765 -0.078525  0.015539   0.097526        0.001847     -0.054650    -0.171852        0.246890  1.000000  0.027869  0.025135\n",
      "products       -0.005167  0.020429  0.012144 -0.067875  0.050912   0.089226       -0.042591     -0.056194    -0.031359        0.086113  0.027869  1.000000  0.335938\n",
      "reviews        -0.000123  0.008610  0.000962 -0.054228  0.019403   0.075168       -0.059781     -0.030664    -0.052080        0.007127  0.025135  0.335938  1.000000\n",
      "\n",
      "=== Categorical Values (Top 20 per column) ===\n",
      "        column               value  count   percent\n",
      "          date 2019-06-29 00:00:00     76  0.760000\n",
      "          date 2021-11-24 00:00:00     42  0.420000\n",
      "          date 2019-06-30 00:00:00     35  0.350000\n",
      "          date 2020-11-25 00:00:00     32  0.320000\n",
      "          date 2021-11-25 00:00:00     32  0.320000\n",
      "          date 2020-11-26 00:00:00     32  0.320000\n",
      "          date 2022-11-22 00:00:00     31  0.310000\n",
      "          date 2019-06-28 00:00:00     28  0.280000\n",
      "          date 2022-11-23 00:00:00     28  0.280000\n",
      "          date 2019-07-01 00:00:00     26  0.260000\n",
      "          date 2018-11-21 00:00:00     25  0.250000\n",
      "          date 2022-11-24 00:00:00     25  0.250000\n",
      "          date 2021-11-27 00:00:00     25  0.250000\n",
      "          date 2016-11-23 00:00:00     24  0.240000\n",
      "          date 2021-11-26 00:00:00     23  0.230000\n",
      "          date 2018-11-22 00:00:00     22  0.220000\n",
      "          date 2022-11-25 00:00:00     22  0.220000\n",
      "          date 2022-11-29 00:00:00     20  0.200000\n",
      "          date 2019-07-02 00:00:00     19  0.190000\n",
      "          date 2019-11-27 00:00:00     19  0.190000\n",
      "is_recommended                True   8578 85.780000\n",
      "is_recommended               False   1422 14.220000\n",
      "         title     Team Fortress 2     77  0.770000\n",
      "         title      Cyberpunk 2077     61  0.610000\n",
      "         title           Paladins®     60  0.600000\n",
      "\n",
      "=== Unique Counts Per Column ===\n",
      "        column  unique_values\n",
      "        app_id    2879.000000\n",
      "       helpful     121.000000\n",
      "         funny      72.000000\n",
      "          date    2762.000000\n",
      "is_recommended       2.000000\n",
      "         hours    2837.000000\n",
      "       user_id    9976.000000\n",
      "     review_id   10000.000000\n",
      "         title    2878.000000\n",
      "  date_release    1831.000000\n",
      "           win       2.000000\n",
      "           mac       2.000000\n",
      "         linux       2.000000\n",
      "        rating       8.000000\n",
      "positive_ratio      78.000000\n",
      "  user_reviews    2588.000000\n",
      "   price_final     120.000000\n",
      "price_original      42.000000\n",
      "      discount      27.000000\n",
      "    steam_deck       2.000000\n",
      "   description    2390.000000\n",
      "          tags            NaN\n",
      "      products    1274.000000\n",
      "       reviews     309.000000\n",
      "\n",
      "=== Head (10 rows) ===\n",
      " app_id  helpful  funny       date  is_recommended      hours  user_id  review_id                            title date_release  win   mac  linux                  rating  positive_ratio  user_reviews  price_final  price_original  discount  steam_deck description tags  products  reviews\n",
      "1222670        0      0 2022-09-18            True 118.800000  3860966       1499                      The Sims™ 4   2020-06-18 True False  False           Very Positive              87        110750     0.000000        0.000000  0.000000        True               []         5        1\n",
      "1091500        0      0 2022-12-28           False   4.400000  3320671       2802                   Cyberpunk 2077   2020-12-09 True False  False           Very Positive              80        557051    60.000000        0.000000  0.000000        True               []        68        5\n",
      "1237320        0      0 2022-12-31            True  27.900000  4887817       3069                  Sonic Frontiers   2022-11-07 True False  False           Very Positive              93         14850    60.000000        0.000000  0.000000        True               []       148        5\n",
      " 233860        3      0 2018-07-13            True  82.300000  5220359       4145                           Kenshi   2018-12-06 True False  False Overwhelmingly Positive              95         62332    30.000000        0.000000  0.000000        True               []       103       16\n",
      " 284160        0      0 2020-08-28            True  73.000000  8726467       6337                     BeamNG.drive   2015-05-29 True False  False Overwhelmingly Positive              97        178635    25.000000        0.000000  0.000000        True               []       119       27\n",
      " 261550        0      0 2021-03-20            True  49.300000 11797182       6847     Mount & Blade II: Bannerlord   2022-10-25 True False  False           Very Positive              87        177725    50.000000        0.000000  0.000000        True               []       249        4\n",
      " 221100        5      0 2016-06-17            True 881.100000   464915      19171                             DayZ   2018-12-13 True False  False         Mostly Positive              74        296845    45.000000        0.000000  0.000000        True               []      1556       12\n",
      " 244850        0      0 2020-01-02            True  10.600000 11660028      21791                  Space Engineers   2019-02-28 True False  False           Very Positive              89         78007    20.000000        0.000000  0.000000        True               []       191       20\n",
      "    730        0      0 2021-07-08            True  19.500000  2471444      23890 Counter-Strike: Global Offensive   2012-08-21 True  True   True           Very Positive              88       7494460    15.000000        0.000000  0.000000        True               []        14        1\n",
      " 581320       11      0 2021-03-10            True 133.800000  2947008      31440            Insurgency: Sandstorm   2018-12-12 True False  False           Very Positive              86         85811    10.000000        0.000000  0.000000        True               []       200       10\n",
      "\n",
      "=== End of EDA Report ===\n",
      "=== Robust EDA Report: olist ===\n",
      "\n",
      "=== Info ===\n",
      " rows  columns  memory_bytes\n",
      "11444       38      12301438\n",
      "\n",
      "=== Dtypes ===\n",
      "                       column          dtype\n",
      "                customer_city         object\n",
      "                  customer_id         object\n",
      "               customer_state         object\n",
      "           customer_unique_id         object\n",
      "     customer_zip_code_prefix          int64\n",
      "                freight_value        float64\n",
      "                   geo_points        float64\n",
      "              geolocation_lat        float64\n",
      "              geolocation_lng        float64\n",
      "            order_approved_at         object\n",
      " order_delivered_carrier_date datetime64[ns]\n",
      "order_delivered_customer_date datetime64[ns]\n",
      "order_estimated_delivery_date datetime64[ns]\n",
      "                     order_id         object\n",
      "                order_item_id        float64\n",
      "     order_purchase_timestamp datetime64[ns]\n",
      "                 order_status         object\n",
      "                payment_count          int64\n",
      "     payment_installments_max          int64\n",
      "          payment_value_total        float64\n",
      "                        price        float64\n",
      "        product_category_name         object\n",
      "product_category_name_english         object\n",
      "   product_description_lenght        float64\n",
      "            product_height_cm        float64\n",
      "\n",
      "=== Missing Values ===\n",
      "                       column  missing_count  missing_percent\n",
      "order_delivered_customer_date            339         2.960000\n",
      "product_category_name_english            249         2.180000\n",
      "        product_category_name            245         2.140000\n",
      "          product_name_lenght            245         2.140000\n",
      "   product_description_lenght            245         2.140000\n",
      "           product_photos_qty            245         2.140000\n",
      " order_delivered_carrier_date            213         1.860000\n",
      "         review_count_product            128         1.120000\n",
      "    review_score_mean_product            128         1.120000\n",
      "             product_weight_g             82         0.720000\n",
      "            product_length_cm             82         0.720000\n",
      "            product_height_cm             82         0.720000\n",
      "             product_width_cm             82         0.720000\n",
      "                order_item_id             80         0.700000\n",
      "                   product_id             80         0.700000\n",
      "                    seller_id             80         0.700000\n",
      "          shipping_limit_date             80         0.700000\n",
      "                        price             80         0.700000\n",
      "                freight_value             80         0.700000\n",
      "       seller_zip_code_prefix             80         0.700000\n",
      "                  seller_city             80         0.700000\n",
      "                 seller_state             80         0.700000\n",
      "              geolocation_lat             28         0.240000\n",
      "              geolocation_lng             28         0.240000\n",
      "                   geo_points             28         0.240000\n",
      "\n",
      "=== Duplicates Summary ===\n",
      " duplicate_rows\n",
      "              0\n",
      "\n",
      "=== Duplicates Preview (up to 20 rows) ===\n",
      "(No duplicate rows found.)\n",
      "\n",
      "=== Numeric Summary (5%..95%) ===\n",
      "                    column        count         mean          std         min          5%          25%          50%          75%          95%          max\n",
      "  customer_zip_code_prefix 11444.000000 34626.971164 29513.915008 1004.000000 3338.050000 11295.000000 24030.000000 57240.000000 90040.000000 99965.000000\n",
      "           geolocation_lat 11416.000000   -21.253642     5.450153  -33.689948  -28.264439   -23.586323   -22.916897   -20.252688    -8.038037     3.842508\n",
      "           geolocation_lng 11416.000000   -46.188584     3.991135  -68.502935  -52.271901   -48.049677   -46.632292   -43.704435   -38.543398   -34.823063\n",
      "                geo_points 11416.000000   157.252278   158.108211    1.000000   18.000000    54.000000   103.000000   202.000000   481.000000  1146.000000\n",
      "             order_item_id 11364.000000     1.201250     0.663923    1.000000    1.000000     1.000000     1.000000     1.000000     2.000000    11.000000\n",
      "                     price 11364.000000   120.279328   186.394305    2.200000   17.000000    39.000000    72.900000   134.900000   349.900000  4690.000000\n",
      "             freight_value 11364.000000    19.744683    15.323438    0.000000    7.780000    13.080000    16.160000    20.980000    44.098000   294.760000\n",
      "       product_name_lenght 11199.000000    48.880346    10.047190    5.000000   29.000000    43.000000    52.000000    57.000000    60.000000    72.000000\n",
      "product_description_lenght 11199.000000   786.621216   650.790272    4.000000  161.000000   345.000000   600.000000   986.500000  2104.100000  3976.000000\n",
      "        product_photos_qty 11199.000000     2.197071     1.693417    1.000000    1.000000     1.000000     1.000000     3.000000     6.000000    15.000000\n",
      "          product_weight_g 11362.000000  2022.428622  3610.452785    0.000000  119.050000   300.000000   650.000000  1750.000000  9400.000000 30000.000000\n",
      "         product_length_cm 11362.000000    29.937863    16.201601    7.000000   16.000000    18.000000    25.000000    37.000000    61.000000   105.000000\n",
      "         product_height_cm 11362.000000    16.337529    13.235647    2.000000    3.000000     8.000000    13.000000    20.000000    44.000000   105.000000\n",
      "          product_width_cm 11362.000000    22.673209    11.517267    6.000000   11.000000    15.000000    20.000000    30.000000    45.000000   100.000000\n",
      "    seller_zip_code_prefix 11364.000000 24627.663675 27594.655486 1001.000000 2955.000000  6653.000000 13843.000000 29157.000000 88303.000000 99730.000000\n",
      "       payment_value_total 11444.000000   180.862915   275.003076    9.590000   33.230000    64.335000   113.800000   194.500000   540.760000  7274.880000\n",
      "  payment_installments_max 11444.000000     3.019748     2.805307    1.000000    1.000000     1.000000     2.000000     4.000000    10.000000    24.000000\n",
      "             payment_count 11444.000000     1.042905     0.348770    1.000000    1.000000     1.000000     1.000000     1.000000     1.000000    13.000000\n",
      "      review_count_product 11316.000000     3.981707     6.840892    1.000000    1.000000     1.000000     1.000000     3.000000    17.000000    47.000000\n",
      " review_score_mean_product 11316.000000     3.995122     1.185562    1.000000    1.000000     3.500000     4.333333     5.000000     5.000000     5.000000\n",
      "\n",
      "=== Skew and Kurtosis ===\n",
      "                    column      skew   kurtosis\n",
      "  customer_zip_code_prefix  0.811566  -0.727678\n",
      "           geolocation_lat  1.660797   3.144177\n",
      "           geolocation_lng -0.024661   2.168439\n",
      "                geo_points  2.246842   6.815536\n",
      "             order_item_id  5.189536  38.346561\n",
      "                     price  7.605577  98.317205\n",
      "             freight_value  5.459946  52.506008\n",
      "       product_name_lenght -0.929388   0.243588\n",
      "product_description_lenght  1.993265   4.887734\n",
      "        product_photos_qty  1.846079   4.303060\n",
      "          product_weight_g  3.668109  17.269872\n",
      "         product_length_cm  1.824148   4.064315\n",
      "         product_height_cm  2.257122   7.683492\n",
      "          product_width_cm  1.699819   4.228914\n",
      "    seller_zip_code_prefix  1.542412   0.901833\n",
      "       payment_value_total  9.899611 185.842563\n",
      "  payment_installments_max  1.491603   1.717324\n",
      "             payment_count 17.649948 455.503815\n",
      "      review_count_product  3.782292  16.040168\n",
      " review_score_mean_product -1.309752   0.848219\n",
      "\n",
      "=== IQR Outlier Counts ===\n",
      "                    column  outlier_count\n",
      "  customer_zip_code_prefix              0\n",
      "           geolocation_lat           1786\n",
      "           geolocation_lng            554\n",
      "                geo_points            775\n",
      "             order_item_id           1444\n",
      "                     price            857\n",
      "             freight_value           1168\n",
      "       product_name_lenght            124\n",
      "product_description_lenght            703\n",
      "        product_photos_qty            269\n",
      "          product_weight_g           1600\n",
      "         product_length_cm            446\n",
      "         product_height_cm            767\n",
      "          product_width_cm            244\n",
      "    seller_zip_code_prefix           1751\n",
      "       payment_value_total            929\n",
      "  payment_installments_max            830\n",
      "             payment_count            331\n",
      "      review_count_product           1644\n",
      " review_score_mean_product            910\n",
      "\n",
      "=== Correlation (first 30 numeric columns) ===\n",
      "                            customer_zip_code_prefix  geolocation_lat  geolocation_lng  geo_points  order_item_id     price  freight_value  product_name_lenght  product_description_lenght  product_photos_qty  product_weight_g  product_length_cm  product_height_cm  product_width_cm  seller_zip_code_prefix  payment_value_total  payment_installments_max  payment_count  review_count_product  review_score_mean_product\n",
      "column                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "customer_zip_code_prefix                    1.000000         0.127568        -0.303235   -0.061367       0.002355  0.027093       0.230137            -0.002643                    0.024913            0.019439         -0.001004           0.010634           0.010078         -0.028543                0.070782             0.047757                  0.065098       0.001930              0.000585                  -0.029763\n",
      "geolocation_lat                             0.127568         1.000000         0.440411   -0.123759       0.003490  0.056296       0.267724             0.010754                    0.040220            0.035102          0.001493           0.000578          -0.002581         -0.029676               -0.032522             0.085453                  0.059068       0.012950              0.016956                  -0.036610\n",
      "geolocation_lng                            -0.303235         0.440411         1.000000    0.067078      -0.010417  0.023436       0.070631             0.008685                    0.019450            0.004794         -0.007583          -0.012891          -0.008259         -0.012075               -0.021762             0.036664                  0.029819       0.007939              0.016562                  -0.038726\n",
      "geo_points                                 -0.061367        -0.123759         0.067078    1.000000       0.031513  0.000657      -0.031276            -0.005950                    0.002488           -0.018113         -0.012367          -0.000781          -0.005108         -0.007496                0.048534             0.016667                 -0.004632      -0.005720             -0.020561                  -0.021148\n",
      "order_item_id                               0.002355         0.003490        -0.010417    0.031513       1.000000 -0.064469      -0.002930            -0.000208                    0.008972           -0.041938          0.001829           0.068192           0.051762         -0.024374               -0.005437             0.257934                  0.070963      -0.016215             -0.012781                  -0.140012\n",
      "price                                       0.027093         0.056296         0.023436    0.000657      -0.064469  1.000000       0.397203             0.010826                    0.203839            0.045801          0.357744           0.154958           0.233589          0.175308                0.073792             0.791285                  0.298205       0.011418             -0.048035                  -0.013628\n",
      "freight_value                               0.230137         0.267724         0.070631   -0.031276      -0.002930  0.397203       1.000000             0.010814                    0.095370            0.021218          0.619779           0.318175           0.395066          0.322149                0.147544             0.387347                  0.200667       0.002480             -0.016032                  -0.050094\n",
      "product_name_lenght                        -0.002643         0.010754         0.008685   -0.005950      -0.000208  0.010826       0.010814             1.000000                    0.096362            0.139280          0.013846           0.042195          -0.027161          0.057028               -0.031519             0.005388                  0.030048       0.012477              0.066203                  -0.015235\n",
      "product_description_lenght                  0.024913         0.040220         0.019450    0.002488       0.008972  0.203839       0.095370             0.096362                    1.000000            0.110989          0.080193           0.026101           0.089622         -0.057006                0.057174             0.183002                  0.068933       0.017202             -0.016824                  -0.000606\n",
      "product_photos_qty                          0.019439         0.035102         0.004794   -0.018113      -0.041938  0.045801       0.021218             0.139280                    0.110989            1.000000          0.022353           0.038727          -0.042768          0.014635               -0.047356             0.017747                  0.028265       0.009471              0.001383                   0.006793\n",
      "product_weight_g                           -0.001004         0.001493        -0.007583   -0.012367       0.001829  0.357744       0.619779             0.013846                    0.080193            0.022353          1.000000           0.457371           0.583286          0.501546                0.004458             0.328457                  0.176960       0.004179              0.003909                  -0.035127\n",
      "product_length_cm                           0.010634         0.000578        -0.012891   -0.000781       0.068192  0.154958       0.318175             0.042195                    0.026101            0.038727          0.457371           1.000000           0.201756          0.511297                0.017917             0.200939                  0.147323       0.013729              0.018398                  -0.049406\n",
      "product_height_cm                           0.010078        -0.002581        -0.008259   -0.005108       0.051762  0.233589       0.395066            -0.027161                    0.089622           -0.042768          0.583286           0.201756           1.000000          0.279064                0.014638             0.237978                  0.123094      -0.011133             -0.030847                  -0.035764\n",
      "product_width_cm                           -0.028543        -0.029676        -0.012075   -0.007496      -0.024374  0.175308       0.322149             0.057028                   -0.057006            0.014635          0.501546           0.511297           0.279064          1.000000               -0.023015             0.141443                  0.131071       0.029566              0.088317                  -0.001738\n",
      "seller_zip_code_prefix                      0.070782        -0.032522        -0.021762    0.048534      -0.005437  0.073792       0.147544            -0.031519                    0.057174           -0.047356          0.004458           0.017917           0.014638         -0.023015                1.000000             0.069138                  0.057798      -0.008886             -0.052583                   0.014069\n",
      "payment_value_total                         0.047757         0.085453         0.036664    0.016667       0.257934  0.791285       0.387347             0.005388                    0.183002            0.017747          0.328457           0.200939           0.237978          0.141443                0.069138             1.000000                  0.277456       0.005832             -0.040692                  -0.095713\n",
      "payment_installments_max                    0.065098         0.059068         0.029819   -0.004632       0.070963  0.298205       0.200667             0.030048                    0.068933            0.028265          0.176960           0.147323           0.123094          0.131071                0.057798             0.277456                  1.000000      -0.042488             -0.022269                  -0.066786\n",
      "payment_count                               0.001930         0.012950         0.007939   -0.005720      -0.016215  0.011418       0.002480             0.012477                    0.017202            0.009471          0.004179           0.013729          -0.011133          0.029566               -0.008886             0.005832                 -0.042488       1.000000              0.011722                  -0.001229\n",
      "review_count_product                        0.000585         0.016956         0.016562   -0.020561      -0.012781 -0.048035      -0.016032             0.066203                   -0.016824            0.001383          0.003909           0.018398          -0.030847          0.088317               -0.052583            -0.040692                 -0.022269       0.011722              1.000000                   0.018501\n",
      "review_score_mean_product                  -0.029763        -0.036610        -0.038726   -0.021148      -0.140012 -0.013628      -0.050094            -0.015235                   -0.000606            0.006793         -0.035127          -0.049406          -0.035764         -0.001738                0.014069            -0.095713                 -0.066786      -0.001229              0.018501                   1.000000\n",
      "\n",
      "=== Categorical Values (Top 20 per column) ===\n",
      "     column                            value  count  percent\n",
      "   order_id 71dab1155600756af6de79de92e712e3     11 0.100000\n",
      "   order_id 6c355e2913545fa6f72c40cbca57729e     11 0.100000\n",
      "   order_id c52c7fbe316b5b9d549e8a25206b8a1f      9 0.080000\n",
      "   order_id ad850e69fce9a512ada84086651a2e7d      7 0.060000\n",
      "   order_id df36d28de846593cdbc153e28f195cd5      7 0.060000\n",
      "   order_id 7d316b369d4c6b0a4ebeebbff5f65466      6 0.050000\n",
      "   order_id 0b0809b2b64511c62fb745fb8b465cc9      6 0.050000\n",
      "   order_id 7e7c209ab4d09fe584822fd6c054023d      6 0.050000\n",
      "   order_id 9c4f3693a36ba481e0d9da739679660c      6 0.050000\n",
      "   order_id 55298973d17a0c1dfb2281ae969a65d8      6 0.050000\n",
      "   order_id 58e7140d9d368a70076dc999a7a7bf85      6 0.050000\n",
      "   order_id 5efc0b7fe9df7f0c567404abaa4d25fc      6 0.050000\n",
      "   order_id 7af905809ddbaaed2af20592006857e0      6 0.050000\n",
      "   order_id 62ce4e3989a3477928510bb4d1064cc4      6 0.050000\n",
      "   order_id 17ca01c238bad17cd6c09179888b6d7f      6 0.050000\n",
      "   order_id 8adafb3466daa5395694d3a906ff9d40      6 0.050000\n",
      "   order_id 7ff9920174c1d74237a6fcb73299c0a0      6 0.050000\n",
      "   order_id f91234b89ca8c758b3d2be234acb0355      6 0.050000\n",
      "   order_id 055cf2a6f9343f99acdb5e8f70e49bd8      6 0.050000\n",
      "   order_id 3304c0c857a9c77a201a551f5a3cacb8      6 0.050000\n",
      "customer_id 8c20d9bfbc96c5d39025d77a3ba83d7f     11 0.100000\n",
      "customer_id d95ca02ab50105ccce682bdf9ffdc3b4     11 0.100000\n",
      "customer_id 88324c93ce11436ae046563bf0da285c      9 0.080000\n",
      "customer_id 30bb84b541c96af98ba7d90b9ebf35d0      7 0.060000\n",
      "customer_id 2f6d72365267ce6048d9bbc1a9bd977e      7 0.060000\n",
      "\n",
      "=== Unique Counts Per Column ===\n",
      "                       column  unique_values\n",
      "                     order_id          10000\n",
      "                  customer_id          10000\n",
      "                 order_status              7\n",
      "     order_purchase_timestamp           9997\n",
      "            order_approved_at           9893\n",
      " order_delivered_carrier_date           9526\n",
      "order_delivered_customer_date           9679\n",
      "order_estimated_delivery_date            421\n",
      "           customer_unique_id           9969\n",
      "     customer_zip_code_prefix           5931\n",
      "                customer_city           1621\n",
      "               customer_state             27\n",
      "              geolocation_lat           5907\n",
      "              geolocation_lng           5907\n",
      "                   geo_points            481\n",
      "                order_item_id             12\n",
      "                   product_id           6759\n",
      "                    seller_id           1631\n",
      "          shipping_limit_date           9905\n",
      "                        price           2000\n",
      "                freight_value           2766\n",
      "        product_category_name             70\n",
      "          product_name_lenght             63\n",
      "   product_description_lenght           1903\n",
      "           product_photos_qty             16\n",
      "\n",
      "=== Head (10 rows) ===\n",
      "                        order_id                      customer_id order_status order_purchase_timestamp   order_approved_at order_delivered_carrier_date order_delivered_customer_date order_estimated_delivery_date               customer_unique_id  customer_zip_code_prefix             customer_city customer_state  geolocation_lat  geolocation_lng  geo_points  order_item_id                       product_id                        seller_id shipping_limit_date      price  freight_value             product_category_name  product_name_lenght  product_description_lenght  product_photos_qty  product_weight_g  product_length_cm  product_height_cm  product_width_cm   product_category_name_english  seller_zip_code_prefix    seller_city seller_state  payment_value_total  payment_installments_max  payment_count  review_count_product  review_score_mean_product\n",
      "e481f51cbdc54678b7cc49136f2d6af7 9ef432eb6251297304e76186b10a928d    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15          2017-10-04 19:55:00           2017-10-10 21:25:13                    2017-10-18 7c396fd4830fd04220f754e42b4e5bff                      3149                 sao paulo             SP       -23.576983       -46.587161   24.000000       1.000000 87285b34884572647811a353c7ac498a 3504c0cb71d7fa48d967e0e4c94d59d9 2017-10-06 11:07:15  29.990000       8.720000             utilidades_domesticas            40.000000                  268.000000            4.000000        500.000000          19.000000           8.000000         13.000000                      housewares             9350.000000           maua           SP            38.710000                         1              3              2.000000                   4.000000\n",
      "5acce57f8d9dfd55fa48e212a641a69d 295ae9b35379e077273387ff64354b6f    delivered      2017-07-31 21:37:10 2017-08-02 02:56:02          2017-08-03 18:32:48           2017-08-08 21:24:41                    2017-08-22 f1f4f45c8602d0db1329eed1c8e935d4                     19780                     quata             SP       -22.247297       -50.700629  115.000000       1.000000 0cd9f302c8a5b076ffa5c3567c6705fd 85d9eb9ddc5d00ca9336a2219c97bb13 2017-08-08 02:56:02  27.900000      15.100000            informatica_acessorios            22.000000                  716.000000            2.000000        200.000000          36.000000           2.000000         28.000000           computers_accessories            31255.000000 belo horizonte           MG            43.000000                         1              1              1.000000                   5.000000\n",
      "138849fd84dff2fb4ca70a0a34c4aa1c 9b18f3fc296990b97854e351334a32f6    delivered      2018-02-01 14:02:19 2018-02-03 02:53:07          2018-02-06 19:13:26           2018-02-14 13:41:59                    2018-02-23 b2cac0b16835dabf811b204127f58afa                      6330               carapicuiba             SP       -23.555518       -46.828339  206.000000       1.000000 304fad8dc4d2012dc4062839972f2d96 6860153b69cc696d5dcfe1cdaaafcf62 2018-02-08 02:53:07  39.470000      13.370000 construcao_ferramentas_construcao            59.000000                 1775.000000            2.000000       1700.000000          16.000000          11.000000         11.000000 construction_tools_construction            13360.000000       capivari           SP            52.840000                         1              1              1.000000                   5.000000\n",
      "e425680f760cbc130be3e53a9773c584 f178c1827f67a8467b0385b7378d951a    delivered      2017-08-31 08:15:24 2017-08-31 08:30:17          2017-08-31 20:06:14           2017-09-04 20:59:55                    2017-09-20 9d9ab3b77f0416765b3fbedf94a942a4                     12070                   taubate             SP       -23.017586       -45.541455  195.000000       1.000000 9ecadb84c81da840dbf3564378b586e9 1025f0e2d44d7041d6cf58b6550e0bfa 2017-09-08 08:30:17  38.400000      11.850000                  moveis_decoracao            41.000000                  789.000000            1.000000        950.000000          20.000000          35.000000         20.000000                 furniture_decor             3204.000000      sao paulo           SP            50.250000                         1              1              2.000000                   4.000000\n",
      "2edfd6d1f0b4cd0db4bf37b1b224d855 241e78de29b3090cfa1b5d73a8130c72    delivered      2017-06-13 21:11:26 2017-06-15 03:05:45          2017-06-16 14:55:37           2017-06-19 18:51:28                    2017-07-06 c63e44efa43f3947087aee96b388d949                      4658                 sao paulo             SP       -23.667958       -46.667300   76.000000       1.000000 30469bb5ea377eae7121981e2f0778e4 80e6699fe29150b372a0c8a1ebf7dcc8 2017-06-21 03:05:45 113.000000      28.150000                     esporte_lazer            57.000000                  574.000000            4.000000       5950.000000          20.000000          30.000000         80.000000                  sports_leisure            83323.000000        pinhais           PR           141.150000                         1              1              3.000000                   4.666667\n",
      "bd4bd0194d6d29f83b8557d4b89b572a 636e15840ab051faa13d3f781b6e4233    delivered      2018-07-28 16:52:55 2018-07-31 03:50:24          2018-08-01 16:01:00           2018-08-06 18:44:46                    2018-08-08 65e5aaf9f721945f29cdba45c206cb83                     14090            ribeirao preto             SP       -21.179893       -47.788429  252.000000       1.000000 7f457254a89d62960399e075711b3deb ea8482cd71df3c1969d7b9473ff13abc 2018-08-02 03:50:24  24.990000      12.840000                        automotivo            60.000000                  558.000000            6.000000        300.000000          17.000000           4.000000         12.000000                            auto             4160.000000      sao paulo           SP            37.830000                         1              1              3.000000                   3.666667\n",
      "0a4a2fccb27bd83a892fa503987a595b 6772a0a230a2667d16c3620f000e1348    delivered      2017-04-20 20:42:44 2017-04-20 20:55:09          2017-04-25 08:23:08           2017-05-11 13:07:46                    2017-05-25 c7a9a76a4b24a7e7b2caff982409b7ee                     58600               santa luzia             PB        -6.872212       -36.917456   33.000000       1.000000 f7d7b5c58704fb359a74580622800051 4a3ca9315b744ce9f8e9374361493884 2017-04-28 20:55:09  38.500000      24.840000                   cama_mesa_banho            53.000000                  223.000000            1.000000        950.000000          45.000000          15.000000         35.000000                  bed_bath_table            14940.000000       ibitinga           SP            63.340000                         5              1              1.000000                   4.000000\n",
      "d3d6788577c9592da441752e8a1dd5e3 8628fac2267e8c8804525da99c10ed0e    delivered      2017-09-19 22:17:15 2017-09-20 07:55:14          2017-09-22 17:23:09           2017-10-10 18:43:53                    2017-10-13 7973a6ba9c81ecaeb3d628c33c7c7c48                     85555                    palmas             PR       -26.485010       -51.990062  192.000000       1.000000 7c1bd920dbdf22470b68bde975dd3ccf cc419e0650a3c5ba77189a1882b7556a 2017-09-27 07:55:14  58.990000      17.660000                      beleza_saude            59.000000                  492.000000            2.000000        200.000000          22.000000          10.000000         18.000000                   health_beauty             9015.000000    santo andre           SP            76.650000                         7              1             15.000000                   3.533333\n",
      "86f21bf63784876b9fd6d35f46581d72 332df68ccac2f2f7d9e11299188f8bce    delivered      2018-04-11 22:32:31 2018-04-11 22:49:32          2018-04-14 00:02:39           2018-04-27 23:14:42                    2018-05-21 bb7ef994cc22b1fc694ac59fb377b824                     39135     presidente kubitschek             MG       -18.618915       -43.573928   10.000000       1.000000 5526b1ae9ab2688cf600783cece249df 0b90b6df587eb83608a64ea8b390cf07 2018-04-23 22:49:32  98.440000      22.400000            informatica_acessorios            49.000000                  385.000000            1.000000        200.000000          16.000000          16.000000         16.000000           computers_accessories            87025.000000        maringa           PR           120.840000                         4              1              4.000000                   3.000000\n",
      "0760a852e4e9d89eb77bf631eaaf1c84 d2a79636084590b7465af8ab374a8cf5     invoiced      2018-08-03 17:44:42 2018-08-07 06:15:14                          NaT                           NaT                    2018-08-21 c7f8d7b1fffc946d7069574f74c39f4e                     88140 santo amaro da imperatriz             SC       -27.687415       -48.771889  115.000000       1.000000 1522589c64efd46731d3522568e5bc83 28405831a29823802aa22c084cfd0649 2018-08-13 06:15:14  35.000000      15.350000                  artigos_de_natal            35.000000                  415.000000            4.000000        550.000000          37.000000          10.000000         37.000000              christmas_supplies             3644.000000      sao paulo           SP            50.350000                         1              1              1.000000                   3.000000\n",
      "\n",
      "=== End of EDA Report ===\n",
      "=== Robust EDA Report: sales ===\n",
      "\n",
      "=== Info ===\n",
      " rows  columns  memory_bytes\n",
      "10000       16       4287700\n",
      "\n",
      "=== Dtypes ===\n",
      "       column   dtype\n",
      " Critic_Score float64\n",
      "    Developer  object\n",
      "  ESRB_Rating  object\n",
      "        Genre  object\n",
      " Global_Sales float64\n",
      "     JP_Sales float64\n",
      "     NA_Sales float64\n",
      "         Name  object\n",
      "  Other_Sales float64\n",
      "    PAL_Sales float64\n",
      "     Platform  object\n",
      "    Publisher  object\n",
      "         Rank   int64\n",
      "Total_Shipped float64\n",
      "   User_Score float64\n",
      "         Year float64\n",
      "\n",
      "=== Missing Values ===\n",
      "       column  missing_count  missing_percent\n",
      "   User_Score           9951        99.510000\n",
      "Total_Shipped           9687        96.870000\n",
      " Critic_Score           8868        88.680000\n",
      "     JP_Sales           8797        87.970000\n",
      "     NA_Sales           7681        76.810000\n",
      "    PAL_Sales           7651        76.510000\n",
      "  Other_Sales           7235        72.350000\n",
      " Global_Sales           6584        65.840000\n",
      "  ESRB_Rating           5805        58.050000\n",
      "         Year            184         1.840000\n",
      "    Developer              6         0.060000\n",
      "         Rank              0         0.000000\n",
      "         Name              0         0.000000\n",
      "        Genre              0         0.000000\n",
      "     Platform              0         0.000000\n",
      "    Publisher              0         0.000000\n",
      "\n",
      "=== Duplicates Summary ===\n",
      " duplicate_rows\n",
      "              0\n",
      "\n",
      "=== Duplicates Preview (up to 20 rows) ===\n",
      "(No duplicate rows found.)\n",
      "\n",
      "=== Numeric Summary (5%..95%) ===\n",
      "       column        count         mean          std         min          5%          25%          50%          75%          95%          max\n",
      "         Rank 10000.000000 28282.991200 16060.599677    6.000000 3029.850000 14369.750000 28577.500000 42212.000000 53129.150000 55783.000000\n",
      " Critic_Score  1132.000000     7.166254     1.462889    2.000000    4.300000     6.400000     7.500000     8.200000     9.100000    10.000000\n",
      "   User_Score    49.000000     8.318367     1.291910    2.000000    6.660000     8.000000     8.500000     9.200000     9.660000    10.000000\n",
      "Total_Shipped   313.000000     1.727827     3.602234    0.080000    0.110000     0.220000     0.590000     1.760000     6.708000    31.380000\n",
      " Global_Sales  3416.000000     0.354359     0.848933    0.000000    0.000000     0.030000     0.110000     0.340000     1.390000    16.150000\n",
      "     NA_Sales  2319.000000     0.261328     0.512168    0.000000    0.010000     0.050000     0.110000     0.270000     1.000000     9.060000\n",
      "    PAL_Sales  2349.000000     0.149515     0.395121    0.000000    0.000000     0.010000     0.040000     0.130000     0.610000     6.210000\n",
      "     JP_Sales  1203.000000     0.110889     0.189141    0.000000    0.000000     0.020000     0.040000     0.130000     0.438000     2.170000\n",
      "  Other_Sales  2765.000000     0.043089     0.122784    0.000000    0.000000     0.000000     0.010000     0.030000     0.188000     2.260000\n",
      "         Year  9816.000000  2005.634780     8.412298 1970.000000 1991.000000  2000.000000  2008.000000  2011.000000  2017.000000  2020.000000\n",
      "\n",
      "=== Skew and Kurtosis ===\n",
      "       column      skew   kurtosis\n",
      "         Rank -0.029666  -1.203515\n",
      " Critic_Score -0.846155   0.443551\n",
      "   User_Score -2.620438  11.100775\n",
      "Total_Shipped  5.448402  36.652629\n",
      " Global_Sales  9.062350 126.844127\n",
      "     NA_Sales  7.536832  91.867361\n",
      "    PAL_Sales  7.902172  87.505177\n",
      "     JP_Sales  4.911197  37.500071\n",
      "  Other_Sales  8.054551  91.500076\n",
      "         Year -0.754684   0.362760\n",
      "\n",
      "=== IQR Outlier Counts ===\n",
      "       column  outlier_count\n",
      "         Rank              0\n",
      " Critic_Score             29\n",
      "   User_Score              2\n",
      "Total_Shipped             26\n",
      " Global_Sales            349\n",
      "     NA_Sales            218\n",
      "    PAL_Sales            270\n",
      "     JP_Sales            113\n",
      "  Other_Sales            342\n",
      "         Year            142\n",
      "\n",
      "=== Correlation (first 30 numeric columns) ===\n",
      "                   Rank  Critic_Score  User_Score  Total_Shipped  Global_Sales  NA_Sales  PAL_Sales  JP_Sales  Other_Sales      Year\n",
      "column                                                                                                                              \n",
      "Rank           1.000000     -0.156397   -0.278770      -0.466286     -0.537923 -0.536338  -0.440430 -0.441845    -0.447695 -0.107046\n",
      "Critic_Score  -0.156397      1.000000    0.673779       0.364024      0.312280  0.335187   0.273022  0.206843     0.285556  0.004554\n",
      "User_Score    -0.278770      0.673779    1.000000       0.188085      0.395085  0.436740   0.486396  0.174519     0.458343 -0.280859\n",
      "Total_Shipped -0.466286      0.364024    0.188085       1.000000           NaN       NaN        NaN       NaN          NaN -0.285192\n",
      "Global_Sales  -0.537923      0.312280    0.395085            NaN      1.000000  0.936823   0.930517  0.211327     0.903602 -0.038429\n",
      "NA_Sales      -0.536338      0.335187    0.436740            NaN      0.936823  1.000000   0.766901  0.062966     0.778130 -0.037921\n",
      "PAL_Sales     -0.440430      0.273022    0.486396            NaN      0.930517  0.766901   1.000000  0.097555     0.882607  0.078028\n",
      "JP_Sales      -0.441845      0.206843    0.174519            NaN      0.211327  0.062966   0.097555  1.000000     0.058907 -0.374985\n",
      "Other_Sales   -0.447695      0.285556    0.458343            NaN      0.903602  0.778130   0.882607  0.058907     1.000000  0.079306\n",
      "Year          -0.107046      0.004554   -0.280859      -0.285192     -0.038429 -0.037921   0.078028 -0.374985     0.079306  1.000000\n",
      "\n",
      "=== Categorical Values (Top 20 per column) ===\n",
      "column                                     value  count   percent\n",
      "  Name                                   FIFA 14      5  0.050000\n",
      "  Name                           Samurai Shodown      5  0.050000\n",
      "  Name                           LEGO The Hobbit      5  0.050000\n",
      "  Name                                Pipe Mania      5  0.050000\n",
      "  Name Ben 10 Ultimate Alien: Cosmic Destruction      4  0.040000\n",
      "  Name                    007: Quantum of Solace      4  0.040000\n",
      "  Name                        A Boy and His Blob      4  0.040000\n",
      "  Name        Cabela's North American Adventures      4  0.040000\n",
      "  Name                           Resident Evil 2      4  0.040000\n",
      "  Name                               The Swapper      4  0.040000\n",
      "  Name                                 Syndicate      4  0.040000\n",
      "  Name                               Angry Birds      4  0.040000\n",
      "  Name                                 Jeopardy!      4  0.040000\n",
      "  Name                Avatar: The Last Airbender      4  0.040000\n",
      "  Name                                 Minecraft      4  0.040000\n",
      "  Name                        Shin Megami Tensei      4  0.040000\n",
      "  Name             Grand Theft Auto: San Andreas      4  0.040000\n",
      "  Name                                   Othello      4  0.040000\n",
      "  Name                              Battlefied 2      4  0.040000\n",
      "  Name                          Mortal Kombat II      4  0.040000\n",
      " Genre                                      Misc   1698 16.980000\n",
      " Genre                                    Action   1374 13.740000\n",
      " Genre                                 Adventure    949  9.490000\n",
      " Genre                                    Sports    940  9.400000\n",
      " Genre                                   Shooter    822  8.220000\n",
      "\n",
      "=== Unique Counts Per Column ===\n",
      "       column  unique_values\n",
      "         Rank          10000\n",
      "         Name           9052\n",
      "        Genre             20\n",
      "  ESRB_Rating              9\n",
      "     Platform             64\n",
      "    Publisher           1431\n",
      "    Developer           3277\n",
      " Critic_Score             73\n",
      "   User_Score             25\n",
      "Total_Shipped            168\n",
      " Global_Sales            265\n",
      "     NA_Sales            178\n",
      "    PAL_Sales            143\n",
      "     JP_Sales             84\n",
      "  Other_Sales             74\n",
      "         Year             45\n",
      "\n",
      "=== Head (10 rows) ===\n",
      " Rank                               Name            Genre ESRB_Rating Platform      Publisher      Developer  Critic_Score  User_Score  Total_Shipped  Global_Sales  NA_Sales  PAL_Sales  JP_Sales  Other_Sales        Year\n",
      "    6 Pokemon Red / Green / Blue Version     Role-Playing           E       GB       Nintendo     Game Freak      9.400000         NaN      31.380000           NaN       NaN        NaN       NaN          NaN 1998.000000\n",
      "   10                          Minecraft             Misc         NaN       PC         Mojang      Mojang AB     10.000000         NaN      30.010000           NaN       NaN        NaN       NaN          NaN 2010.000000\n",
      "   11                          Duck Hunt          Shooter         NaN      NES       Nintendo  Nintendo R&D1           NaN         NaN      28.310000           NaN       NaN        NaN       NaN          NaN 1985.000000\n",
      "   28                Super Mario Bros. 3         Platform           E      NES       Nintendo  Nintendo R&D2           NaN         NaN      17.280000           NaN       NaN        NaN       NaN          NaN 1990.000000\n",
      "   31        Grand Theft Auto: Vice City           Action           M      PS2 Rockstar Games Rockstar North      9.600000         NaN            NaN     16.150000  8.410000   5.490000  0.470000     1.780000 2002.000000\n",
      "   33                 Grand Theft Auto V           Action           M     X360 Rockstar Games Rockstar North           NaN         NaN            NaN     15.860000  9.060000   5.330000  0.060000     1.420000 2013.000000\n",
      "   46              Red Dead Redemption 2 Action-Adventure           M      PS4 Rockstar Games Rockstar Games      9.800000         NaN            NaN     13.940000  5.260000   6.210000  0.210000     2.260000 2018.000000\n",
      "   52     Call of Duty: Modern Warfare 3          Shooter           M      PS3     Activision  Infinity Ward      8.800000         NaN            NaN     13.350000  5.540000   5.780000  0.490000     1.540000 2011.000000\n",
      "   53            Super Smash Bros. Brawl         Fighting           T      Wii       Nintendo   Project Sora      9.200000    9.700000      13.290000           NaN       NaN        NaN       NaN          NaN 2008.000000\n",
      "   54               Grand Theft Auto III           Action           M      PS2 Rockstar Games     DMA Design      9.500000         NaN            NaN     13.100000  6.990000   4.510000  0.300000     1.300000 2001.000000\n",
      "\n",
      "=== End of EDA Report ===\n"
     ]
    }
   ],
   "source": [
    "robust_eda(steam, name=\"steam\")\n",
    "robust_eda(olist, name=\"olist\")\n",
    "robust_eda(sales, name=\"sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3177e9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81e30164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] steam split: 0.01 s\n",
      "[info] steam post split | X_train shape=(8000, 22) mem=0.004 GB | X_test shape=(2000, 22) mem=0.001 GB\n",
      "[timer] steam datetime to numeric: 0.00 s\n",
      "[timer] steam text features: 0.10 s\n",
      "[info] steam after text | X_train shape=(8000, 24) mem=0.004 GB | X_test shape=(2000, 45) mem=0.001 GB\n",
      "[info] steam tags unique=422, kept=100 (min_count=5, top_k=100)\n",
      "[timer] steam tags multi-hot: 0.11 s\n",
      "[info] steam after tags | X_train shape=(8000, 123) mem=0.004 GB | X_test shape=(2000, 144) mem=0.001 GB\n",
      "[timer] steam OHE: 0.01 s\n",
      "[info] steam after OHE | X_train shape=(8000, 223) mem=0.003 GB | X_test shape=(2000, 223) mem=0.001 GB\n",
      "steam non-numeric after OHE: []\n",
      "[timer] steam impute: 0.03 s\n",
      "[info] steam after impute+downcast | X_train shape=(8000, 121) mem=0.004 GB | X_test shape=(2000, 125) mem=0.001 GB\n",
      "[timer] steam poly: 0.03 s\n",
      "[info] steam after poly | X_train shape=(8000, 128) mem=0.004 GB | X_test shape=(2000, 132) mem=0.001 GB\n",
      "[timer] steam outlier clip: 0.03 s\n",
      "[info] steam after outlier clip | X_train shape=(8000, 128) mem=0.004 GB | X_test shape=(2000, 132) mem=0.001 GB\n",
      "[timer] steam scale: 0.02 s\n",
      "[timer] steam select features: 0.38 s\n",
      "[info] steam after select | X_train shape=(8000, 50) mem=0.002 GB | X_test shape=(2000, 50) mem=0.000 GB\n",
      "[timer] olist split: 0.01 s\n",
      "[info] olist post split | X_train shape=(9155, 37) mem=0.007 GB | X_test shape=(2289, 37) mem=0.002 GB\n",
      "[timer] olist datetime to numeric: 0.01 s\n",
      "[timer] olist text features: 0.03 s\n",
      "[timer] olist OHE: 0.03 s\n",
      "[info] olist after OHE | X_train shape=(9155, 431) mem=0.006 GB | X_test shape=(2289, 431) mem=0.002 GB\n",
      "[timer] olist impute: 0.02 s\n",
      "[info] olist after impute+downcast | X_train shape=(9155, 37) mem=0.001 GB | X_test shape=(2289, 57) mem=0.001 GB\n",
      "[timer] olist poly: 0.01 s\n",
      "[info] olist after poly | X_train shape=(9155, 45) mem=0.002 GB | X_test shape=(2289, 65) mem=0.001 GB\n",
      "[timer] olist outlier clip: 0.03 s\n",
      "[info] olist after outlier clip | X_train shape=(9155, 45) mem=0.002 GB | X_test shape=(2289, 65) mem=0.001 GB\n",
      "[timer] olist scale: 0.01 s\n",
      "[timer] olist select features: 1.66 s\n",
      "[info] olist after select | X_train shape=(9155, 45) mem=0.002 GB | X_test shape=(2289, 45) mem=0.000 GB\n",
      "[timer] sales split: 0.01 s\n",
      "[info] sales post split | X_train shape=(8000, 15) mem=0.003 GB | X_test shape=(2000, 15) mem=0.001 GB\n",
      "[timer] sales datetime to numeric: 0.00 s\n",
      "[timer] sales text features: 0.04 s\n",
      "[timer] sales OHE: 0.02 s\n",
      "[info] sales after OHE | X_train shape=(8000, 196) mem=0.002 GB | X_test shape=(2000, 196) mem=0.001 GB\n",
      "[timer] sales impute: 0.01 s\n",
      "[info] sales after impute+downcast | X_train shape=(8000, 15) mem=0.001 GB | X_test shape=(2000, 18) mem=0.000 GB\n",
      "[timer] sales poly: 0.00 s\n",
      "[info] sales after poly | X_train shape=(8000, 20) mem=0.001 GB | X_test shape=(2000, 23) mem=0.000 GB\n",
      "[timer] sales outlier clip: 0.01 s\n",
      "[info] sales after outlier clip | X_train shape=(8000, 20) mem=0.001 GB | X_test shape=(2000, 23) mem=0.000 GB\n",
      "[timer] sales scale: 0.00 s\n",
      "[timer] sales select features: 0.37 s\n",
      "[info] sales after select | X_train shape=(8000, 20) mem=0.001 GB | X_test shape=(2000, 20) mem=0.000 GB\n",
      "[steam] X_train: (8000, 50) | X_test: (2000, 50) | y_train: (8000,) | y_test: (2000,)\n",
      "[olist] X_train: (9155, 45) | X_test: (2289, 45) | y_train: (9155,) | y_test: (2289,)\n",
      "[sales] X_train: (8000, 20) | X_test: (2000, 20) | y_train: (8000,) | y_test: (2000,)\n",
      "\n",
      "=== STEAM Dataset ===\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=5 | F1_macro=0.667344 ± 0.019751\n",
      "[2/40] GBT | k=13 | F1_macro=0.667344 ± 0.019751\n",
      "[3/40] GBT | k=25 | F1_macro=0.667344 ± 0.019751\n",
      "[4/40] GBT | k=38 | F1_macro=0.667344 ± 0.019751\n",
      "[5/40] GBT | k=50 | F1_macro=0.667344 ± 0.019751\n",
      "[6/40] RandomForest | k=5 | F1_macro=0.695528 ± 0.025943\n",
      "[7/40] RandomForest | k=13 | F1_macro=0.695528 ± 0.025943\n",
      "[8/40] RandomForest | k=25 | F1_macro=0.695528 ± 0.025943\n",
      "[9/40] RandomForest | k=38 | F1_macro=0.695528 ± 0.025943\n",
      "[10/40] RandomForest | k=50 | F1_macro=0.695528 ± 0.025943\n",
      "[11/40] DecisionTree | k=5 | F1_macro=0.656578 ± 0.017430\n",
      "[12/40] DecisionTree | k=13 | F1_macro=0.656578 ± 0.017430\n",
      "[13/40] DecisionTree | k=25 | F1_macro=0.656578 ± 0.017430\n",
      "[14/40] DecisionTree | k=38 | F1_macro=0.656578 ± 0.017430\n",
      "[15/40] DecisionTree | k=50 | F1_macro=0.656578 ± 0.017430\n",
      "[16/40] LogisticRegression | k=5 | F1_macro=0.501790 ± 0.008918\n",
      "[17/40] LogisticRegression | k=13 | F1_macro=0.498109 ± 0.006292\n",
      "[18/40] LogisticRegression | k=25 | F1_macro=0.511346 ± 0.007081\n",
      "[19/40] LogisticRegression | k=38 | F1_macro=0.524361 ± 0.006320\n",
      "[20/40] LogisticRegression | k=50 | F1_macro=0.537413 ± 0.007856\n",
      "[21/40] LinearSVM | k=5 | F1_macro=0.506491 ± 0.012062\n",
      "[22/40] LinearSVM | k=13 | F1_macro=0.499464 ± 0.005905\n",
      "[23/40] LinearSVM | k=25 | F1_macro=0.508519 ± 0.007116\n",
      "[24/40] LinearSVM | k=38 | F1_macro=0.520452 ± 0.005079\n",
      "[25/40] LinearSVM | k=50 | F1_macro=0.537006 ± 0.006168\n",
      "[26/40] NaiveBayes | k=5 | F1_macro=0.502300 ± 0.011743\n",
      "[27/40] NaiveBayes | k=13 | F1_macro=0.479883 ± 0.005373\n",
      "[28/40] NaiveBayes | k=25 | F1_macro=0.473967 ± 0.004368\n",
      "[29/40] NaiveBayes | k=38 | F1_macro=0.455375 ± 0.000917\n",
      "[30/40] NaiveBayes | k=50 | F1_macro=0.453811 ± 0.000666\n",
      "[31/40] KNN | k=5 | F1_macro=0.581701 ± 0.010939\n",
      "[32/40] KNN | k=13 | F1_macro=0.575243 ± 0.017975\n",
      "[33/40] KNN | k=25 | F1_macro=0.621644 ± 0.023751\n",
      "[34/40] KNN | k=38 | F1_macro=0.652851 ± 0.004489\n",
      "[35/40] KNN | k=50 | F1_macro=0.680104 ± 0.018443\n",
      "[36/40] Dummy | k=5 | F1_macro=0.012468 ± 0.000170\n",
      "[37/40] Dummy | k=13 | F1_macro=0.012468 ± 0.000170\n",
      "[38/40] Dummy | k=25 | F1_macro=0.012468 ± 0.000170\n",
      "[39/40] Dummy | k=38 | F1_macro=0.012468 ± 0.000170\n",
      "[40/40] Dummy | k=50 | F1_macro=0.012468 ± 0.000170\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "                 Model  K_features  MeanScore   StdDev    Metric\n",
      "0         RandomForest           5   0.695528 0.025943  F1_macro\n",
      "1         RandomForest          13   0.695528 0.025943  F1_macro\n",
      "2         RandomForest          25   0.695528 0.025943  F1_macro\n",
      "3         RandomForest          38   0.695528 0.025943  F1_macro\n",
      "4         RandomForest          50   0.695528 0.025943  F1_macro\n",
      "5                  KNN          50   0.680104 0.018443  F1_macro\n",
      "6                  GBT           5   0.667344 0.019751  F1_macro\n",
      "7                  GBT          13   0.667344 0.019751  F1_macro\n",
      "8                  GBT          25   0.667344 0.019751  F1_macro\n",
      "9                  GBT          38   0.667344 0.019751  F1_macro\n",
      "10                 GBT          50   0.667344 0.019751  F1_macro\n",
      "11        DecisionTree           5   0.656578 0.017430  F1_macro\n",
      "12        DecisionTree          13   0.656578 0.017430  F1_macro\n",
      "13        DecisionTree          25   0.656578 0.017430  F1_macro\n",
      "14        DecisionTree          38   0.656578 0.017430  F1_macro\n",
      "15        DecisionTree          50   0.656578 0.017430  F1_macro\n",
      "16                 KNN          38   0.652851 0.004489  F1_macro\n",
      "17                 KNN          25   0.621644 0.023751  F1_macro\n",
      "18                 KNN           5   0.581701 0.010939  F1_macro\n",
      "19                 KNN          13   0.575243 0.017975  F1_macro\n",
      "20  LogisticRegression          50   0.537413 0.007856  F1_macro\n",
      "21           LinearSVM          50   0.537006 0.006168  F1_macro\n",
      "22  LogisticRegression          38   0.524361 0.006320  F1_macro\n",
      "23           LinearSVM          38   0.520452 0.005079  F1_macro\n",
      "24  LogisticRegression          25   0.511346 0.007081  F1_macro\n",
      "25           LinearSVM          25   0.508519 0.007116  F1_macro\n",
      "26           LinearSVM           5   0.506491 0.012062  F1_macro\n",
      "27          NaiveBayes           5   0.502300 0.011743  F1_macro\n",
      "28  LogisticRegression           5   0.501790 0.008918  F1_macro\n",
      "29           LinearSVM          13   0.499464 0.005905  F1_macro\n",
      "30  LogisticRegression          13   0.498109 0.006292  F1_macro\n",
      "31          NaiveBayes          13   0.479883 0.005373  F1_macro\n",
      "32          NaiveBayes          25   0.473967 0.004368  F1_macro\n",
      "33          NaiveBayes          38   0.455375 0.000917  F1_macro\n",
      "34          NaiveBayes          50   0.453811 0.000666  F1_macro\n",
      "35               Dummy           5   0.012468 0.000170  F1_macro\n",
      "36               Dummy          13   0.012468 0.000170  F1_macro\n",
      "37               Dummy          25   0.012468 0.000170  F1_macro\n",
      "38               Dummy          38   0.012468 0.000170  F1_macro\n",
      "39               Dummy          50   0.012468 0.000170  F1_macro\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.6s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   2.7s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   2.8s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   3.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   4.6s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   4.5s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.9s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   4.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   6.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  11.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   6.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  11.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  11.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  12.0s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  12.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   5.9s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  10.9s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   3.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   3.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   4.1s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   3.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   3.8s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   3.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   4.6s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   3.3s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   4.6s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   3.1s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   5.1s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   2.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  19.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  20.0s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  21.0s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   2.7s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   3.3s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   6.3s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   2.5s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   6.2s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   6.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  17.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  19.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  19.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  15.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  16.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  14.8s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  17.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  15.0s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  15.7s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 5\n",
      "Best hyperparameters: {'model__n_estimators': 200, 'model__min_samples_split': 10, 'model__min_samples_leaf': 4, 'model__max_features': 'sqrt', 'model__max_depth': None}\n",
      "Best CV score (F1 macro): 0.724488\n",
      "\n",
      "=== Threshold tuning (OOF on train) ===\n",
      "Best threshold: 0.65 | F1_macro: 0.772422\n",
      "Confusion matrix at best threshold:\n",
      "[[  47   54]\n",
      " [  23 7876]]\n",
      "\n",
      "=== Holdout (time split) ===\n",
      "F1 macro: 0.722222\n",
      "Confusion matrix:\n",
      "[[   9   16]\n",
      " [   6 1969]]\n",
      "steam threshold: 0.65\n",
      "\n",
      "=== OLIST Dataset ===\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=5 | F1_macro=0.631967 ± 0.006646\n",
      "[2/40] GBT | k=12 | F1_macro=0.631967 ± 0.006646\n",
      "[3/40] GBT | k=23 | F1_macro=0.631967 ± 0.006646\n",
      "[4/40] GBT | k=34 | F1_macro=0.631967 ± 0.006646\n",
      "[5/40] GBT | k=45 | F1_macro=0.631967 ± 0.006646\n",
      "[6/40] RandomForest | k=5 | F1_macro=0.716258 ± 0.016644\n",
      "[7/40] RandomForest | k=12 | F1_macro=0.716258 ± 0.016644\n",
      "[8/40] RandomForest | k=23 | F1_macro=0.716258 ± 0.016644\n",
      "[9/40] RandomForest | k=34 | F1_macro=0.716258 ± 0.016644\n",
      "[10/40] RandomForest | k=45 | F1_macro=0.716258 ± 0.016644\n",
      "[11/40] DecisionTree | k=5 | F1_macro=0.667263 ± 0.017115\n",
      "[12/40] DecisionTree | k=12 | F1_macro=0.667263 ± 0.017115\n",
      "[13/40] DecisionTree | k=23 | F1_macro=0.667263 ± 0.017115\n",
      "[14/40] DecisionTree | k=34 | F1_macro=0.667263 ± 0.017115\n",
      "[15/40] DecisionTree | k=45 | F1_macro=0.667263 ± 0.017115\n",
      "[16/40] LogisticRegression | k=5 | F1_macro=0.494973 ± 0.004448\n",
      "[17/40] LogisticRegression | k=12 | F1_macro=0.523947 ± 0.004448\n",
      "[18/40] LogisticRegression | k=23 | F1_macro=0.544734 ± 0.005420\n",
      "[19/40] LogisticRegression | k=34 | F1_macro=0.570101 ± 0.003380\n",
      "[20/40] LogisticRegression | k=45 | F1_macro=0.571611 ± 0.004678\n",
      "[21/40] LinearSVM | k=5 | F1_macro=0.494564 ± 0.004093\n",
      "[22/40] LinearSVM | k=12 | F1_macro=0.519936 ± 0.004618\n",
      "[23/40] LinearSVM | k=23 | F1_macro=0.539818 ± 0.004940\n",
      "[24/40] LinearSVM | k=34 | F1_macro=0.563748 ± 0.002549\n",
      "[25/40] LinearSVM | k=45 | F1_macro=0.566177 ± 0.004552\n",
      "[26/40] NaiveBayes | k=5 | F1_macro=0.521111 ± 0.003186\n",
      "[27/40] NaiveBayes | k=12 | F1_macro=0.520893 ± 0.003356\n",
      "[28/40] NaiveBayes | k=23 | F1_macro=0.520873 ± 0.003937\n",
      "[29/40] NaiveBayes | k=34 | F1_macro=0.521249 ± 0.003444\n",
      "[30/40] NaiveBayes | k=45 | F1_macro=0.520462 ± 0.003246\n",
      "[31/40] KNN | k=5 | F1_macro=0.521111 ± 0.003186\n",
      "[32/40] KNN | k=12 | F1_macro=0.542029 ± 0.008372\n",
      "[33/40] KNN | k=23 | F1_macro=0.563286 ± 0.002789\n",
      "[34/40] KNN | k=34 | F1_macro=0.576815 ± 0.012564\n",
      "[35/40] KNN | k=45 | F1_macro=0.589551 ± 0.009694\n",
      "[36/40] Dummy | k=5 | F1_macro=0.109522 ± 0.000116\n",
      "[37/40] Dummy | k=12 | F1_macro=0.109522 ± 0.000116\n",
      "[38/40] Dummy | k=23 | F1_macro=0.109522 ± 0.000116\n",
      "[39/40] Dummy | k=34 | F1_macro=0.109522 ± 0.000116\n",
      "[40/40] Dummy | k=45 | F1_macro=0.109522 ± 0.000116\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "                 Model  K_features  MeanScore   StdDev    Metric\n",
      "0         RandomForest           5   0.716258 0.016644  F1_macro\n",
      "1         RandomForest          12   0.716258 0.016644  F1_macro\n",
      "2         RandomForest          23   0.716258 0.016644  F1_macro\n",
      "3         RandomForest          34   0.716258 0.016644  F1_macro\n",
      "4         RandomForest          45   0.716258 0.016644  F1_macro\n",
      "5         DecisionTree           5   0.667263 0.017115  F1_macro\n",
      "6         DecisionTree          12   0.667263 0.017115  F1_macro\n",
      "7         DecisionTree          23   0.667263 0.017115  F1_macro\n",
      "8         DecisionTree          34   0.667263 0.017115  F1_macro\n",
      "9         DecisionTree          45   0.667263 0.017115  F1_macro\n",
      "10                 GBT           5   0.631967 0.006646  F1_macro\n",
      "11                 GBT          12   0.631967 0.006646  F1_macro\n",
      "12                 GBT          23   0.631967 0.006646  F1_macro\n",
      "13                 GBT          34   0.631967 0.006646  F1_macro\n",
      "14                 GBT          45   0.631967 0.006646  F1_macro\n",
      "15                 KNN          45   0.589551 0.009694  F1_macro\n",
      "16                 KNN          34   0.576815 0.012564  F1_macro\n",
      "17  LogisticRegression          45   0.571611 0.004678  F1_macro\n",
      "18  LogisticRegression          34   0.570101 0.003380  F1_macro\n",
      "19           LinearSVM          45   0.566177 0.004552  F1_macro\n",
      "20           LinearSVM          34   0.563748 0.002549  F1_macro\n",
      "21                 KNN          23   0.563286 0.002789  F1_macro\n",
      "22  LogisticRegression          23   0.544734 0.005420  F1_macro\n",
      "23                 KNN          12   0.542029 0.008372  F1_macro\n",
      "24           LinearSVM          23   0.539818 0.004940  F1_macro\n",
      "25  LogisticRegression          12   0.523947 0.004448  F1_macro\n",
      "26          NaiveBayes          34   0.521249 0.003444  F1_macro\n",
      "27                 KNN           5   0.521111 0.003186  F1_macro\n",
      "28          NaiveBayes           5   0.521111 0.003186  F1_macro\n",
      "29          NaiveBayes          12   0.520893 0.003356  F1_macro\n",
      "30          NaiveBayes          23   0.520873 0.003937  F1_macro\n",
      "31          NaiveBayes          45   0.520462 0.003246  F1_macro\n",
      "32           LinearSVM          12   0.519936 0.004618  F1_macro\n",
      "33  LogisticRegression           5   0.494973 0.004448  F1_macro\n",
      "34           LinearSVM           5   0.494564 0.004093  F1_macro\n",
      "35               Dummy           5   0.109522 0.000116  F1_macro\n",
      "36               Dummy          12   0.109522 0.000116  F1_macro\n",
      "37               Dummy          23   0.109522 0.000116  F1_macro\n",
      "38               Dummy          34   0.109522 0.000116  F1_macro\n",
      "39               Dummy          45   0.109522 0.000116  F1_macro\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   3.5s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   3.5s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   3.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   3.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   3.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   3.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   7.7s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   7.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   7.9s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  11.9s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  12.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   4.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  11.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   4.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   3.9s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  21.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  21.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=  12.1s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  20.8s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   3.5s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=  12.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  39.2s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   3.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  39.6s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   3.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  40.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  40.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  41.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  39.0s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=  11.3s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  11.2s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   8.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  11.2s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  11.5s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   8.0s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  13.3s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  13.5s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  13.7s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   7.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   4.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   4.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time= 1.1min\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time= 1.2min\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time= 1.2min\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   7.0s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   7.1s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  20.9s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   6.3s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  20.8s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  18.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time= 1.0min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time= 1.0min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time= 1.0min\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  54.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  54.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  53.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  55.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  51.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  52.5s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 5\n",
      "Best hyperparameters: {'model__n_estimators': 200, 'model__min_samples_split': 10, 'model__min_samples_leaf': 4, 'model__max_features': 'sqrt', 'model__max_depth': None}\n",
      "Best CV score (F1 macro): 0.732914\n",
      "\n",
      "=== Threshold tuning (OOF on train) ===\n",
      "Best threshold: 0.55 | F1_macro: 0.739200\n",
      "Confusion matrix at best threshold:\n",
      "[[ 477  649]\n",
      " [ 198 7831]]\n",
      "\n",
      "=== Holdout (time split) ===\n",
      "F1 macro: 0.741824\n",
      "Confusion matrix:\n",
      "[[ 127  155]\n",
      " [  64 1943]]\n",
      "olist threshold: 0.5499999999999999\n",
      "\n",
      "=== SALES Dataset ===\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=2 | F1_macro=0.659087 ± 0.002475\n",
      "[2/40] GBT | k=5 | F1_macro=0.659087 ± 0.002475\n",
      "[3/40] GBT | k=10 | F1_macro=0.659087 ± 0.002475\n",
      "[4/40] GBT | k=15 | F1_macro=0.659087 ± 0.002475\n",
      "[5/40] GBT | k=20 | F1_macro=0.659087 ± 0.002475\n",
      "[6/40] RandomForest | k=2 | F1_macro=0.688351 ± 0.013791\n",
      "[7/40] RandomForest | k=5 | F1_macro=0.688351 ± 0.013791\n",
      "[8/40] RandomForest | k=10 | F1_macro=0.688351 ± 0.013791\n",
      "[9/40] RandomForest | k=15 | F1_macro=0.688351 ± 0.013791\n",
      "[10/40] RandomForest | k=20 | F1_macro=0.688351 ± 0.013791\n",
      "[11/40] DecisionTree | k=2 | F1_macro=0.646293 ± 0.025687\n",
      "[12/40] DecisionTree | k=5 | F1_macro=0.646293 ± 0.025687\n",
      "[13/40] DecisionTree | k=10 | F1_macro=0.646293 ± 0.025687\n",
      "[14/40] DecisionTree | k=15 | F1_macro=0.646293 ± 0.025687\n",
      "[15/40] DecisionTree | k=20 | F1_macro=0.646293 ± 0.025687\n",
      "[16/40] LogisticRegression | k=2 | F1_macro=0.599335 ± 0.009849\n",
      "[17/40] LogisticRegression | k=5 | F1_macro=0.601998 ± 0.002358\n",
      "[18/40] LogisticRegression | k=10 | F1_macro=0.601563 ± 0.003573\n",
      "[19/40] LogisticRegression | k=15 | F1_macro=0.611682 ± 0.000530\n",
      "[20/40] LogisticRegression | k=20 | F1_macro=0.612326 ± 0.000726\n",
      "[21/40] LinearSVM | k=2 | F1_macro=0.589561 ± 0.005583\n",
      "[22/40] LinearSVM | k=5 | F1_macro=0.588978 ± 0.003705\n",
      "[23/40] LinearSVM | k=10 | F1_macro=0.589746 ± 0.003369\n",
      "[24/40] LinearSVM | k=15 | F1_macro=0.596383 ± 0.001574\n",
      "[25/40] LinearSVM | k=20 | F1_macro=0.600003 ± 0.000880\n",
      "[26/40] NaiveBayes | k=2 | F1_macro=0.599500 ± 0.010954\n",
      "[27/40] NaiveBayes | k=5 | F1_macro=0.676032 ± 0.007006\n",
      "[28/40] NaiveBayes | k=10 | F1_macro=0.631334 ± 0.015377\n",
      "[29/40] NaiveBayes | k=15 | F1_macro=0.633015 ± 0.012474\n",
      "[30/40] NaiveBayes | k=20 | F1_macro=0.632318 ± 0.013540\n",
      "[31/40] KNN | k=2 | F1_macro=0.629147 ± 0.003008\n",
      "[32/40] KNN | k=5 | F1_macro=0.627883 ± 0.006144\n",
      "[33/40] KNN | k=10 | F1_macro=0.635715 ± 0.007389\n",
      "[34/40] KNN | k=15 | F1_macro=0.649841 ± 0.008637\n",
      "[35/40] KNN | k=20 | F1_macro=0.648425 ± 0.013897\n",
      "[36/40] Dummy | k=2 | F1_macro=0.473060 ± 0.000044\n",
      "[37/40] Dummy | k=5 | F1_macro=0.473060 ± 0.000044\n",
      "[38/40] Dummy | k=10 | F1_macro=0.473060 ± 0.000044\n",
      "[39/40] Dummy | k=15 | F1_macro=0.473060 ± 0.000044\n",
      "[40/40] Dummy | k=20 | F1_macro=0.473060 ± 0.000044\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "                 Model  K_features  MeanScore   StdDev    Metric\n",
      "0         RandomForest           2   0.688351 0.013791  F1_macro\n",
      "1         RandomForest           5   0.688351 0.013791  F1_macro\n",
      "2         RandomForest          10   0.688351 0.013791  F1_macro\n",
      "3         RandomForest          15   0.688351 0.013791  F1_macro\n",
      "4         RandomForest          20   0.688351 0.013791  F1_macro\n",
      "5           NaiveBayes           5   0.676032 0.007006  F1_macro\n",
      "6                  GBT           2   0.659087 0.002475  F1_macro\n",
      "7                  GBT           5   0.659087 0.002475  F1_macro\n",
      "8                  GBT          10   0.659087 0.002475  F1_macro\n",
      "9                  GBT          15   0.659087 0.002475  F1_macro\n",
      "10                 GBT          20   0.659087 0.002475  F1_macro\n",
      "11                 KNN          15   0.649841 0.008637  F1_macro\n",
      "12                 KNN          20   0.648425 0.013897  F1_macro\n",
      "13        DecisionTree           2   0.646293 0.025687  F1_macro\n",
      "14        DecisionTree           5   0.646293 0.025687  F1_macro\n",
      "15        DecisionTree          10   0.646293 0.025687  F1_macro\n",
      "16        DecisionTree          15   0.646293 0.025687  F1_macro\n",
      "17        DecisionTree          20   0.646293 0.025687  F1_macro\n",
      "18                 KNN          10   0.635715 0.007389  F1_macro\n",
      "19          NaiveBayes          15   0.633015 0.012474  F1_macro\n",
      "20          NaiveBayes          20   0.632318 0.013540  F1_macro\n",
      "21          NaiveBayes          10   0.631334 0.015377  F1_macro\n",
      "22                 KNN           2   0.629147 0.003008  F1_macro\n",
      "23                 KNN           5   0.627883 0.006144  F1_macro\n",
      "24  LogisticRegression          20   0.612326 0.000726  F1_macro\n",
      "25  LogisticRegression          15   0.611682 0.000530  F1_macro\n",
      "26  LogisticRegression           5   0.601998 0.002358  F1_macro\n",
      "27  LogisticRegression          10   0.601563 0.003573  F1_macro\n",
      "28           LinearSVM          20   0.600003 0.000880  F1_macro\n",
      "29          NaiveBayes           2   0.599500 0.010954  F1_macro\n",
      "30  LogisticRegression           2   0.599335 0.009849  F1_macro\n",
      "31           LinearSVM          15   0.596383 0.001574  F1_macro\n",
      "32           LinearSVM          10   0.589746 0.003369  F1_macro\n",
      "33           LinearSVM           2   0.589561 0.005583  F1_macro\n",
      "34           LinearSVM           5   0.588978 0.003705  F1_macro\n",
      "35               Dummy           2   0.473060 0.000044  F1_macro\n",
      "36               Dummy           5   0.473060 0.000044  F1_macro\n",
      "37               Dummy          10   0.473060 0.000044  F1_macro\n",
      "38               Dummy          15   0.473060 0.000044  F1_macro\n",
      "39               Dummy          20   0.473060 0.000044  F1_macro\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   2.7s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   2.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   3.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   4.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   5.0s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   7.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   7.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   7.5s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   7.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   8.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   4.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   7.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.5s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.0s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  11.8s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  12.2s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   4.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   4.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   5.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   4.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  13.2s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   4.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   4.8s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   4.5s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   4.8s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   2.6s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   2.7s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   2.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.5s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   5.0s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   3.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   2.9s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   2.8s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   4.2s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   4.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   2.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  12.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  12.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  12.8s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  10.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  10.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  10.1s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  10.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   9.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   9.5s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 2\n",
      "Best hyperparameters: {'model__n_estimators': 700, 'model__min_samples_split': 2, 'model__min_samples_leaf': 4, 'model__max_features': 'log2', 'model__max_depth': 40}\n",
      "Best CV score (F1 macro): 0.710241\n",
      "\n",
      "=== Threshold tuning (OOF on train) ===\n",
      "Best threshold: 0.55 | F1_macro: 0.710989\n",
      "Confusion matrix at best threshold:\n",
      "[[6732  450]\n",
      " [ 415  403]]\n",
      "\n",
      "=== Holdout (time split) ===\n",
      "F1 macro: 0.725660\n",
      "Confusion matrix:\n",
      "[[1679  116]\n",
      " [  95  110]]\n",
      "sales threshold: 0.5499999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Classification call\n",
    "splits = prepare_data(\n",
    "    steam, olist, sales,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    feature_selection=\"tree\",  # try none first\n",
    "    max_features=50,\n",
    "    task_type=\"classification\",\n",
    "    scale_method=\"standard\",\n",
    "    tag_min_count=5, tag_top_k=100,\n",
    "    verbose=True,\n",
    "    ohe_top_k_per_col=50,        # cap per column\n",
    "    ohe_min_freq_per_col=5,\n",
    "    ohe_auto_exclude=True,       # auto skip long/free-text\n",
    "    ohe_high_card_threshold=500, # tune if needed\n",
    "    ohe_long_text_avglen=25\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "print(\"\\n=== STEAM Dataset ===\")\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ")\n",
    "\n",
    "score_steam = evaluate_on_holdout(best_steam_model, X_test_steam, y_test_steam, task_type=\"classification\")\n",
    "\n",
    "print(\"steam threshold:\", getattr(best_steam_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== OLIST Dataset ===\")\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ")\n",
    "\n",
    "score_olist = evaluate_on_holdout(best_olist_model, X_test_olist, y_test_olist, task_type=\"classification\")\n",
    "\n",
    "print(\"olist threshold:\", getattr(best_olist_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== SALES Dataset ===\")\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ") \n",
    "\n",
    "score_sales = evaluate_on_holdout(best_sales_model, X_test_sales, y_test_sales, task_type=\"classification\")\n",
    "\n",
    "print(\"sales threshold:\", getattr(best_sales_model, \"best_threshold_\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec77e4c",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc10f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] steam split: 0.01 s\n",
      "[info] steam post split | X_train shape=(8000, 22) mem=0.004 GB | X_test shape=(2000, 22) mem=0.001 GB\n",
      "[timer] steam datetime to numeric: 0.00 s\n",
      "[timer] steam text features: 0.04 s\n",
      "[info] steam after text | X_train shape=(8000, 24) mem=0.004 GB | X_test shape=(2000, 45) mem=0.001 GB\n",
      "[info] steam tags unique=426, kept=200 (min_count=5, top_k=200)\n",
      "[timer] steam tags multi-hot: 0.22 s\n",
      "[info] steam after tags | X_train shape=(8000, 223) mem=0.005 GB | X_test shape=(2000, 244) mem=0.001 GB\n",
      "[timer] steam OHE: 0.02 s\n",
      "[info] steam after OHE | X_train shape=(8000, 323) mem=0.004 GB | X_test shape=(2000, 323) mem=0.001 GB\n",
      "steam non-numeric after OHE: []\n",
      "[timer] steam impute: 0.06 s\n",
      "[info] steam after impute+downcast | X_train shape=(8000, 221) mem=0.007 GB | X_test shape=(2000, 229) mem=0.002 GB\n",
      "[timer] steam poly: 0.05 s\n",
      "[info] steam after poly | X_train shape=(8000, 228) mem=0.007 GB | X_test shape=(2000, 236) mem=0.002 GB\n",
      "[timer] steam outlier clip: 0.03 s\n",
      "[info] steam after outlier clip | X_train shape=(8000, 228) mem=0.007 GB | X_test shape=(2000, 236) mem=0.002 GB\n",
      "[timer] steam scale: 0.02 s\n",
      "[timer] steam select features: 0.00 s\n",
      "[info] steam after select | X_train shape=(8000, 228) mem=0.007 GB | X_test shape=(2000, 228) mem=0.002 GB\n",
      "[timer] olist split: 0.01 s\n",
      "[info] olist post split | X_train shape=(9052, 37) mem=0.007 GB | X_test shape=(2264, 37) mem=0.002 GB\n",
      "[timer] olist datetime to numeric: 0.01 s\n",
      "[timer] olist text features: 0.08 s\n",
      "[timer] olist OHE: 0.03 s\n",
      "[info] olist after OHE | X_train shape=(9052, 421) mem=0.005 GB | X_test shape=(2264, 421) mem=0.002 GB\n",
      "[timer] olist impute: 0.03 s\n",
      "[info] olist after impute+downcast | X_train shape=(9052, 37) mem=0.001 GB | X_test shape=(2264, 55) mem=0.001 GB\n",
      "[timer] olist poly: 0.01 s\n",
      "[info] olist after poly | X_train shape=(9052, 45) mem=0.002 GB | X_test shape=(2264, 63) mem=0.001 GB\n",
      "[timer] olist outlier clip: 0.03 s\n",
      "[info] olist after outlier clip | X_train shape=(9052, 45) mem=0.002 GB | X_test shape=(2264, 63) mem=0.001 GB\n",
      "[timer] olist scale: 0.01 s\n",
      "[timer] olist select features: 0.00 s\n",
      "[info] olist after select | X_train shape=(9052, 45) mem=0.002 GB | X_test shape=(2264, 45) mem=0.000 GB\n",
      "[timer] sales split: 0.00 s\n",
      "[info] sales post split | X_train shape=(905, 15) mem=0.000 GB | X_test shape=(227, 15) mem=0.000 GB\n",
      "[timer] sales datetime to numeric: 0.00 s\n",
      "[timer] sales text features: 0.01 s\n",
      "[timer] sales OHE: 0.01 s\n",
      "[info] sales after OHE | X_train shape=(905, 136) mem=0.000 GB | X_test shape=(227, 136) mem=0.000 GB\n",
      "[timer] sales impute: 0.00 s\n",
      "[info] sales after impute+downcast | X_train shape=(905, 15) mem=0.000 GB | X_test shape=(227, 31) mem=0.000 GB\n",
      "[timer] sales poly: 0.00 s\n",
      "[info] sales after poly | X_train shape=(905, 20) mem=0.000 GB | X_test shape=(227, 36) mem=0.000 GB\n",
      "[timer] sales outlier clip: 0.01 s\n",
      "[info] sales after outlier clip | X_train shape=(905, 20) mem=0.000 GB | X_test shape=(227, 36) mem=0.000 GB\n",
      "[timer] sales scale: 0.00 s\n",
      "[timer] sales select features: 0.00 s\n",
      "[info] sales after select | X_train shape=(905, 20) mem=0.000 GB | X_test shape=(227, 20) mem=0.000 GB\n",
      "[steam] X_train: (8000, 228) | X_test: (2000, 228) | y_train: (8000,) | y_test: (2000,)\n",
      "[olist] X_train: (9052, 45) | X_test: (2264, 45) | y_train: (9052,) | y_test: (2264,)\n",
      "[sales] X_train: (905, 20) | X_test: (227, 20) | y_train: (905,) | y_test: (227,)\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/45] GBT | k=23 | CV_MAE=5.384984 ± 0.068922\n",
      "[2/45] GBT | k=57 | CV_MAE=5.384984 ± 0.068922\n",
      "[3/45] GBT | k=114 | CV_MAE=5.384984 ± 0.068922\n",
      "[4/45] GBT | k=171 | CV_MAE=5.384984 ± 0.068922\n",
      "[5/45] GBT | k=228 | CV_MAE=5.384984 ± 0.068922\n",
      "[6/45] RandomForest | k=23 | CV_MAE=3.547768 ± 0.063481\n",
      "[7/45] RandomForest | k=57 | CV_MAE=3.547768 ± 0.063481\n",
      "[8/45] RandomForest | k=114 | CV_MAE=3.547768 ± 0.063481\n",
      "[9/45] RandomForest | k=171 | CV_MAE=3.547768 ± 0.063481\n",
      "[10/45] RandomForest | k=228 | CV_MAE=3.547768 ± 0.063481\n",
      "[11/45] DecisionTree | k=23 | CV_MAE=4.042264 ± 0.095437\n",
      "[12/45] DecisionTree | k=57 | CV_MAE=4.042264 ± 0.095437\n",
      "[13/45] DecisionTree | k=114 | CV_MAE=4.042264 ± 0.095437\n",
      "[14/45] DecisionTree | k=171 | CV_MAE=4.042264 ± 0.095437\n",
      "[15/45] DecisionTree | k=228 | CV_MAE=4.042264 ± 0.095437\n",
      "[16/45] LinearRegression | k=23 | CV_MAE=7.003015 ± 0.066647\n",
      "[17/45] LinearRegression | k=57 | CV_MAE=6.872029 ± 0.052489\n",
      "[18/45] LinearRegression | k=114 | CV_MAE=6.783448 ± 0.094815\n",
      "[19/45] LinearRegression | k=171 | CV_MAE=6.696969 ± 0.073103\n",
      "[20/45] LinearRegression | k=228 | CV_MAE=6.680351 ± 0.091632\n",
      "[21/45] Ridge | k=23 | CV_MAE=7.002985 ± 0.066641\n",
      "[22/45] Ridge | k=57 | CV_MAE=6.871938 ± 0.052497\n",
      "[23/45] Ridge | k=114 | CV_MAE=6.783356 ± 0.094831\n",
      "[24/45] Ridge | k=171 | CV_MAE=6.696865 ± 0.072943\n",
      "[25/45] Ridge | k=228 | CV_MAE=6.679962 ± 0.091376\n",
      "[26/45] Lasso | k=23 | CV_MAE=7.200431 ± 0.104154\n",
      "[27/45] Lasso | k=57 | CV_MAE=7.200431 ± 0.104153\n",
      "[28/45] Lasso | k=114 | CV_MAE=7.200431 ± 0.104154\n",
      "[29/45] Lasso | k=171 | CV_MAE=7.200431 ± 0.104153\n",
      "[30/45] Lasso | k=228 | CV_MAE=7.200431 ± 0.104154\n",
      "[31/45] ElasticNet | k=23 | CV_MAE=7.140693 ± 0.092685\n",
      "[32/45] ElasticNet | k=57 | CV_MAE=7.078764 ± 0.101695\n",
      "[33/45] ElasticNet | k=114 | CV_MAE=7.067824 ± 0.101002\n",
      "[34/45] ElasticNet | k=171 | CV_MAE=7.067582 ± 0.101056\n",
      "[35/45] ElasticNet | k=228 | CV_MAE=7.067582 ± 0.101056\n",
      "[36/45] KNN | k=23 | CV_MAE=5.998020 ± 0.083197\n",
      "[37/45] KNN | k=57 | CV_MAE=5.726401 ± 0.014229\n",
      "[38/45] KNN | k=114 | CV_MAE=5.919148 ± 0.045940\n",
      "[39/45] KNN | k=171 | CV_MAE=5.568856 ± 0.133340\n",
      "[40/45] KNN | k=228 | CV_MAE=5.638528 ± 0.068949\n",
      "[41/45] Dummy | k=23 | CV_MAE=8.085962 ± 0.093027\n",
      "[42/45] Dummy | k=57 | CV_MAE=8.085962 ± 0.093027\n",
      "[43/45] Dummy | k=114 | CV_MAE=8.085962 ± 0.093027\n",
      "[44/45] Dummy | k=171 | CV_MAE=8.085962 ± 0.093027\n",
      "[45/45] Dummy | k=228 | CV_MAE=8.085962 ± 0.093027\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "               Model  K_features  MeanScore   StdDev  Metric\n",
      "0       RandomForest          23   3.547768 0.063481  CV_MAE\n",
      "1       RandomForest          57   3.547768 0.063481  CV_MAE\n",
      "2       RandomForest         114   3.547768 0.063481  CV_MAE\n",
      "3       RandomForest         171   3.547768 0.063481  CV_MAE\n",
      "4       RandomForest         228   3.547768 0.063481  CV_MAE\n",
      "5       DecisionTree          23   4.042264 0.095437  CV_MAE\n",
      "6       DecisionTree          57   4.042264 0.095437  CV_MAE\n",
      "7       DecisionTree         114   4.042264 0.095437  CV_MAE\n",
      "8       DecisionTree         171   4.042264 0.095437  CV_MAE\n",
      "9       DecisionTree         228   4.042264 0.095437  CV_MAE\n",
      "10               GBT          23   5.384984 0.068922  CV_MAE\n",
      "11               GBT          57   5.384984 0.068922  CV_MAE\n",
      "12               GBT         114   5.384984 0.068922  CV_MAE\n",
      "13               GBT         171   5.384984 0.068922  CV_MAE\n",
      "14               GBT         228   5.384984 0.068922  CV_MAE\n",
      "15               KNN         171   5.568856 0.133340  CV_MAE\n",
      "16               KNN         228   5.638528 0.068949  CV_MAE\n",
      "17               KNN          57   5.726401 0.014229  CV_MAE\n",
      "18               KNN         114   5.919148 0.045940  CV_MAE\n",
      "19               KNN          23   5.998020 0.083197  CV_MAE\n",
      "20             Ridge         228   6.679962 0.091376  CV_MAE\n",
      "21  LinearRegression         228   6.680351 0.091632  CV_MAE\n",
      "22             Ridge         171   6.696865 0.072943  CV_MAE\n",
      "23  LinearRegression         171   6.696969 0.073103  CV_MAE\n",
      "24             Ridge         114   6.783356 0.094831  CV_MAE\n",
      "25  LinearRegression         114   6.783448 0.094815  CV_MAE\n",
      "26             Ridge          57   6.871938 0.052497  CV_MAE\n",
      "27  LinearRegression          57   6.872029 0.052489  CV_MAE\n",
      "28             Ridge          23   7.002985 0.066641  CV_MAE\n",
      "29  LinearRegression          23   7.003015 0.066647  CV_MAE\n",
      "30        ElasticNet         171   7.067582 0.101056  CV_MAE\n",
      "31        ElasticNet         228   7.067582 0.101056  CV_MAE\n",
      "32        ElasticNet         114   7.067824 0.101002  CV_MAE\n",
      "33        ElasticNet          57   7.078764 0.101695  CV_MAE\n",
      "34        ElasticNet          23   7.140693 0.092685  CV_MAE\n",
      "35             Lasso          23   7.200431 0.104154  CV_MAE\n",
      "36             Lasso         114   7.200431 0.104154  CV_MAE\n",
      "37             Lasso         228   7.200431 0.104154  CV_MAE\n",
      "38             Lasso          57   7.200431 0.104153  CV_MAE\n",
      "39             Lasso         171   7.200431 0.104153  CV_MAE\n",
      "40             Dummy          23   8.085962 0.093027  CV_MAE\n",
      "41             Dummy          57   8.085962 0.093027  CV_MAE\n",
      "42             Dummy         114   8.085962 0.093027  CV_MAE\n",
      "43             Dummy         171   8.085962 0.093027  CV_MAE\n",
      "44             Dummy         228   8.085962 0.093027  CV_MAE\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   0.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   0.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   0.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   0.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   0.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   0.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   3.1s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   3.2s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   3.5s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   3.6s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   3.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   2.2s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   3.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   2.2s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   4.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   4.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  11.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  11.6s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.6s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  12.2s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.2s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   4.3s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   4.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   4.0s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  20.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  21.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   4.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  21.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  21.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  21.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  22.2s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   3.6s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   4.1s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   4.4s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   6.2s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   5.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   6.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   2.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   2.4s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   2.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  36.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  37.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  39.7s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   3.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   2.9s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   3.0s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  13.8s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  13.8s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  12.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  38.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  33.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  34.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  38.6s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  32.2s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  37.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  31.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  31.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  31.7s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 23\n",
      "Best hyperparameters: {'model__n_estimators': 700, 'model__min_samples_split': 2, 'model__min_samples_leaf': 2, 'model__max_features': None, 'model__max_depth': 40}\n",
      "Best CV score (CV MAE): 3.591767\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/45] GBT | k=5 | CV_MAE=0.850078 ± 0.010467\n",
      "[2/45] GBT | k=12 | CV_MAE=0.850078 ± 0.010467\n",
      "[3/45] GBT | k=23 | CV_MAE=0.850078 ± 0.010467\n",
      "[4/45] GBT | k=34 | CV_MAE=0.850078 ± 0.010467\n",
      "[5/45] GBT | k=45 | CV_MAE=0.850078 ± 0.010467\n",
      "[6/45] RandomForest | k=5 | CV_MAE=0.826819 ± 0.008710\n",
      "[7/45] RandomForest | k=12 | CV_MAE=0.826819 ± 0.008710\n",
      "[8/45] RandomForest | k=23 | CV_MAE=0.826819 ± 0.008710\n",
      "[9/45] RandomForest | k=34 | CV_MAE=0.826819 ± 0.008710\n",
      "[10/45] RandomForest | k=45 | CV_MAE=0.826819 ± 0.008710\n",
      "[11/45] DecisionTree | k=5 | CV_MAE=1.017110 ± 0.009112\n",
      "[12/45] DecisionTree | k=12 | CV_MAE=1.017110 ± 0.009112\n",
      "[13/45] DecisionTree | k=23 | CV_MAE=1.017110 ± 0.009112\n",
      "[14/45] DecisionTree | k=34 | CV_MAE=1.017110 ± 0.009112\n",
      "[15/45] DecisionTree | k=45 | CV_MAE=1.017110 ± 0.009112\n",
      "[16/45] LinearRegression | k=5 | CV_MAE=0.898753 ± 0.009353\n",
      "[17/45] LinearRegression | k=12 | CV_MAE=0.898367 ± 0.010372\n",
      "[18/45] LinearRegression | k=23 | CV_MAE=0.899441 ± 0.010683\n",
      "[19/45] LinearRegression | k=34 | CV_MAE=0.884712 ± 0.010838\n",
      "[20/45] LinearRegression | k=45 | CV_MAE=0.884547 ± 0.010803\n",
      "[21/45] Ridge | k=5 | CV_MAE=0.898761 ± 0.009357\n",
      "[22/45] Ridge | k=12 | CV_MAE=0.898372 ± 0.010372\n",
      "[23/45] Ridge | k=23 | CV_MAE=0.899441 ± 0.010678\n",
      "[24/45] Ridge | k=34 | CV_MAE=0.884551 ± 0.010732\n",
      "[25/45] Ridge | k=45 | CV_MAE=0.884555 ± 0.010740\n",
      "[26/45] Lasso | k=5 | CV_MAE=0.905201 ± 0.011217\n",
      "[27/45] Lasso | k=12 | CV_MAE=0.905201 ± 0.011217\n",
      "[28/45] Lasso | k=23 | CV_MAE=0.905201 ± 0.011217\n",
      "[29/45] Lasso | k=34 | CV_MAE=0.905201 ± 0.011217\n",
      "[30/45] Lasso | k=45 | CV_MAE=0.905201 ± 0.011217\n",
      "[31/45] ElasticNet | k=5 | CV_MAE=0.905201 ± 0.011217\n",
      "[32/45] ElasticNet | k=12 | CV_MAE=0.905201 ± 0.011217\n",
      "[33/45] ElasticNet | k=23 | CV_MAE=0.905201 ± 0.011217\n",
      "[34/45] ElasticNet | k=34 | CV_MAE=0.905201 ± 0.011217\n",
      "[35/45] ElasticNet | k=45 | CV_MAE=0.905201 ± 0.011217\n",
      "[36/45] KNN | k=5 | CV_MAE=0.973970 ± 0.006340\n",
      "[37/45] KNN | k=12 | CV_MAE=0.962591 ± 0.010158\n",
      "[38/45] KNN | k=23 | CV_MAE=0.919416 ± 0.021080\n",
      "[39/45] KNN | k=34 | CV_MAE=0.895007 ± 0.009830\n",
      "[40/45] KNN | k=45 | CV_MAE=0.890108 ± 0.013965\n",
      "[41/45] Dummy | k=5 | CV_MAE=0.905201 ± 0.011217\n",
      "[42/45] Dummy | k=12 | CV_MAE=0.905201 ± 0.011217\n",
      "[43/45] Dummy | k=23 | CV_MAE=0.905201 ± 0.011217\n",
      "[44/45] Dummy | k=34 | CV_MAE=0.905201 ± 0.011217\n",
      "[45/45] Dummy | k=45 | CV_MAE=0.905201 ± 0.011217\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "               Model  K_features  MeanScore   StdDev  Metric\n",
      "0       RandomForest           5   0.826819 0.008710  CV_MAE\n",
      "1       RandomForest          23   0.826819 0.008710  CV_MAE\n",
      "2       RandomForest          12   0.826819 0.008710  CV_MAE\n",
      "3       RandomForest          34   0.826819 0.008710  CV_MAE\n",
      "4       RandomForest          45   0.826819 0.008710  CV_MAE\n",
      "5                GBT           5   0.850078 0.010467  CV_MAE\n",
      "6                GBT          12   0.850078 0.010467  CV_MAE\n",
      "7                GBT          23   0.850078 0.010467  CV_MAE\n",
      "8                GBT          34   0.850078 0.010467  CV_MAE\n",
      "9                GBT          45   0.850078 0.010467  CV_MAE\n",
      "10  LinearRegression          45   0.884547 0.010803  CV_MAE\n",
      "11             Ridge          34   0.884551 0.010732  CV_MAE\n",
      "12             Ridge          45   0.884555 0.010740  CV_MAE\n",
      "13  LinearRegression          34   0.884712 0.010838  CV_MAE\n",
      "14               KNN          45   0.890108 0.013965  CV_MAE\n",
      "15               KNN          34   0.895007 0.009830  CV_MAE\n",
      "16  LinearRegression          12   0.898367 0.010372  CV_MAE\n",
      "17             Ridge          12   0.898372 0.010372  CV_MAE\n",
      "18  LinearRegression           5   0.898753 0.009353  CV_MAE\n",
      "19             Ridge           5   0.898761 0.009357  CV_MAE\n",
      "20  LinearRegression          23   0.899441 0.010683  CV_MAE\n",
      "21             Ridge          23   0.899441 0.010678  CV_MAE\n",
      "22             Dummy           5   0.905201 0.011217  CV_MAE\n",
      "23             Dummy          12   0.905201 0.011217  CV_MAE\n",
      "24             Dummy          23   0.905201 0.011217  CV_MAE\n",
      "25             Dummy          34   0.905201 0.011217  CV_MAE\n",
      "26             Dummy          45   0.905201 0.011217  CV_MAE\n",
      "27        ElasticNet           5   0.905201 0.011217  CV_MAE\n",
      "28        ElasticNet          12   0.905201 0.011217  CV_MAE\n",
      "29        ElasticNet          23   0.905201 0.011217  CV_MAE\n",
      "30        ElasticNet          34   0.905201 0.011217  CV_MAE\n",
      "31        ElasticNet          45   0.905201 0.011217  CV_MAE\n",
      "32             Lasso           5   0.905201 0.011217  CV_MAE\n",
      "33             Lasso          12   0.905201 0.011217  CV_MAE\n",
      "34             Lasso          23   0.905201 0.011217  CV_MAE\n",
      "35             Lasso          34   0.905201 0.011217  CV_MAE\n",
      "36             Lasso          45   0.905201 0.011217  CV_MAE\n",
      "37               KNN          23   0.919416 0.021080  CV_MAE\n",
      "38               KNN          12   0.962591 0.010158  CV_MAE\n",
      "39               KNN           5   0.973970 0.006340  CV_MAE\n",
      "40      DecisionTree           5   1.017110 0.009112  CV_MAE\n",
      "41      DecisionTree          12   1.017110 0.009112  CV_MAE\n",
      "42      DecisionTree          23   1.017110 0.009112  CV_MAE\n",
      "43      DecisionTree          34   1.017110 0.009112  CV_MAE\n",
      "44      DecisionTree          45   1.017110 0.009112  CV_MAE\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   2.5s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   2.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   4.1s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   6.0s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   6.1s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   6.2s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   9.1s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   9.3s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   9.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   3.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   3.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   3.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  17.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  17.2s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   3.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=  10.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  31.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  31.6s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  32.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  31.8s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   2.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=  10.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  33.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=  10.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  17.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   9.4s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   9.8s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   9.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=  34.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  11.5s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  11.9s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  12.2s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   7.2s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   7.4s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   7.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.9s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  57.6s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  58.6s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  58.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   5.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   6.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   5.8s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  19.6s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  19.2s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  18.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  57.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  58.0s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  58.8s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  50.8s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  49.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  48.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  48.2s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  45.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  46.3s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 5\n",
      "Best hyperparameters: {'model__n_estimators': 400, 'model__min_samples_split': 2, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20}\n",
      "Best CV score (CV MAE): 0.806095\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/45] GBT | k=2 | CV_MAE=1.038132 ± 0.005860\n",
      "[2/45] GBT | k=5 | CV_MAE=1.038132 ± 0.005860\n",
      "[3/45] GBT | k=10 | CV_MAE=1.038132 ± 0.005860\n",
      "[4/45] GBT | k=15 | CV_MAE=1.038132 ± 0.005860\n",
      "[5/45] GBT | k=20 | CV_MAE=1.038132 ± 0.005860\n",
      "[6/45] RandomForest | k=2 | CV_MAE=1.042625 ± 0.007322\n",
      "[7/45] RandomForest | k=5 | CV_MAE=1.042625 ± 0.007322\n",
      "[8/45] RandomForest | k=10 | CV_MAE=1.042625 ± 0.007322\n",
      "[9/45] RandomForest | k=15 | CV_MAE=1.042625 ± 0.007322\n",
      "[10/45] RandomForest | k=20 | CV_MAE=1.042625 ± 0.007322\n",
      "[11/45] DecisionTree | k=2 | CV_MAE=1.397125 ± 0.078343\n",
      "[12/45] DecisionTree | k=5 | CV_MAE=1.397125 ± 0.078343\n",
      "[13/45] DecisionTree | k=10 | CV_MAE=1.397125 ± 0.078343\n",
      "[14/45] DecisionTree | k=15 | CV_MAE=1.397125 ± 0.078343\n",
      "[15/45] DecisionTree | k=20 | CV_MAE=1.397125 ± 0.078343\n",
      "[16/45] LinearRegression | k=2 | CV_MAE=1.100396 ± 0.044071\n",
      "[17/45] LinearRegression | k=5 | CV_MAE=1.105640 ± 0.034875\n",
      "[18/45] LinearRegression | k=10 | CV_MAE=1.095406 ± 0.023618\n",
      "[19/45] LinearRegression | k=15 | CV_MAE=1.086858 ± 0.018349\n",
      "[20/45] LinearRegression | k=20 | CV_MAE=1.083056 ± 0.022478\n",
      "[21/45] Ridge | k=2 | CV_MAE=1.100414 ± 0.044073\n",
      "[22/45] Ridge | k=5 | CV_MAE=1.105461 ± 0.034845\n",
      "[23/45] Ridge | k=10 | CV_MAE=1.095003 ± 0.023314\n",
      "[24/45] Ridge | k=15 | CV_MAE=1.090240 ± 0.019720\n",
      "[25/45] Ridge | k=20 | CV_MAE=1.085168 ± 0.027138\n",
      "[26/45] Lasso | k=2 | CV_MAE=1.152468 ± 0.038557\n",
      "[27/45] Lasso | k=5 | CV_MAE=1.152468 ± 0.038557\n",
      "[28/45] Lasso | k=10 | CV_MAE=1.152468 ± 0.038557\n",
      "[29/45] Lasso | k=15 | CV_MAE=1.152468 ± 0.038557\n",
      "[30/45] Lasso | k=20 | CV_MAE=1.152468 ± 0.038557\n",
      "[31/45] ElasticNet | k=2 | CV_MAE=1.152468 ± 0.038557\n",
      "[32/45] ElasticNet | k=5 | CV_MAE=1.152468 ± 0.038557\n",
      "[33/45] ElasticNet | k=10 | CV_MAE=1.152468 ± 0.038557\n",
      "[34/45] ElasticNet | k=15 | CV_MAE=1.152468 ± 0.038557\n",
      "[35/45] ElasticNet | k=20 | CV_MAE=1.152468 ± 0.038557\n",
      "[36/45] KNN | k=2 | CV_MAE=1.151018 ± 0.021631\n",
      "[37/45] KNN | k=5 | CV_MAE=1.134959 ± 0.061077\n",
      "[38/45] KNN | k=10 | CV_MAE=1.093991 ± 0.005696\n",
      "[39/45] KNN | k=15 | CV_MAE=1.102568 ± 0.044006\n",
      "[40/45] KNN | k=20 | CV_MAE=1.111478 ± 0.036626\n",
      "[41/45] Dummy | k=2 | CV_MAE=1.152468 ± 0.038557\n",
      "[42/45] Dummy | k=5 | CV_MAE=1.152468 ± 0.038557\n",
      "[43/45] Dummy | k=10 | CV_MAE=1.152468 ± 0.038557\n",
      "[44/45] Dummy | k=15 | CV_MAE=1.152468 ± 0.038557\n",
      "[45/45] Dummy | k=20 | CV_MAE=1.152468 ± 0.038557\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "               Model  K_features  MeanScore   StdDev  Metric\n",
      "0                GBT           2   1.038132 0.005860  CV_MAE\n",
      "1                GBT           5   1.038132 0.005860  CV_MAE\n",
      "2                GBT          10   1.038132 0.005860  CV_MAE\n",
      "3                GBT          15   1.038132 0.005860  CV_MAE\n",
      "4                GBT          20   1.038132 0.005860  CV_MAE\n",
      "5       RandomForest           2   1.042625 0.007322  CV_MAE\n",
      "6       RandomForest           5   1.042625 0.007322  CV_MAE\n",
      "7       RandomForest          10   1.042625 0.007322  CV_MAE\n",
      "8       RandomForest          15   1.042625 0.007322  CV_MAE\n",
      "9       RandomForest          20   1.042625 0.007322  CV_MAE\n",
      "10  LinearRegression          20   1.083056 0.022478  CV_MAE\n",
      "11             Ridge          20   1.085168 0.027138  CV_MAE\n",
      "12  LinearRegression          15   1.086858 0.018349  CV_MAE\n",
      "13             Ridge          15   1.090240 0.019720  CV_MAE\n",
      "14               KNN          10   1.093991 0.005696  CV_MAE\n",
      "15             Ridge          10   1.095003 0.023314  CV_MAE\n",
      "16  LinearRegression          10   1.095406 0.023618  CV_MAE\n",
      "17  LinearRegression           2   1.100396 0.044071  CV_MAE\n",
      "18             Ridge           2   1.100414 0.044073  CV_MAE\n",
      "19               KNN          15   1.102568 0.044006  CV_MAE\n",
      "20             Ridge           5   1.105461 0.034845  CV_MAE\n",
      "21  LinearRegression           5   1.105640 0.034875  CV_MAE\n",
      "22               KNN          20   1.111478 0.036626  CV_MAE\n",
      "23               KNN           5   1.134959 0.061077  CV_MAE\n",
      "24               KNN           2   1.151018 0.021631  CV_MAE\n",
      "25        ElasticNet           2   1.152468 0.038557  CV_MAE\n",
      "26        ElasticNet           5   1.152468 0.038557  CV_MAE\n",
      "27        ElasticNet          10   1.152468 0.038557  CV_MAE\n",
      "28        ElasticNet          15   1.152468 0.038557  CV_MAE\n",
      "29        ElasticNet          20   1.152468 0.038557  CV_MAE\n",
      "30             Lasso           2   1.152468 0.038557  CV_MAE\n",
      "31             Lasso           5   1.152468 0.038557  CV_MAE\n",
      "32             Lasso          10   1.152468 0.038557  CV_MAE\n",
      "33             Lasso          15   1.152468 0.038557  CV_MAE\n",
      "34             Lasso          20   1.152468 0.038557  CV_MAE\n",
      "35             Dummy           2   1.152468 0.038557  CV_MAE\n",
      "36             Dummy           5   1.152468 0.038557  CV_MAE\n",
      "37             Dummy          10   1.152468 0.038557  CV_MAE\n",
      "38             Dummy          15   1.152468 0.038557  CV_MAE\n",
      "39             Dummy          20   1.152468 0.038557  CV_MAE\n",
      "40      DecisionTree           2   1.397125 0.078343  CV_MAE\n",
      "41      DecisionTree           5   1.397125 0.078343  CV_MAE\n",
      "42      DecisionTree          10   1.397125 0.078343  CV_MAE\n",
      "43      DecisionTree          15   1.397125 0.078343  CV_MAE\n",
      "44      DecisionTree          20   1.397125 0.078343  CV_MAE\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=4, model__n_estimators=100, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=2, model__n_estimators=300, model__subsample=0.9; total time=   0.2s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=2, model__n_estimators=300, model__subsample=0.9; total time=   0.2s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=2, model__n_estimators=300, model__subsample=0.9; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.006579332246575682, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=4, model__n_estimators=100, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.006579332246575682, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.006579332246575682, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=3, model__n_estimators=500, model__subsample=0.7; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=3, model__n_estimators=500, model__subsample=0.7; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=3, model__n_estimators=500, model__subsample=0.7; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.001873817422860383, model__max_depth=2, model__n_estimators=200, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.001873817422860383, model__max_depth=2, model__n_estimators=200, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.001873817422860383, model__max_depth=2, model__n_estimators=200, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=2, model__n_estimators=300, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=2, model__n_estimators=300, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=2, model__n_estimators=300, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.6; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.6; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.6; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=2, model__n_estimators=500, model__subsample=0.7; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=2, model__n_estimators=500, model__subsample=0.7; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.04328761281083057, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.04328761281083057, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=5, model__n_estimators=500, model__subsample=0.9; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.04328761281083057, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=4, model__n_estimators=100, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=100, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=5, model__n_estimators=500, model__subsample=0.9; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=2, model__n_estimators=500, model__subsample=0.7; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.9; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.2848035868435799, model__max_depth=5, model__n_estimators=200, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=5, model__n_estimators=500, model__subsample=0.6; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=100, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=3, model__n_estimators=200, model__subsample=0.9; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.9; total time=   0.6s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=3, model__n_estimators=200, model__subsample=0.9; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2848035868435799, model__max_depth=5, model__n_estimators=200, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=100, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=3, model__n_estimators=200, model__subsample=0.9; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2848035868435799, model__max_depth=5, model__n_estimators=200, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=5, model__n_estimators=500, model__subsample=0.6; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.9; total time=   0.5s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=5, model__n_estimators=500, model__subsample=0.6; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=5, model__n_estimators=500, model__subsample=0.9; total time=   0.5s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: GBT\n",
      "Number of features: 2\n",
      "Best hyperparameters: {'model__subsample': np.float64(0.8), 'model__n_estimators': 100, 'model__max_depth': 3, 'model__learning_rate': np.float64(0.08111308307896868)}\n",
      "Best CV score (CV MAE): 1.033591\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regression call\n",
    "splits = prepare_data(\n",
    "    steam, olist, sales,\n",
    "    task_type=\"regression\",\n",
    "    random_state=42,\n",
    "    feature_selection=\"none\",  # try none first\n",
    "    max_features=None,\n",
    "    scale_method=\"standard\",\n",
    "    tag_min_count=5, tag_top_k=200,\n",
    "    verbose=True,\n",
    "    ohe_top_k_per_col=50,        # cap per column\n",
    "    ohe_min_freq_per_col=5,\n",
    "    ohe_auto_exclude=True,       # auto skip long/free-text\n",
    "    ohe_high_card_threshold=500, # tune if needed\n",
    "    ohe_long_text_avglen=25,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
