{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de57485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Standard Libraries\n",
    "# =============================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# progress / kaggle\n",
    "from tqdm.auto import tqdm\n",
    "import kagglehub\n",
    "from IPython.display import display\n",
    "\n",
    "# =============================\n",
    "# Data Science Libraries\n",
    "# =============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "# =============================\n",
    "# Visualization\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn\n",
    "# =============================\n",
    "from sklearn import clone\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    ParameterGrid,\n",
    "    ParameterSampler,\n",
    "    KFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "import sklearn.model_selection._search as sk_search\n",
    "import sklearn.model_selection._validation as sk_validation\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    MultiLabelBinarizer,\n",
    "    Normalizer,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    get_scorer,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SequentialFeatureSelector,\n",
    "    f_regression,\n",
    "    SelectKBest,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    "    RidgeClassifier,\n",
    "    LogisticRegression,\n",
    "    RidgeCV,\n",
    "    LassoCV,\n",
    "    ElasticNetCV,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# extra joblib tools\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline as SKPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif, VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, mean_absolute_error\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Global Settings\n",
    "# =============================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "random_state = 42\n",
    "N_ROWS = 1_000_000\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")  # no scientific notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1cdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_eda(df, name):\n",
    "    # simple settings\n",
    "    top_k_categories = 20\n",
    "    max_corr_cols = 30\n",
    "    max_rows_to_show = 25\n",
    "\n",
    "    # make printing wide and avoid scientific notation\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\", max_rows_to_show,\n",
    "        \"display.max_columns\", None,\n",
    "        \"display.width\", 1000,\n",
    "        \"display.max_colwidth\", 200,\n",
    "        \"display.float_format\", lambda x: f\"{x:.6f}\"\n",
    "    ):\n",
    "        report_lines = []\n",
    "\n",
    "        # title\n",
    "        report_lines.append(f\"=== Robust EDA Report: {name} ===\")\n",
    "\n",
    "        # shapes and memory\n",
    "        info_df = pd.DataFrame(\n",
    "            {\n",
    "                \"rows\": [df.shape[0]],\n",
    "                \"columns\": [df.shape[1]],\n",
    "                \"memory_bytes\": [int(df.memory_usage(deep=True).sum())],\n",
    "            }\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Info ===\")\n",
    "        report_lines.append(info_df.to_string(index=False))\n",
    "\n",
    "        # dtypes\n",
    "        dtypes_df = (\n",
    "            df.dtypes.rename(\"dtype\")\n",
    "            .astype(str)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"column\"})\n",
    "            .sort_values(\"column\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Dtypes ===\")\n",
    "        report_lines.append(dtypes_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # missing values\n",
    "        total_rows = len(df)\n",
    "        missing_counts = df.isna().sum()\n",
    "        if total_rows > 0:\n",
    "            missing_percent = (missing_counts / total_rows * 100).round(2)\n",
    "        else:\n",
    "            missing_percent = pd.Series([0] * len(df.columns), index=df.columns)\n",
    "        missing_df = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": df.columns,\n",
    "                    \"missing_count\": missing_counts.values,\n",
    "                    \"missing_percent\": missing_percent.values,\n",
    "                }\n",
    "            )\n",
    "            .sort_values([\"missing_count\", \"missing_percent\"], ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Missing Values ===\")\n",
    "        report_lines.append(missing_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # duplicates (safe fallback for unhashable types)\n",
    "        try:\n",
    "            duplicate_count = int(df.duplicated().sum())\n",
    "            duplicate_index = df.index[df.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "        except TypeError:\n",
    "            df_hashable = df.astype(str)\n",
    "            duplicate_count = int(df_hashable.duplicated().sum())\n",
    "            duplicate_index = df_hashable.index[df_hashable.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "\n",
    "        duplicates_summary_df = pd.DataFrame({\"duplicate_rows\": [duplicate_count]})\n",
    "        report_lines.append(\"\\n=== Duplicates Summary ===\")\n",
    "        report_lines.append(duplicates_summary_df.to_string(index=False))\n",
    "        report_lines.append(\"\\n=== Duplicates Preview (up to 20 rows) ===\")\n",
    "        if len(duplicates_preview_df) > 0:\n",
    "            report_lines.append(duplicates_preview_df.to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"(No duplicate rows found.)\")\n",
    "\n",
    "        # column groups\n",
    "        numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        categorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "        # numeric summary\n",
    "        if len(numeric_columns) > 0:\n",
    "            percentiles = [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "            numeric_summary_df = (\n",
    "                df[numeric_columns]\n",
    "                .describe(percentiles=percentiles)\n",
    "                .T.reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Numeric Summary (5%..95%) ===\")\n",
    "            report_lines.append(numeric_summary_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # skew and kurtosis\n",
    "            skew_kurt_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": numeric_columns,\n",
    "                    \"skew\": df[numeric_columns].skew(numeric_only=True).values,\n",
    "                    \"kurtosis\": df[numeric_columns].kurtosis(numeric_only=True).values,\n",
    "                }\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Skew and Kurtosis ===\")\n",
    "            report_lines.append(skew_kurt_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # IQR outliers per column\n",
    "            q1 = df[numeric_columns].quantile(0.25)\n",
    "            q3 = df[numeric_columns].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outlier_mask = (df[numeric_columns] < (q1 - 1.5 * iqr)) | (df[numeric_columns] > (q3 + 1.5 * iqr))\n",
    "            iqr_outliers_df = (\n",
    "                outlier_mask.sum()\n",
    "                .rename(\"outlier_count\")\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== IQR Outlier Counts ===\")\n",
    "            report_lines.append(iqr_outliers_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # correlation on first N numeric columns\n",
    "            if len(numeric_columns) > 1:\n",
    "                selected_cols = numeric_columns[:max_corr_cols]\n",
    "                correlation_df = df[selected_cols].corr(method=\"pearson\", numeric_only=True)\n",
    "                correlation_df.index.name = \"column\"\n",
    "                report_lines.append(f\"\\n=== Correlation (first {max_corr_cols} numeric columns) ===\")\n",
    "                report_lines.append(correlation_df.to_string())\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No numeric columns found.)\")\n",
    "\n",
    "        # categorical value counts (top K each)\n",
    "        if len(categorical_columns) > 0:\n",
    "            cat_rows = []\n",
    "            for col in categorical_columns:\n",
    "                try:\n",
    "                    vc = df[col].value_counts(dropna=False).head(top_k_categories)\n",
    "                except TypeError:\n",
    "                    vc = df[col].astype(str).value_counts(dropna=False).head(top_k_categories)\n",
    "                for value, count in vc.items():\n",
    "                    percent = (count / total_rows * 100) if total_rows > 0 else 0\n",
    "                    cat_rows.append(\n",
    "                        {\"column\": col, \"value\": value, \"count\": int(count), \"percent\": round(percent, 2)}\n",
    "                    )\n",
    "            categorical_values_df = pd.DataFrame(cat_rows)\n",
    "            report_lines.append(f\"\\n=== Categorical Values (Top {top_k_categories} per column) ===\")\n",
    "            report_lines.append(categorical_values_df.head(max_rows_to_show).to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No categorical columns found.)\")\n",
    "\n",
    "        # unique counts per column\n",
    "        def _safe_nunique(series):\n",
    "            try:\n",
    "                return int(series.nunique(dropna=False))\n",
    "            except TypeError:\n",
    "                return np.nan\n",
    "\n",
    "        unique_counts_df = pd.DataFrame(\n",
    "            {\"column\": df.columns, \"unique_values\": [_safe_nunique(df[c]) for c in df.columns]}\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Unique Counts Per Column ===\")\n",
    "        report_lines.append(unique_counts_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # sample head\n",
    "        report_lines.append(\"\\n=== Head (10 rows) ===\")\n",
    "        report_lines.append(df.head(10).to_string(index=False))\n",
    "\n",
    "        # end\n",
    "        report_lines.append(\"\\n=== End of EDA Report ===\")\n",
    "\n",
    "        # one giant print\n",
    "        print(\"\\n\".join(report_lines))\n",
    "\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "        return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bd5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb965d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeepTrainColumns(BaseEstimator, TransformerMixin):\n",
    "    # remembers training columns and reindexes any input to match\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.keep_columns_ = list(X.columns)\n",
    "        else:\n",
    "            self.keep_columns_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.keep_columns_ is None:\n",
    "            return X\n",
    "        if hasattr(X, \"reindex\"):\n",
    "            return X.reindex(columns=self.keep_columns_, fill_value=0)\n",
    "        return X\n",
    "\n",
    "\n",
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    # turn scores into 0/1 using a chosen threshold\n",
    "    import numpy as np\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        raw = model.decision_function(X)\n",
    "        raw_min, raw_max = float(raw.min()), float(raw.max())\n",
    "        scores = (raw - raw_min) / (raw_max - raw_min + 1e-9)\n",
    "    else:\n",
    "        scores = model.predict(X).astype(float)\n",
    "    return (scores >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model builder + tuner\n",
    "# =============================\n",
    "def build_and_tune_models(\n",
    "    X_train, y_train,\n",
    "    task_type,\n",
    "    num_folds,\n",
    "    num_iterations,\n",
    "    oversample=False,\n",
    "    oversample_method=\"random\"\n",
    "):\n",
    "\n",
    "    # optional oversampling tools\n",
    "    ImbPipeline = None\n",
    "    RandomOverSampler = None\n",
    "    SMOTE = None\n",
    "    if oversample and str(task_type).strip().lower() == \"classification\":\n",
    "        try:\n",
    "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "        except Exception:\n",
    "            print(\"imblearn not available. Oversampling disabled.\")\n",
    "            oversample = False\n",
    "\n",
    "    # task settings\n",
    "    task = str(task_type).strip().lower()\n",
    "    if task == \"classification\":\n",
    "        scoring = \"f1_macro\"\n",
    "        selector_score_func = f_classif\n",
    "        min_class = int(y_train.value_counts().min())\n",
    "        eff_folds = max(2, min(int(num_folds), min_class))\n",
    "        baseline_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        sampler_obj = None\n",
    "        if oversample:\n",
    "            if oversample_method == \"smote\":\n",
    "                k_neighbors_for_smote = max(1, min(5, min_class - 1))\n",
    "                if k_neighbors_for_smote < 1:\n",
    "                    print(\"SMOTE not possible (minority class too small). Using RandomOverSampler.\")\n",
    "                    sampler_obj = RandomOverSampler(random_state=42)\n",
    "                else:\n",
    "                    sampler_obj = SMOTE(random_state=42, k_neighbors=k_neighbors_for_smote)\n",
    "            else:\n",
    "                sampler_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "        class_weight_choice = None if oversample else \"balanced\"\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingClassifier(random_state=42),\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=42, class_weight=class_weight_choice, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeClassifier(random_state=42, class_weight=class_weight_choice),\n",
    "            \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"LinearSVM\": LinearSVC(max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"NaiveBayes\": GaussianNB(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"Dummy\": DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "        }\n",
    "        metric_name = \"F1_macro\"\n",
    "    elif task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "        selector_score_func = f_regression\n",
    "        eff_folds = max(2, int(num_folds))\n",
    "        baseline_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingRegressor(random_state=42),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(max_iter=5000),\n",
    "            \"Lasso\": Lasso(max_iter=5000),\n",
    "            \"ElasticNet\": ElasticNet(max_iter=5000),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"Dummy\": DummyRegressor(strategy=\"mean\"),\n",
    "        }\n",
    "        metric_name = \"CV_MAE\"\n",
    "        sampler_obj = None\n",
    "    else:\n",
    "        raise ValueError('task_type must be \"classification\" or \"regression\"')\n",
    "\n",
    "    total_features = X_train.shape[1]\n",
    "    feature_fractions = [0.10, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "    # which models need scaling and selection\n",
    "    needs_scaling = {\"LogisticRegression\", \"LinearSVM\", \"KNN\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"NaiveBayes\"}\n",
    "    skip_selection = {\"Dummy\"}\n",
    "    tree_like = {\"RandomForest\", \"DecisionTree\", \"GBT\"}\n",
    "\n",
    "    def k_from_fraction(frac, total_cols):\n",
    "        if frac >= 1.0:\n",
    "            return \"all\"\n",
    "        k = int(max(1, math.ceil(frac * total_cols)))\n",
    "        return min(k, total_cols)\n",
    "\n",
    "    # dynamic KNN neighbors cap\n",
    "    per_fold_train = int(len(X_train) * (eff_folds - 1) / eff_folds)\n",
    "    max_knn_k = max(3, min(101, per_fold_train - 1))\n",
    "    knn_ks = list(range(3, max_knn_k + 1, 2))\n",
    "\n",
    "    def logspace_list(low_exp, high_exp, num):\n",
    "        return list(np.logspace(low_exp, high_exp, num))\n",
    "\n",
    "    def linspace_list(low_val, high_val, num):\n",
    "        return list(np.linspace(low_val, high_val, num))\n",
    "\n",
    "    param_spaces_classification = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LogisticRegression\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__solver\": [\"saga\"],\n",
    "        },\n",
    "        \"LinearSVM\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        },\n",
    "        \"NaiveBayes\": {\n",
    "            \"model__var_smoothing\": list(10 ** np.linspace(-11, -7, 9))\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"most_frequent\", \"stratified\", \"uniform\"]},\n",
    "    }\n",
    "\n",
    "    param_spaces_regression = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LinearRegression\": {},\n",
    "        \"Ridge\": {\"model__alpha\": logspace_list(-3, 3, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"Lasso\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"ElasticNet\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__l1_ratio\": linspace_list(0.1, 0.9, 9), \"model__fit_intercept\": [True, False]},\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"mean\", \"median\"]},\n",
    "    }\n",
    "\n",
    "    # build a pipeline for a given k\n",
    "    def make_pipeline_for_k(model_name, model_obj, k_value):\n",
    "        # order: align -> impute -> variance -> select -> scale -> sampler -> model\n",
    "        align_step = (\"align\", KeepTrainColumns())\n",
    "        impute_step = (\"impute\", SimpleImputer(strategy=\"median\"))\n",
    "        variance_step = (\"variance\", VarianceThreshold(threshold=0.0))\n",
    "\n",
    "        if model_name in skip_selection or model_name in tree_like:\n",
    "            select_step = (\"select\", \"passthrough\")\n",
    "        else:\n",
    "            select_step = (\"select\", SelectKBest(score_func=selector_score_func, k=k_value))\n",
    "\n",
    "        scale_step = (\"scale\", StandardScaler() if model_name in needs_scaling else \"passthrough\")\n",
    "\n",
    "        steps = [align_step, impute_step, variance_step, select_step, scale_step]\n",
    "\n",
    "        if task == \"classification\" and oversample and sampler_obj is not None:\n",
    "            steps.append((\"sampler\", sampler_obj))\n",
    "\n",
    "        steps.append((\"model\", model_obj))\n",
    "\n",
    "        if oversample and ImbPipeline is not None and task == \"classification\":\n",
    "            return ImbPipeline(steps)\n",
    "        else:\n",
    "            return SKPipeline(steps)\n",
    "\n",
    "    # baseline sweep across models × k\n",
    "    rows = []\n",
    "    total_steps = len(model_space) * len(feature_fractions)\n",
    "    step = 0\n",
    "    print(\"Streaming results (each line is one model × feature count):\")\n",
    "    for model_name, model_obj in model_space.items():\n",
    "        for frac in feature_fractions:\n",
    "            step += 1\n",
    "            k_val = k_from_fraction(frac, total_features)\n",
    "            k_print = total_features if k_val == \"all\" else int(k_val)\n",
    "            pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=baseline_cv, scoring=scoring, n_jobs=1)\n",
    "            mean_score = float(np.mean(scores))\n",
    "            std_score = float(np.std(scores))\n",
    "            if task == \"regression\":\n",
    "                display_mean = -mean_score\n",
    "                display_std = float(np.std(-scores))\n",
    "            else:\n",
    "                display_mean = mean_score\n",
    "                display_std = std_score\n",
    "            rows.append({\"Model\": model_name, \"K_features\": k_print, \"MeanScore\": display_mean, \"StdDev\": display_std, \"Metric\": metric_name})\n",
    "            print(f\"[{step}/{total_steps}] {model_name} | k={k_print} | {metric_name}={display_mean:.6f} ± {display_std:.6f}\", flush=True)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    if task == \"classification\":\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    else:\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Baseline results (CV) ===\")\n",
    "    print(results_df[[\"Model\", \"K_features\", \"MeanScore\", \"StdDev\", \"Metric\"]])\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    best_model_name = str(best_row[\"Model\"])\n",
    "    best_k = int(best_row[\"K_features\"])\n",
    "    best_model_obj = model_space[best_model_name]\n",
    "    k_val_for_search = \"all\" if best_k >= total_features else best_k\n",
    "    best_pipeline = make_pipeline_for_k(best_model_name, best_model_obj, k_val_for_search)\n",
    "\n",
    "    # pick search space\n",
    "    search_space = (param_spaces_classification if task == \"classification\" else param_spaces_regression).get(best_model_name, {})\n",
    "    if len(search_space) == 0:\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        try:\n",
    "            best_pipeline.input_columns_ = list(X_train.columns)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # threshold tuning only for classification\n",
    "        if task == \"classification\":\n",
    "            try:\n",
    "                _tune_threshold_inplace(best_pipeline, X_train, y_train, search_cv)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] threshold tuning failed: {e}\")\n",
    "        print(\"\\nBest model had no tunable params. Returning fitted pipeline.\")\n",
    "        return best_pipeline\n",
    "\n",
    "    # hyperparameter search\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=best_pipeline,\n",
    "        param_distributions=search_space,\n",
    "        n_iter=int(max(1, num_iterations)),\n",
    "        scoring=scoring,\n",
    "        cv=search_cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # print tuned CV result\n",
    "    if task == \"regression\":\n",
    "        tuned_score_display = -float(search.best_score_)\n",
    "        tuned_metric_name = \"CV MAE\"\n",
    "    else:\n",
    "        tuned_score_display = float(search.best_score_)\n",
    "        tuned_metric_name = \"F1 macro\"\n",
    "\n",
    "    print(\"\\n=== Best model after randomized search ===\")\n",
    "    print(f\"Model name: {best_model_name}\")\n",
    "    print(f\"Number of features: {best_k}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best CV score ({tuned_metric_name}): {tuned_score_display:.6f}\")\n",
    "\n",
    "    # remember training columns\n",
    "    try:\n",
    "        search.best_estimator_.input_columns_ = list(X_train.columns)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # threshold tuning only for classification\n",
    "    if task == \"classification\":\n",
    "        try:\n",
    "            _tune_threshold_inplace(search.best_estimator_, X_train, y_train, search_cv)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] threshold tuning failed: {e}\")\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "def _tune_threshold_inplace(fitted_estimator, X, y, cv):\n",
    "    \"\"\"\n",
    "    Finds a good decision threshold using out-of-fold scores on the training set.\n",
    "    Stores results on the estimator as .best_threshold_ and .best_threshold_cv_f1_.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    # try probabilities first\n",
    "    scores = None\n",
    "    try:\n",
    "        proba_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"predict_proba\", n_jobs=1)  # shape (n, 2)\n",
    "        scores = proba_oof[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback to decision function\n",
    "    if scores is None:\n",
    "        try:\n",
    "            decision_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"decision_function\", n_jobs=1)\n",
    "            dec_min, dec_max = float(decision_oof.min()), float(decision_oof.max())\n",
    "            scores = (decision_oof - dec_min) / (dec_max - dec_min + 1e-9)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # if no scores available, keep default threshold\n",
    "    if scores is None:\n",
    "        fitted_estimator.best_threshold_ = 0.5\n",
    "        fitted_estimator.best_threshold_cv_f1_ = None\n",
    "        print(\"[info] model does not expose scores for thresholding. Using 0.5.\")\n",
    "        return\n",
    "\n",
    "    # sweep thresholds\n",
    "    best_threshold = 0.5\n",
    "    best_f1_macro = -1.0\n",
    "    thresholds_to_try = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    for t in thresholds_to_try:\n",
    "        y_hat = (scores >= t).astype(int)\n",
    "        f1_macro_val = float(f1_score(y, y_hat, average=\"macro\"))\n",
    "        if f1_macro_val > best_f1_macro:\n",
    "            best_f1_macro = f1_macro_val\n",
    "            best_threshold = float(t)\n",
    "\n",
    "    # show OOF result at best threshold\n",
    "    y_hat_final = (scores >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Threshold tuning (OOF on train) ===\")\n",
    "    print(f\"Best threshold: {best_threshold:.2f} | F1_macro: {best_f1_macro:.6f}\")\n",
    "    print(\"Confusion matrix at best threshold:\")\n",
    "    print(confusion_matrix(y, y_hat_final))\n",
    "\n",
    "    # store on estimator\n",
    "    fitted_estimator.best_threshold_ = best_threshold\n",
    "    fitted_estimator.best_threshold_cv_f1_ = best_f1_macro\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Holdout evaluation helper (uses tuned threshold if available)\n",
    "# =============================\n",
    "def evaluate_on_holdout(model, X_test, y_test, task_type, threshold=None):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import f1_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "    # align columns to what the model saw at fit\n",
    "    try:\n",
    "        if hasattr(model, \"input_columns_\") and hasattr(X_test, \"reindex\"):\n",
    "            X_test = X_test.reindex(columns=model.input_columns_, fill_value=0)\n",
    "        elif hasattr(model, \"named_steps\") and \"align\" in getattr(model, \"named_steps\", {}):\n",
    "            keep_cols = getattr(model.named_steps[\"align\"], \"keep_columns_\", None)\n",
    "            if keep_cols is not None and hasattr(X_test, \"reindex\"):\n",
    "                X_test = X_test.reindex(columns=list(keep_cols), fill_value=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not align columns: {e}\")\n",
    "\n",
    "    # choose prediction path\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        final_threshold = threshold\n",
    "        if final_threshold is None and hasattr(model, \"best_threshold_\"):\n",
    "            final_threshold = float(model.best_threshold_)\n",
    "        if final_threshold is not None:\n",
    "            y_pred = predict_with_threshold(model, X_test, threshold=final_threshold)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n=== Holdout (time split) ===\")\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(f\"F1 macro: {f1:.6f}\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        return f1\n",
    "    else:\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93693335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Timer + memory helpers\n",
    "# =========================\n",
    "class SimpleTimer:\n",
    "    def __init__(self, enabled=True):\n",
    "        self.enabled = enabled\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "    def tick(self, label):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        t = time.perf_counter() - self.t0\n",
    "        print(f\"[timer] {label}: {t:.2f} s\")\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "def df_mem_gb(df):\n",
    "    try:\n",
    "        return float(df.memory_usage(deep=True).sum()) / (1024**3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def show_shape_mem(label, X_train=None, X_test=None):\n",
    "    parts = [label]\n",
    "    if X_train is not None:\n",
    "        parts.append(f\"X_train shape={tuple(X_train.shape)} mem={df_mem_gb(X_train):.3f} GB\")\n",
    "    if X_test is not None:\n",
    "        parts.append(f\"X_test shape={tuple(X_test.shape)} mem={df_mem_gb(X_test):.3f} GB\")\n",
    "    print(\"[info]\", \" | \".join(parts))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Text feature helpers (fast)\n",
    "# =========================\n",
    "def clean_keyword_name(s):\n",
    "    s = str(s).lower().strip().replace(\" \", \"_\")\n",
    "    keep = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch == \"_\":\n",
    "            keep.append(ch)\n",
    "    return \"\".join(keep)[:60]\n",
    "\n",
    "def text_features_fit(X, keyword_map):\n",
    "    keyword_map = keyword_map or {}\n",
    "    new_cols = []\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        if col not in X.columns:\n",
    "            continue\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"]:\n",
    "            continue\n",
    "        if col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            safe = clean_keyword_name(kw)\n",
    "            name = f\"{col}_has_{safe}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        df_part = pd.DataFrame(part, index=X.index)\n",
    "        new_parts.append(df_part)\n",
    "        new_cols.extend(df_part.columns.tolist())\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    return {\"new_cols\": new_cols, \"keyword_map\": keyword_map}\n",
    "\n",
    "def text_features_apply(X, text_info):\n",
    "    keyword_map = text_info.get(\"keyword_map\") or {}\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        if col not in X.columns:\n",
    "            part = {\n",
    "                len_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "                wc_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "            }\n",
    "            for kw in keywords:\n",
    "                name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "                part[name] = pd.Series(0, index=X.index, dtype=np.uint8)\n",
    "            new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "            continue\n",
    "\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"] or col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    for c in text_info.get(\"new_cols\", []):\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# General helpers\n",
    "# =========================\n",
    "def datetimes_to_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        if np.issubdtype(X[c].dtype, np.datetime64):\n",
    "            mask = X[c].isna()\n",
    "            vals_int = X[c].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "            arr = vals_int.astype(\"float64\") / 1000000000.0\n",
    "            if mask.any():\n",
    "                arr[mask.values] = np.nan\n",
    "            X[c] = arr\n",
    "    return X\n",
    "\n",
    "def downcast_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        dt = X[c].dtype\n",
    "        if np.issubdtype(dt, np.floating):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "        elif np.issubdtype(dt, np.integer) and X[c].nunique(dropna=True) > 2:\n",
    "            X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "def scale_fit(method, X_train_num):\n",
    "    if method == \"standard\":\n",
    "        return StandardScaler().fit(X_train_num)\n",
    "    if method == \"minmax\":\n",
    "        return MinMaxScaler().fit(X_train_num)\n",
    "    return None\n",
    "\n",
    "def scale_numeric_only(X_train, X_test, scale_method):\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return X_train, X_test\n",
    "    scaler = scale_fit(scale_method, X_train[num_cols])\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols]).astype(\"float32\")\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols]).astype(\"float32\")\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Safer OHE with caps and drop of excluded\n",
    "# =========================\n",
    "def _auto_exclude_mask(series, max_unique=500, max_avg_len=25):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    nunq = int(s.nunique(dropna=False))\n",
    "    avg_len = float(s.map(len).mean())\n",
    "    return (nunq > max_unique) or (avg_len > max_avg_len and nunq > 50)\n",
    "\n",
    "def _cap_categories(series, top_k=50, min_freq=5, other_label=\"Other\"):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    vc = s.value_counts()\n",
    "    kept = vc[vc >= min_freq].index.tolist()\n",
    "    if top_k is not None and len(kept) > top_k:\n",
    "        kept = vc.index[:top_k].tolist()\n",
    "    mapped = s.where(s.isin(kept), other_label)\n",
    "    return mapped.astype(\"category\"), kept\n",
    "\n",
    "def ohe_fit(\n",
    "    X,\n",
    "    exclude_cols=None,\n",
    "    top_k_per_col=50,\n",
    "    min_freq_per_col=5,\n",
    "    auto_exclude=True,\n",
    "    high_card_threshold=500,\n",
    "    long_text_avglen=25,\n",
    "):\n",
    "    exclude = set(exclude_cols or [])\n",
    "    value_map = {}\n",
    "\n",
    "    X_tmp = X.drop(columns=list(exclude), errors=\"ignore\").copy()\n",
    "\n",
    "    obj_all = [c for c in X_tmp.select_dtypes(include=[\"object\", \"category\"]).columns]\n",
    "\n",
    "    auto_excluded = []\n",
    "    for c in obj_all:\n",
    "        s = X_tmp[c]\n",
    "        if auto_exclude and _auto_exclude_mask(s, high_card_threshold, long_text_avglen):\n",
    "            auto_excluded.append(c)\n",
    "\n",
    "    excluded = list(exclude) + auto_excluded\n",
    "    X_tmp = X_tmp.drop(columns=auto_excluded, errors=\"ignore\")\n",
    "\n",
    "    obj_cols = [c for c in X_tmp.select_dtypes(include=[\"object\", \"category\"]).columns]\n",
    "    for c in obj_cols:\n",
    "        capped, kept = _cap_categories(X_tmp[c], top_k=top_k_per_col, min_freq=min_freq_per_col)\n",
    "        X_tmp[c] = capped\n",
    "        value_map[c] = kept\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=obj_cols, dummy_na=False)\n",
    "    schema_cols = X_ohe.columns.tolist()\n",
    "\n",
    "    return {\n",
    "        \"obj_cols\": obj_cols,\n",
    "        \"schema_cols\": schema_cols,\n",
    "        \"value_map\": value_map,\n",
    "        \"excluded\": excluded,\n",
    "        \"other_label\": \"Other\",\n",
    "    }\n",
    "\n",
    "def ohe_apply(X, ohe_info):\n",
    "    obj_cols = ohe_info[\"obj_cols\"]\n",
    "    schema_cols = ohe_info[\"schema_cols\"]\n",
    "    value_map = ohe_info[\"value_map\"]\n",
    "    other = ohe_info.get(\"other_label\", \"Other\")\n",
    "    excluded = ohe_info.get(\"excluded\", [])\n",
    "\n",
    "    X_tmp = X.drop(columns=excluded, errors=\"ignore\").copy()\n",
    "\n",
    "    for c in obj_cols:\n",
    "        if c in X_tmp.columns:\n",
    "            s = X_tmp[c].fillna(\"Unknown\").astype(str)\n",
    "            kept = set(value_map.get(c, []))\n",
    "            s = s.where(s.isin(kept), other).astype(\"category\")\n",
    "            X_tmp[c] = s\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=[c for c in obj_cols if c in X_tmp.columns], dummy_na=False)\n",
    "    X_ohe = X_ohe.reindex(columns=schema_cols, fill_value=0)\n",
    "    return X_ohe\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Outliers\n",
    "# =========================\n",
    "def outlier_bounds_fit(X_num, lower_q=0.025, upper_q=0.975, exclude_binary=True, sample_rows=200000):\n",
    "    bounds = {}\n",
    "    if lower_q is None or upper_q is None:\n",
    "        return bounds\n",
    "    X_use = X_num\n",
    "    if len(X_num) > sample_rows:\n",
    "        X_use = X_num.sample(n=sample_rows, random_state=123)\n",
    "    for c in X_use.columns:\n",
    "        vals = X_use[c].astype(\"float32\")\n",
    "        if exclude_binary and X_use[c].nunique(dropna=True) <= 2:\n",
    "            continue\n",
    "        lo = np.nanquantile(vals, lower_q)\n",
    "        hi = np.nanquantile(vals, upper_q)\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and hi >= lo:\n",
    "            bounds[c] = (float(lo), float(hi))\n",
    "    return bounds\n",
    "\n",
    "def outlier_mask(X, bounds):\n",
    "    if not bounds:\n",
    "        return pd.Series(True, index=X.index)\n",
    "    m = pd.Series(True, index=X.index)\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in X.columns:\n",
    "            col = X[c].astype(\"float32\")\n",
    "            m &= (col >= lo) & (col <= hi)\n",
    "    return m\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature selection\n",
    "# =========================\n",
    "def forward_feature_selection(X, y, model,\n",
    "                              scoring=\"neg_mean_absolute_error\",\n",
    "                              cv=5, tol=None, max_features=None, n_jobs=-1, verbose=False):\n",
    "    try:\n",
    "        feature_names = list(X.columns)\n",
    "        X_arr = X.values\n",
    "    except AttributeError:\n",
    "        X_arr = X\n",
    "        feature_names = [f\"f{i}\" for i in range(X_arr.shape[1])]\n",
    "\n",
    "    selected_idx = []\n",
    "    remaining_idx = list(range(X_arr.shape[1]))\n",
    "    best_scores = []\n",
    "    previous_score = float(\"inf\")\n",
    "    best_feature_set_idx = []\n",
    "    best_score = float(\"inf\")\n",
    "\n",
    "    while remaining_idx:\n",
    "        scores = {}\n",
    "        for idx in remaining_idx:\n",
    "            trial_idx = selected_idx + [idx]\n",
    "            cv_score = -cross_val_score(\n",
    "                model, X_arr[:, trial_idx], y,\n",
    "                scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "            ).mean()\n",
    "            scores[idx] = cv_score\n",
    "\n",
    "        best_idx = min(scores, key=scores.get)\n",
    "        current_score = scores[best_idx]\n",
    "\n",
    "        if tol is not None and previous_score - current_score < tol:\n",
    "            if verbose:\n",
    "                print(\"Stopping early (improvement < tol).\")\n",
    "            break\n",
    "\n",
    "        selected_idx.append(best_idx)\n",
    "        remaining_idx.remove(best_idx)\n",
    "        best_scores.append(current_score)\n",
    "        previous_score = current_score\n",
    "\n",
    "        if verbose:\n",
    "            name = feature_names[best_idx]\n",
    "            print(f\"Added {name} -> CV score = {current_score:.4f}\")\n",
    "\n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_feature_set_idx = selected_idx.copy()\n",
    "\n",
    "        if max_features is not None and len(selected_idx) >= max_features:\n",
    "            break\n",
    "\n",
    "    selected_features = [feature_names[i] for i in selected_idx]\n",
    "    best_feature_set = [feature_names[i] for i in best_feature_set_idx]\n",
    "\n",
    "    if not best_feature_set:\n",
    "        best_feature_set = selected_features[:]\n",
    "        best_score = best_scores[-1] if best_scores else float(\"inf\")\n",
    "\n",
    "    try:\n",
    "        index = np.argmax(np.array(selected_features) == best_feature_set[-1])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(best_scores) + 1), best_scores, marker=\".\")\n",
    "        plt.plot([index + 1], [best_score], marker=\"x\")\n",
    "        plt.xticks(range(1, len(selected_features) + 1),\n",
    "                   selected_features, rotation=60, ha=\"right\", fontsize=6)\n",
    "        plt.title(\"Forward Feature Selection and CV Scores\")\n",
    "        plt.xlabel(\"Features Added\")\n",
    "        plt.ylabel(\"CV Score (MAE)\")\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"Best Features: {best_feature_set}\")\n",
    "    print(f\"Best CV MAE Score: {best_score:.4f}\")\n",
    "    return selected_features, best_scores, best_feature_set, best_score\n",
    "\n",
    "def select_features(method, max_features, task_type, random_state, X_train, y_train):\n",
    "    if method is None or method == \"none\":\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    k = min(max_features, X_train.shape[1]) if max_features else X_train.shape[1]\n",
    "    if k < 1:\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    if method == \"tree\":\n",
    "        est = RandomForestClassifier(random_state=random_state) if task_type == \"classification\" else RandomForestRegressor(random_state=random_state)\n",
    "        est.fit(X_train, y_train)\n",
    "        imp = pd.Series(est.feature_importances_, index=X_train.columns)\n",
    "        return imp.nlargest(k).index.tolist()\n",
    "\n",
    "    if method == \"forward\":\n",
    "        if task_type != \"regression\":\n",
    "            raise ValueError(\"Forward selection is only supported for regression tasks.\")\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        _, _, best_set, _ = forward_feature_selection(\n",
    "            X=X_train, y=y_train, model=model,\n",
    "            scoring=\"neg_mean_absolute_error\", cv=3,\n",
    "            tol=None, max_features=max_features, n_jobs=-1, verbose=True\n",
    "        )\n",
    "        return best_set\n",
    "\n",
    "    if method == \"mutual_info\":\n",
    "        sel = SelectKBest(mutual_info_classif if task_type == \"classification\" else mutual_info_regression, k=k)\n",
    "        sel.fit(X_train, y_train)\n",
    "        return X_train.columns[sel.get_support()].tolist()\n",
    "\n",
    "    return X_train.columns.tolist()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Batched poly features\n",
    "# =========================\n",
    "def add_poly_features_batched(X_train, X_test, squares, pairs):\n",
    "    train_parts = {}\n",
    "    for c in squares:\n",
    "        if c in X_train.columns:\n",
    "            train_parts[f\"{c}_sq\"] = X_train[c].astype(\"float32\") ** 2\n",
    "    for a, b in pairs:\n",
    "        if (a in X_train.columns) and (b in X_train.columns):\n",
    "            name = f\"{a}_x_{b}\"\n",
    "            train_parts[name] = (X_train[a].astype(\"float32\") * X_train[b].astype(\"float32\"))\n",
    "\n",
    "    test_parts = {}\n",
    "    for name in train_parts:\n",
    "        if name.endswith(\"_sq\"):\n",
    "            c = name[:-3]\n",
    "            if c in X_test.columns:\n",
    "                test_parts[name] = X_test[c].astype(\"float32\") ** 2\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "        else:\n",
    "            a, b = name.split(\"_x_\")\n",
    "            if (a in X_test.columns) and (b in X_test.columns):\n",
    "                test_parts[name] = (X_test[a].astype(\"float32\") * X_test[b].astype(\"float32\"))\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "\n",
    "    if train_parts:\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(train_parts, index=X_train.index)], axis=1)\n",
    "    if test_parts:\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(test_parts, index=X_test.index)], axis=1)\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train.copy())\n",
    "    X_test = downcast_numeric_inplace(X_test.copy())\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main prep\n",
    "# =========================\n",
    "def prepare_data(steam_df, olist_df, sales_df, test_size, random_state,\n",
    "                 feature_selection, max_features, task_type, scale_method,\n",
    "                 tag_min_count=5, tag_top_k=200,\n",
    "                 outlier_lower_q=0.025, outlier_upper_q=0.975,\n",
    "                 verbose=False,\n",
    "                 ohe_top_k_per_col=50,\n",
    "                 ohe_min_freq_per_col=5,\n",
    "                 ohe_auto_exclude=True,\n",
    "                 ohe_high_card_threshold=500,\n",
    "                 ohe_long_text_avglen=25):\n",
    "    feature_selection = (feature_selection or \"none\").lower()\n",
    "    outputs = {}\n",
    "    timer = SimpleTimer(enabled=verbose)\n",
    "\n",
    "    # ---------- STEAM ----------\n",
    "    steam = steam_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        steam[\"target\"] = (steam[\"positive_ratio\"] >= 80).astype(int)\n",
    "    else:\n",
    "        steam[\"target\"] = steam[\"positive_ratio\"]\n",
    "    steam.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    steam.drop(columns=[\"app_id\", \"user_id\", \"review_id\", \"positive_ratio\"],\n",
    "               errors=\"ignore\", inplace=True)\n",
    "\n",
    "    if {\"date\", \"date_release\"}.issubset(steam.columns):\n",
    "        steam[\"days_since_release\"] = (steam[\"date\"] - steam[\"date_release\"]).dt.days\n",
    "\n",
    "    for col in [\"is_recommended\", \"mac\", \"linux\", \"win\", \"steam_deck\"]:\n",
    "        if col in steam.columns:\n",
    "            steam[col] = steam[col].astype(int)\n",
    "\n",
    "    if \"hours\" in steam.columns:\n",
    "        steam[\"log_hours\"] = np.log1p(steam[\"hours\"])\n",
    "    if {\"hours\", \"user_reviews\"}.issubset(steam.columns):\n",
    "        steam[\"reviews_per_hour\"] = steam[\"user_reviews\"] / (steam[\"hours\"] + 0.000000001)\n",
    "\n",
    "    X = steam.drop(columns=[\"target\", \"rating\"], errors=\"ignore\")\n",
    "    y = steam[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"steam split\")\n",
    "    show_shape_mem(\"steam post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"steam datetime to numeric\")\n",
    "\n",
    "    steam_kw = {\n",
    "        \"title\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"coop\", \"online\", \"free\", \"demo\", \"survival\"],\n",
    "        \"description\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"open world\", \"story\", \"puzzle\", \"horror\", \"early access\"],\n",
    "    }\n",
    "    steam_text_info = text_features_fit(X_train, steam_kw)\n",
    "    X_test = text_features_apply(X_test, steam_text_info)\n",
    "    timer.tick(\"steam text features\")\n",
    "    show_shape_mem(\"steam after text\", X_train, X_test)\n",
    "\n",
    "    if \"tags\" in X_train.columns:\n",
    "        from collections import Counter\n",
    "\n",
    "        def tag_col_name(t):\n",
    "            s = str(t).lower().strip().replace(\" \", \"_\")\n",
    "            return \"tag_\" + \"\".join(ch for ch in s if ch.isalnum() or ch == \"_\")[:60]\n",
    "\n",
    "        cnt = Counter()\n",
    "        for v in X_train[\"tags\"].fillna(\"\").values:\n",
    "            lst = v if isinstance(v, list) else []\n",
    "            for t in lst:\n",
    "                cnt[t] += 1\n",
    "\n",
    "        items = [(t, n) for t, n in cnt.items() if n >= tag_min_count]\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        vocab = [t for t, _ in items[:tag_top_k]]\n",
    "        tag_cols = [tag_col_name(t) for t in vocab]\n",
    "        if verbose:\n",
    "            print(f\"[info] steam tags unique={len(cnt)}, kept={len(vocab)} (min_count={tag_min_count}, top_k={tag_top_k})\")\n",
    "\n",
    "        def add_tag_cols_fast(df):\n",
    "            if \"tags\" in df.columns:\n",
    "                tag_lists = df[\"tags\"].apply(lambda v: v if isinstance(v, list) else [])\n",
    "            else:\n",
    "                tag_lists = pd.Series([[]] * len(df), index=df.index)\n",
    "            new_data = {}\n",
    "            for tag, col_name in zip(vocab, tag_cols):\n",
    "                new_data[col_name] = np.fromiter(\n",
    "                    (1 if tag in lst else 0 for lst in tag_lists),\n",
    "                    dtype=np.uint8,\n",
    "                    count=len(df)\n",
    "                )\n",
    "            new_df = pd.DataFrame(new_data, index=df.index)\n",
    "            return pd.concat([df.drop(columns=[\"tags\"], errors=\"ignore\"), new_df], axis=1)\n",
    "\n",
    "        X_train = add_tag_cols_fast(X_train)\n",
    "        X_test = add_tag_cols_fast(X_test)\n",
    "        timer.tick(\"steam tags multi-hot\")\n",
    "        show_shape_mem(\"steam after tags\", X_train, X_test)\n",
    "\n",
    "    explicit_exclude = [c for c in [\"title\", \"description\"] if c in X_train.columns]\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=explicit_exclude,\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    if verbose and ohe_info.get(\"excluded\"):\n",
    "        print(f\"[info] OHE auto-excluded (steam): {sorted(ohe_info['excluded'])[:10]}{'...' if len(ohe_info['excluded'])>10 else ''}\")\n",
    "\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"steam OHE\")\n",
    "    show_shape_mem(\"steam after OHE\", X_train, X_test)\n",
    "\n",
    "    # numeric-only safety\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "    if verbose:\n",
    "        print(\"steam non-numeric after OHE:\", X_train.select_dtypes(exclude=[\"number\"]).columns.tolist())\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"steam impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    show_shape_mem(\"steam after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    steam_squares = []\n",
    "    if \"log_hours\" in X_train.columns:\n",
    "        steam_squares.append(\"log_hours\")\n",
    "    elif \"hours\" in X_train.columns:\n",
    "        steam_squares.append(\"hours\")\n",
    "    if \"discount\" in X_train.columns:\n",
    "        steam_squares.append(\"discount\")\n",
    "    if \"days_since_release\" in X_train.columns:\n",
    "        steam_squares.append(\"days_since_release\")\n",
    "\n",
    "    steam_pairs = []\n",
    "    if (\"price_final\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_final\", \"discount\"))\n",
    "    if (\"price_original\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_original\", \"discount\"))\n",
    "    if (\"days_since_release\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"days_since_release\", \"discount\"))\n",
    "    if (\"user_reviews\" in X_train.columns) and (\"reviews\" in X_train.columns):\n",
    "        steam_pairs.append((\"user_reviews\", \"reviews\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, steam_squares, steam_pairs)\n",
    "    timer.tick(\"steam poly\")\n",
    "    show_shape_mem(\"steam after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    m_tr = outlier_mask(X_train, bounds)\n",
    "    m_te = outlier_mask(X_test, bounds)\n",
    "    X_train = X_train[m_tr]\n",
    "    y_train = y[y.index.isin(X_train.index)]\n",
    "    X_test = X_test[m_te]\n",
    "    y_test = y[y.index.isin(X_test.index)]\n",
    "    timer.tick(\"steam outlier filter\")\n",
    "    show_shape_mem(\"steam after outlier\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"steam scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"steam select features\")\n",
    "    show_shape_mem(\"steam after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"steam\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- OLIST ----------\n",
    "    olist = olist_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        olist[\"target\"] = (olist[\"review_score_mean_product\"] >= 4.0).astype(int)\n",
    "    else:\n",
    "        olist[\"target\"] = olist[\"review_score_mean_product\"]\n",
    "    olist.dropna(subset=[\"target\"], inplace=True)\n",
    "    olist.drop(columns=[\"order_id\", \"customer_id\", \"customer_unique_id\"],\n",
    "               errors=\"ignore\", inplace=True)\n",
    "\n",
    "    olist[\"delivery_delay\"] = (olist[\"order_estimated_delivery_date\"] - olist[\"order_purchase_timestamp\"]).dt.days\n",
    "    denom = olist[\"payment_installments_max\"].replace(0, 1)\n",
    "    olist[\"avg_installment\"] = olist[\"payment_value_total\"] / denom\n",
    "\n",
    "    if {\"product_length_cm\", \"product_width_cm\", \"product_height_cm\"}.issubset(olist.columns):\n",
    "        olist[\"product_volume_cm3\"] = (\n",
    "            olist[\"product_length_cm\"] * olist[\"product_width_cm\"] * olist[\"product_height_cm\"]\n",
    "        )\n",
    "\n",
    "    X = olist.drop(columns=[\"review_score_mean_product\", \"target\"], errors=\"ignore\")\n",
    "    y = olist[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"olist split\")\n",
    "    show_shape_mem(\"olist post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"olist datetime to numeric\")\n",
    "\n",
    "    olist_kw = {\n",
    "        \"order_status\": [\"delivered\", \"shipped\", \"canceled\", \"invoiced\", \"processing\"],\n",
    "        \"product_category_name\": [\"moveis\", \"auto\", \"pet\", \"perfumaria\", \"utilidades\", \"brinquedos\"]\n",
    "    }\n",
    "    olist_text_info = text_features_fit(X_train, olist_kw)\n",
    "    X_test = text_features_apply(X_test, olist_text_info)\n",
    "    timer.tick(\"olist text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],  # add explicit excludes if needed\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    if verbose and ohe_info.get(\"excluded\"):\n",
    "        print(f\"[info] OHE auto-excluded (olist): {sorted(ohe_info['excluded'])[:10]}{'...' if len(ohe_info['excluded'])>10 else ''}\")\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"olist OHE\")\n",
    "    show_shape_mem(\"olist after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"olist impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    show_shape_mem(\"olist after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    olist_squares = []\n",
    "    if \"delivery_delay\" in X_train.columns:\n",
    "        olist_squares.append(\"delivery_delay\")\n",
    "    if \"price\" in X_train.columns:\n",
    "        olist_squares.append(\"price\")\n",
    "    if \"freight_value\" in X_train.columns:\n",
    "        olist_squares.append(\"freight_value\")\n",
    "\n",
    "    olist_pairs = []\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_weight_g\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_weight_g\"))\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_volume_cm3\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_volume_cm3\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"price\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"freight_value\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"freight_value\"))\n",
    "    if (\"payment_installments_max\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"payment_installments_max\", \"price\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, olist_squares, olist_pairs)\n",
    "    timer.tick(\"olist poly\")\n",
    "    show_shape_mem(\"olist after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    m_tr = outlier_mask(X_train, bounds)\n",
    "    m_te = outlier_mask(X_test, bounds)\n",
    "    X_train = X_train[m_tr]\n",
    "    y_train = y[y.index.isin(X_train.index)]\n",
    "    X_test = X_test[m_te]\n",
    "    y_test = y[y.index.isin(X_test.index)]\n",
    "    timer.tick(\"olist outlier filter\")\n",
    "    show_shape_mem(\"olist after outlier\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"olist scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"olist select features\")\n",
    "    show_shape_mem(\"olist after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"olist\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- SALES ----------\n",
    "    sales = sales_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        sales[\"target\"] = (sales[\"Critic_Score\"] >= 8.0).astype(int)\n",
    "    else:\n",
    "        sales[\"target\"] = sales[\"Critic_Score\"]\n",
    "    sales.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    for c in [\"ESRB_Rating\", \"Genre\", \"Platform\", \"Publisher\", \"Developer\"]:\n",
    "        if c in sales.columns:\n",
    "            sales[c] = sales[c].fillna(\"Unknown\")\n",
    "\n",
    "    X = sales.drop(columns=[\"target\", \"Critic_Score\"], errors=\"ignore\")\n",
    "    y = sales[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"sales split\")\n",
    "    show_shape_mem(\"sales post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"sales datetime to numeric\")\n",
    "\n",
    "    sales_kw = {\n",
    "        \"Name\": [\"mario\", \"pokemon\", \"zelda\", \"call of duty\", \"fifa\", \"minecraft\", \"final fantasy\"],\n",
    "        \"Genre\": [\"action\", \"sports\", \"shooter\", \"racing\", \"role\", \"adventure\", \"platform\", \"puzzle\"],\n",
    "        \"Publisher\": [\"nintendo\", \"electronic arts\", \"ea\", \"activision\", \"ubisoft\", \"sony\", \"sega\"],\n",
    "        \"ESRB_Rating\": [\"e\", \"t\", \"m\"]\n",
    "    }\n",
    "    sales_text_info = text_features_fit(X_train, sales_kw)\n",
    "    X_test = text_features_apply(X_test, sales_text_info)\n",
    "    timer.tick(\"sales text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],  # add explicit excludes if needed\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    if verbose and ohe_info.get(\"excluded\"):\n",
    "        print(f\"[info] OHE auto-excluded (sales): {sorted(ohe_info['excluded'])[:10]}{'...' if len(ohe_info['excluded'])>10 else ''}\")\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"sales OHE\")\n",
    "    show_shape_mem(\"sales after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"sales impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    show_shape_mem(\"sales after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    sales_squares = []\n",
    "    if \"Year\" in X_train.columns:\n",
    "        sales_squares.append(\"Year\")\n",
    "    if \"User_Score\" in X_train.columns:\n",
    "        sales_squares.append(\"User_Score\")\n",
    "\n",
    "    sales_pairs = []\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"PAL_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"PAL_Sales\"))\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"JP_Sales\"))\n",
    "    if (\"PAL_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"PAL_Sales\", \"JP_Sales\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, sales_squares, sales_pairs)\n",
    "    timer.tick(\"sales poly\")\n",
    "    show_shape_mem(\"sales after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    m_tr = outlier_mask(X_train, bounds)\n",
    "    m_te = outlier_mask(X_test, bounds)\n",
    "    X_train = X_train[m_tr]\n",
    "    y_train = y[y.index.isin(X_train.index)]\n",
    "    X_test = X_test[m_te]\n",
    "    y_test = y[y.index.isin(X_test.index)]\n",
    "    timer.tick(\"sales outlier filter\")\n",
    "    show_shape_mem(\"sales after outlier\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"sales scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"sales select features\")\n",
    "    show_shape_mem(\"sales after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"sales\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- Shape summary ----------\n",
    "    for name, parts in outputs.items():\n",
    "        Xtr, Xte, ytr, yte = parts\n",
    "        print(f\"[{name}] X_train: {Xtr.shape} | X_test: {Xte.shape} | y_train: {ytr.shape} | y_test: {yte.shape}\")\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18a3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.315 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.173 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.184 sec\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 1000001 of 41154794 rows in 5.889 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(1000001, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: taking all rows\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (113425, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: taking all rows\n",
      "vg2019: done shape=(55792, 16)\n",
      "main: load all done in 17.441 sec (00:00:17)\n",
      "download: shapes summary\n",
      "download: steam shape = (1000001, 24)\n",
      "download: olist shape = (113425, 38)\n",
      "download: sales shape = (55792, 16)\n"
     ]
    }
   ],
   "source": [
    "# Download Paths\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "\n",
    "# Load All\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# Download Shapes\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e087b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Robust EDA Report: steam ===\n",
      "\n",
      "=== Info ===\n",
      "   rows  columns  memory_bytes\n",
      "1000001       24     588511404\n",
      "\n",
      "=== Dtypes ===\n",
      "        column          dtype\n",
      "        app_id          int64\n",
      "          date datetime64[ns]\n",
      "  date_release datetime64[ns]\n",
      "   description         object\n",
      "      discount        float64\n",
      "         funny          int64\n",
      "       helpful          int64\n",
      "         hours        float64\n",
      "is_recommended           bool\n",
      "         linux           bool\n",
      "           mac           bool\n",
      "positive_ratio          int64\n",
      "   price_final        float64\n",
      "price_original        float64\n",
      "      products          int64\n",
      "        rating         object\n",
      "     review_id          int64\n",
      "       reviews          int64\n",
      "    steam_deck           bool\n",
      "          tags         object\n",
      "         title         object\n",
      "       user_id          int64\n",
      "  user_reviews          int64\n",
      "           win           bool\n",
      "\n",
      "=== Missing Values ===\n",
      "        column  missing_count  missing_percent\n",
      "        app_id              0         0.000000\n",
      "       helpful              0         0.000000\n",
      "         funny              0         0.000000\n",
      "          date              0         0.000000\n",
      "is_recommended              0         0.000000\n",
      "         hours              0         0.000000\n",
      "       user_id              0         0.000000\n",
      "     review_id              0         0.000000\n",
      "         title              0         0.000000\n",
      "  date_release              0         0.000000\n",
      "           win              0         0.000000\n",
      "           mac              0         0.000000\n",
      "         linux              0         0.000000\n",
      "        rating              0         0.000000\n",
      "positive_ratio              0         0.000000\n",
      "  user_reviews              0         0.000000\n",
      "   price_final              0         0.000000\n",
      "price_original              0         0.000000\n",
      "      discount              0         0.000000\n",
      "    steam_deck              0         0.000000\n",
      "   description              0         0.000000\n",
      "          tags              0         0.000000\n",
      "      products              0         0.000000\n",
      "       reviews              0         0.000000\n",
      "\n",
      "=== Duplicates Summary ===\n",
      " duplicate_rows\n",
      "              0\n",
      "\n",
      "=== Duplicates Preview (up to 20 rows) ===\n",
      "(No duplicate rows found.)\n",
      "\n",
      "=== Numeric Summary (5%..95%) ===\n",
      "        column          count            mean             std       min             5%             25%             50%             75%             95%             max\n",
      "        app_id 1000001.000000   603802.619586   472475.958557 10.000000    8500.000000   254700.000000   436150.000000   933110.000000  1544020.000000  2241860.000000\n",
      "       helpful 1000001.000000        3.147633       39.967256  0.000000       0.000000        0.000000        0.000000        0.000000        8.000000    12337.000000\n",
      "         funny 1000001.000000        1.052633       27.111337  0.000000       0.000000        0.000000        0.000000        0.000000        2.000000    11410.000000\n",
      "         hours 1000001.000000      100.640346      176.353283  0.000000       0.900000        7.800000       27.300000       99.200000      518.100000     1000.000000\n",
      "       user_id 1000001.000000  7451712.057089  4011164.563344  0.000000  790078.000000  4290895.000000  7550701.000000 10976223.000000 13581373.000000 14306060.000000\n",
      "     review_id 1000001.000000 20570854.487421 11880951.046380 82.000000 2059824.000000 10283794.000000 20564982.000000 30849831.000000 39101627.000000 41154746.000000\n",
      "positive_ratio 1000001.000000       86.131060       11.263825  0.000000      62.000000       82.000000       89.000000       94.000000       97.000000      100.000000\n",
      "  user_reviews 1000001.000000   178113.377705   600533.298958 10.000000     589.000000    10073.000000    46016.000000   138503.000000   637341.000000  7494460.000000\n",
      "   price_final 1000001.000000       18.661480       16.608594  0.000000       0.000000        4.990000       15.000000       29.990000       59.990000      269.990000\n",
      "price_original 1000001.000000        7.339585       11.955246  0.000000       0.000000        0.000000        0.000000       14.990000       29.990000      269.990000\n",
      "      discount 1000001.000000        3.141671       14.731670  0.000000       0.000000        0.000000        0.000000        0.000000        0.000000       90.000000\n",
      "      products 1000001.000000      281.775818      658.781570  0.000000      12.000000       50.000000      119.000000      273.000000     1000.000000    30108.000000\n",
      "       reviews 1000001.000000       24.869067      138.473945  1.000000       1.000000        2.000000        5.000000       15.000000       77.000000     6045.000000\n",
      "\n",
      "=== Skew and Kurtosis ===\n",
      "        column       skew     kurtosis\n",
      "        app_id   0.915646     0.028025\n",
      "       helpful  82.901730 14126.423939\n",
      "         funny 166.640502 48619.313657\n",
      "         hours   2.761034     7.778423\n",
      "       user_id  -0.114074    -1.097349\n",
      "     review_id   0.001314    -1.199978\n",
      "positive_ratio  -1.893584     4.911155\n",
      "  user_reviews  10.017308   115.798724\n",
      "   price_final   0.926367     0.435548\n",
      "price_original   1.977530     5.172365\n",
      "      discount   4.682959    20.647640\n",
      "      products  10.923548   204.514744\n",
      "       reviews  23.701503   758.848774\n",
      "\n",
      "=== IQR Outlier Counts ===\n",
      "        column  outlier_count\n",
      "        app_id           5134\n",
      "       helpful         210140\n",
      "         funny          63186\n",
      "         hours         127455\n",
      "       user_id              0\n",
      "     review_id              0\n",
      "positive_ratio          53096\n",
      "  user_reviews         120580\n",
      "   price_final           1442\n",
      "price_original          33780\n",
      "      discount          46428\n",
      "      products          97295\n",
      "       reviews         120731\n",
      "\n",
      "=== Correlation (first 30 numeric columns) ===\n",
      "                  app_id   helpful     funny     hours   user_id  review_id  positive_ratio  user_reviews  price_final  price_original  discount  products   reviews\n",
      "column                                                                                                                                                              \n",
      "app_id          1.000000  0.003797 -0.003372 -0.127575 -0.028903   0.018218       -0.179577     -0.155734     0.106930        0.015750 -0.033161 -0.022260 -0.002063\n",
      "helpful         0.003797  1.000000  0.499458  0.007515  0.004489  -0.003371       -0.018747      0.013515     0.006496        0.003415 -0.000437  0.036493  0.009343\n",
      "funny          -0.003372  0.499458  1.000000  0.005326  0.001438  -0.003354       -0.003286      0.011254     0.001725       -0.002513 -0.001118  0.009673  0.000670\n",
      "hours          -0.127575  0.007515  0.005326  1.000000  0.005302  -0.228546        0.007398      0.271708     0.043680       -0.158829 -0.079351 -0.080431 -0.061947\n",
      "user_id        -0.028903  0.004489  0.001438  0.005302  1.000000   0.015519       -0.013623     -0.021839     0.006165        0.028412  0.008981  0.058119  0.022937\n",
      "review_id       0.018218 -0.003371 -0.003354 -0.228546  0.015519   1.000000       -0.116582     -0.165371    -0.248442        0.206165  0.096775  0.104187  0.091177\n",
      "positive_ratio -0.179577 -0.018747 -0.003286  0.007398 -0.013623  -0.116582        1.000000      0.024435     0.020221       -0.100801 -0.012511 -0.054844 -0.056990\n",
      "user_reviews   -0.155734  0.013515  0.011254  0.271708 -0.021839  -0.165371        0.024435      1.000000    -0.049015       -0.158825 -0.053854 -0.058498 -0.034736\n",
      "price_final     0.106930  0.006496  0.001725  0.043680  0.006165  -0.248442        0.020221     -0.049015     1.000000        0.203542 -0.166698 -0.038859 -0.054995\n",
      "price_original  0.015750  0.003415 -0.002513 -0.158829  0.028412   0.206165       -0.100801     -0.158825     0.203542        1.000000  0.246429  0.076506  0.023155\n",
      "discount       -0.033161 -0.000437 -0.001118 -0.079351  0.008981   0.096775       -0.012511     -0.053854    -0.166698        0.246429  1.000000  0.041753  0.025866\n",
      "products       -0.022260  0.036493  0.009673 -0.080431  0.058119   0.104187       -0.054844     -0.058498    -0.038859        0.076506  0.041753  1.000000  0.365864\n",
      "reviews        -0.002063  0.009343  0.000670 -0.061947  0.022937   0.091177       -0.056990     -0.034736    -0.054995        0.023155  0.025866  0.365864  1.000000\n",
      "\n",
      "=== Categorical Values (Top 20 per column) ===\n",
      "        column               value  count   percent\n",
      "          date 2019-06-29 00:00:00   5290  0.530000\n",
      "          date 2021-11-24 00:00:00   3578  0.360000\n",
      "          date 2022-11-22 00:00:00   3538  0.350000\n",
      "          date 2022-11-23 00:00:00   3522  0.350000\n",
      "          date 2019-06-30 00:00:00   3489  0.350000\n",
      "          date 2021-11-25 00:00:00   3378  0.340000\n",
      "          date 2020-11-25 00:00:00   3333  0.330000\n",
      "          date 2020-11-26 00:00:00   3094  0.310000\n",
      "          date 2019-07-01 00:00:00   2755  0.280000\n",
      "          date 2021-11-26 00:00:00   2429  0.240000\n",
      "          date 2022-11-24 00:00:00   2416  0.240000\n",
      "          date 2018-11-22 00:00:00   2380  0.240000\n",
      "          date 2018-11-21 00:00:00   2144  0.210000\n",
      "          date 2019-11-26 00:00:00   2097  0.210000\n",
      "          date 2020-11-27 00:00:00   2085  0.210000\n",
      "          date 2019-11-27 00:00:00   2031  0.200000\n",
      "          date 2022-11-26 00:00:00   2017  0.200000\n",
      "          date 2021-11-27 00:00:00   1986  0.200000\n",
      "          date 2017-11-22 00:00:00   1978  0.200000\n",
      "          date 2016-11-23 00:00:00   1954  0.200000\n",
      "is_recommended                True 857845 85.780000\n",
      "is_recommended               False 142156 14.220000\n",
      "         title     Team Fortress 2   7761  0.780000\n",
      "         title                Rust   6609  0.660000\n",
      "         title      Cyberpunk 2077   5548  0.550000\n",
      "\n",
      "=== Unique Counts Per Column ===\n",
      "        column  unique_values\n",
      "        app_id   22777.000000\n",
      "       helpful     970.000000\n",
      "         funny     651.000000\n",
      "          date    4387.000000\n",
      "is_recommended       2.000000\n",
      "         hours   10000.000000\n",
      "       user_id  873669.000000\n",
      "     review_id 1000001.000000\n",
      "         title   22751.000000\n",
      "  date_release    3779.000000\n",
      "           win       2.000000\n",
      "           mac       2.000000\n",
      "         linux       2.000000\n",
      "        rating       9.000000\n",
      "positive_ratio      99.000000\n",
      "  user_reviews    4631.000000\n",
      "   price_final     290.000000\n",
      "price_original     113.000000\n",
      "      discount      58.000000\n",
      "    steam_deck       2.000000\n",
      "   description   22096.000000\n",
      "          tags            NaN\n",
      "      products    5220.000000\n",
      "       reviews     680.000000\n",
      "\n",
      "=== Head (10 rows) ===\n",
      " app_id  helpful  funny       date  is_recommended      hours  user_id  review_id                           title date_release  win   mac  linux                  rating  positive_ratio  user_reviews  price_final  price_original  discount  steam_deck description tags  products  reviews\n",
      "1174180        0      0 2021-01-18            True  30.000000  1518638         82           Red Dead Redemption 2   2019-12-05 True False  False           Very Positive              90        410074    20.000000        0.000000  0.000000        True               []        23        2\n",
      " 620980        0      0 2022-07-24            True  33.000000   628967         89                      Beat Saber   2019-05-21 True False  False Overwhelmingly Positive              95         63695    30.000000        0.000000  0.000000        True               []        72       14\n",
      " 582660        0      0 2021-03-19            True  35.100000  4270852        126                    Black Desert   2017-05-24 True False  False         Mostly Positive              76         49539    10.000000        0.000000  0.000000        True               []        55        1\n",
      " 359550        0      0 2016-12-26            True 743.800000  4469817        129 Tom Clancy's Rainbow Six® Siege   2015-12-01 True False  False           Very Positive              86        993312    20.000000        0.000000  0.000000        True               []        57        1\n",
      " 703080        0      0 2022-11-16           False   8.100000 11530062        214                      Planet Zoo   2019-11-05 True False  False           Very Positive              90         60113    45.000000        0.000000  0.000000        True               []       520       38\n",
      "1527950        0      0 2021-12-10            True 117.300000 11618879        216                        Wartales   2023-04-12 True False  False           Very Positive              90         17609    35.000000        0.000000  0.000000        True               []        87        6\n",
      " 975370        0      0 2022-12-22            True  24.300000 12159611        226                  Dwarf Fortress   2022-12-06 True False  False Overwhelmingly Positive              95         19665    30.000000        0.000000  0.000000        True               []       119        3\n",
      " 244850        0      0 2019-11-21            True   8.200000 12691232        230                 Space Engineers   2019-02-28 True False  False           Very Positive              89         78007    20.000000        0.000000  0.000000        True               []         7        1\n",
      "1172620        0      0 2021-12-30            True 385.700000  1586796        244     Sea of Thieves 2023 Edition   2020-06-03 True False  False           Very Positive              90        253844    40.000000        0.000000  0.000000        True               []        14        1\n",
      " 239030        2      0 2019-01-08            True  34.400000  5755947        268                   Papers Please   2013-08-08 True  True   True Overwhelmingly Positive              97         57117    10.000000        0.000000  0.000000        True               []       209       32\n",
      "\n",
      "=== End of EDA Report ===\n",
      "=== Robust EDA Report: olist ===\n",
      "\n",
      "=== Info ===\n",
      "  rows  columns  memory_bytes\n",
      "113425       38     121929004\n",
      "\n",
      "=== Dtypes ===\n",
      "                       column          dtype\n",
      "                customer_city         object\n",
      "                  customer_id         object\n",
      "               customer_state         object\n",
      "           customer_unique_id         object\n",
      "     customer_zip_code_prefix          int64\n",
      "                freight_value        float64\n",
      "                   geo_points        float64\n",
      "              geolocation_lat        float64\n",
      "              geolocation_lng        float64\n",
      "            order_approved_at         object\n",
      " order_delivered_carrier_date datetime64[ns]\n",
      "order_delivered_customer_date datetime64[ns]\n",
      "order_estimated_delivery_date datetime64[ns]\n",
      "                     order_id         object\n",
      "                order_item_id        float64\n",
      "     order_purchase_timestamp datetime64[ns]\n",
      "                 order_status         object\n",
      "                payment_count        float64\n",
      "     payment_installments_max        float64\n",
      "          payment_value_total        float64\n",
      "                        price        float64\n",
      "        product_category_name         object\n",
      "product_category_name_english         object\n",
      "   product_description_lenght        float64\n",
      "            product_height_cm        float64\n",
      "\n",
      "=== Missing Values ===\n",
      "                       column  missing_count  missing_percent\n",
      "order_delivered_customer_date           3229         2.850000\n",
      "product_category_name_english           2402         2.120000\n",
      "        product_category_name           2378         2.100000\n",
      "          product_name_lenght           2378         2.100000\n",
      "   product_description_lenght           2378         2.100000\n",
      "           product_photos_qty           2378         2.100000\n",
      " order_delivered_carrier_date           1968         1.740000\n",
      "         review_count_product            982         0.870000\n",
      "    review_score_mean_product            982         0.870000\n",
      "             product_weight_g            793         0.700000\n",
      "            product_length_cm            793         0.700000\n",
      "            product_height_cm            793         0.700000\n",
      "             product_width_cm            793         0.700000\n",
      "                order_item_id            775         0.680000\n",
      "                   product_id            775         0.680000\n",
      "                    seller_id            775         0.680000\n",
      "          shipping_limit_date            775         0.680000\n",
      "                        price            775         0.680000\n",
      "                freight_value            775         0.680000\n",
      "       seller_zip_code_prefix            775         0.680000\n",
      "                  seller_city            775         0.680000\n",
      "                 seller_state            775         0.680000\n",
      "              geolocation_lat            306         0.270000\n",
      "              geolocation_lng            306         0.270000\n",
      "                   geo_points            306         0.270000\n",
      "\n",
      "=== Duplicates Summary ===\n",
      " duplicate_rows\n",
      "              0\n",
      "\n",
      "=== Duplicates Preview (up to 20 rows) ===\n",
      "(No duplicate rows found.)\n",
      "\n",
      "=== Numeric Summary (5%..95%) ===\n",
      "                    column         count         mean          std         min          5%          25%          50%          75%          95%          max\n",
      "  customer_zip_code_prefix 113425.000000 35102.472965 29864.919733 1003.000000 3304.400000 11250.000000 24320.000000 59020.000000 90570.000000 99990.000000\n",
      "           geolocation_lat 113119.000000   -21.236147     5.567214  -33.689948  -28.598993   -23.591282   -22.930174   -20.179524    -7.973234    42.184003\n",
      "           geolocation_lng 113119.000000   -46.203868     4.036390  -72.668881  -52.374663   -48.109940   -46.634236   -43.644304   -38.507143    -8.723762\n",
      "                geo_points 113119.000000   153.055057   153.206849    1.000000   17.000000    53.000000   103.000000   196.000000   469.000000  1146.000000\n",
      "             order_item_id 112650.000000     1.197834     0.705124    1.000000    1.000000     1.000000     1.000000     1.000000     2.000000    21.000000\n",
      "                     price 112650.000000   120.653739   183.633928    0.850000   17.000000    39.900000    74.990000   134.900000   349.900000  6735.000000\n",
      "             freight_value 112650.000000    19.990320    15.806405    0.000000    7.780000    13.080000    16.260000    21.150000    45.120000   409.680000\n",
      "       product_name_lenght 111047.000000    48.775978    10.025581    5.000000   29.000000    42.000000    52.000000    57.000000    60.000000    76.000000\n",
      "product_description_lenght 111047.000000   787.867029   652.135608    4.000000  161.000000   348.000000   603.000000   987.000000  2124.000000  3992.000000\n",
      "        product_photos_qty 111047.000000     2.209713     1.721438    1.000000    1.000000     1.000000     1.000000     3.000000     6.000000    20.000000\n",
      "          product_weight_g 112632.000000  2093.672047  3751.596884    0.000000  125.000000   300.000000   700.000000  1800.000000  9750.000000 40425.000000\n",
      "         product_length_cm 112632.000000    30.153669    16.153449    7.000000   16.000000    18.000000    25.000000    38.000000    62.000000   105.000000\n",
      "         product_height_cm 112632.000000    16.593766    13.443483    2.000000    3.000000     8.000000    13.000000    20.000000    45.000000   105.000000\n",
      "          product_width_cm 112632.000000    22.996546    11.707268    6.000000   11.000000    15.000000    20.000000    30.000000    45.000000   118.000000\n",
      "    seller_zip_code_prefix 112650.000000 24439.170431 27596.030909 1001.000000 2969.000000  6429.000000 13568.000000 27930.000000 88330.000000 99730.000000\n",
      "       payment_value_total 113422.000000   180.482857   273.548227    0.000000   33.420500    65.680000   114.400000   195.360000   534.086000 13664.080000\n",
      "  payment_installments_max 113422.000000     3.016611     2.802682    0.000000    1.000000     1.000000     2.000000     4.000000    10.000000    24.000000\n",
      "             payment_count 113422.000000     1.044163     0.386006    1.000000    1.000000     1.000000     1.000000     1.000000     1.000000    29.000000\n",
      "      review_count_product 112443.000000    31.972884    69.483075    1.000000    1.000000     2.000000     6.000000    24.000000   154.000000   458.000000\n",
      " review_score_mean_product 112443.000000     4.057299     0.853133    1.000000    2.333333     3.800000     4.166667     4.600000     5.000000     5.000000\n",
      "\n",
      "=== Skew and Kurtosis ===\n",
      "                    column      skew    kurtosis\n",
      "  customer_zip_code_prefix  0.781055   -0.791527\n",
      "           geolocation_lat  1.642852    3.137063\n",
      "           geolocation_lng  0.006367    2.091179\n",
      "                geo_points  2.305338    7.289282\n",
      "             order_item_id  7.580356  103.857361\n",
      "                     price  7.923208  120.828298\n",
      "             freight_value  5.639870   59.788253\n",
      "       product_name_lenght -0.907144    0.156610\n",
      "product_description_lenght  2.005550    4.901255\n",
      "        product_photos_qty  1.907908    4.834775\n",
      "          product_weight_g  3.598715   16.264762\n",
      "         product_length_cm  1.761463    3.745488\n",
      "         product_height_cm  2.253810    7.384688\n",
      "          product_width_cm  1.726601    4.686966\n",
      "    seller_zip_code_prefix  1.555411    0.932614\n",
      "       payment_value_total 13.757697  481.658747\n",
      "  payment_installments_max  1.568688    2.334876\n",
      "             payment_count 24.662079 1031.648447\n",
      "      review_count_product  3.715817   15.095643\n",
      " review_score_mean_product -1.645040    3.447657\n",
      "\n",
      "=== IQR Outlier Counts ===\n",
      "                    column  outlier_count\n",
      "  customer_zip_code_prefix              0\n",
      "           geolocation_lat          18343\n",
      "           geolocation_lng           4781\n",
      "                geo_points           7551\n",
      "             order_item_id          13984\n",
      "                     price           8427\n",
      "             freight_value          12134\n",
      "       product_name_lenght            772\n",
      "product_description_lenght           7011\n",
      "        product_photos_qty           3073\n",
      "          product_weight_g          15807\n",
      "         product_length_cm           3617\n",
      "         product_height_cm           7670\n",
      "          product_width_cm           2563\n",
      "    seller_zip_code_prefix          17384\n",
      "       payment_value_total           9285\n",
      "  payment_installments_max           8052\n",
      "             payment_count           3309\n",
      "      review_count_product          15185\n",
      " review_score_mean_product           6673\n",
      "\n",
      "=== Correlation (first 30 numeric columns) ===\n",
      "                            customer_zip_code_prefix  geolocation_lat  geolocation_lng  geo_points  order_item_id     price  freight_value  product_name_lenght  product_description_lenght  product_photos_qty  product_weight_g  product_length_cm  product_height_cm  product_width_cm  seller_zip_code_prefix  payment_value_total  payment_installments_max  payment_count  review_count_product  review_score_mean_product\n",
      "column                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
      "customer_zip_code_prefix                    1.000000         0.120178        -0.311110   -0.075147      -0.000874  0.042467       0.225690             0.015515                    0.023709            0.019626          0.003242           0.007491           0.011746         -0.008723                0.076078             0.054596                  0.055849      -0.006180              0.015302                  -0.008042\n",
      "geolocation_lat                             0.120178         1.000000         0.447071   -0.132107      -0.019372  0.056704       0.273041             0.013558                    0.031295            0.022868          0.002883          -0.017386          -0.004904         -0.019519               -0.036425             0.060718                  0.060907       0.004291              0.012204                  -0.019486\n",
      "geolocation_lng                            -0.311110         0.447071         1.000000    0.063343      -0.023267  0.018646       0.093279            -0.002677                    0.005302            0.001915          0.002063          -0.011024          -0.006957         -0.005889               -0.035775             0.015405                  0.035644       0.009003              0.017569                  -0.023477\n",
      "geo_points                                 -0.075147        -0.132107         0.063343    1.000000       0.012231  0.004072      -0.027094            -0.013364                   -0.010607           -0.008938          0.008363           0.012244           0.005519          0.007312                0.024215             0.006758                 -0.001415       0.000141             -0.010202                  -0.001445\n",
      "order_item_id                              -0.000874        -0.019372        -0.023267    0.012231       1.000000 -0.060522      -0.029380            -0.022662                   -0.014311           -0.055852         -0.003549           0.005628           0.029323         -0.010732               -0.018721             0.266002                  0.072769      -0.007368             -0.005996                  -0.100873\n",
      "price                                       0.042467         0.056704         0.018646    0.004072      -0.060522  1.000000       0.414204             0.017001                    0.198166            0.051848          0.338819           0.145811           0.223602          0.172467                0.086416             0.761235                  0.285593       0.002500             -0.060756                  -0.014161\n",
      "freight_value                               0.225690         0.273041         0.093279   -0.027094      -0.029380  0.414204       1.000000             0.023611                    0.093855            0.022259          0.610420           0.309086           0.391831          0.323777                0.150204             0.386453                  0.192770       0.009487             -0.037750                  -0.037331\n",
      "product_name_lenght                         0.015515         0.013558        -0.002677   -0.013364      -0.022662  0.017001       0.023611             1.000000                    0.091524            0.145904          0.022731           0.060447          -0.028170          0.064442               -0.031412             0.002254                  0.020483      -0.002390              0.056451                  -0.013127\n",
      "product_description_lenght                  0.023709         0.031295         0.005302   -0.010607      -0.014311  0.198166       0.093855             0.091524                    1.000000            0.118087          0.059710           0.005971           0.072724         -0.068419                0.059581             0.159276                  0.036357      -0.003803             -0.030893                   0.024539\n",
      "product_photos_qty                          0.019626         0.022868         0.001915   -0.008938      -0.055852  0.051848       0.022259             0.145904                    0.118087            1.000000          0.022667           0.047503          -0.033399          0.010367               -0.045357             0.009791                 -0.000272      -0.002954              0.017089                   0.034204\n",
      "product_weight_g                            0.003242         0.002883         0.002063    0.008363      -0.003549  0.338819       0.610420             0.022731                    0.059710            0.022667          1.000000           0.460919           0.583277          0.506609                0.009709             0.316895                  0.186941       0.014920             -0.023441                  -0.049110\n",
      "product_length_cm                           0.007491        -0.017386        -0.011024    0.012244       0.005628  0.145811       0.309086             0.060447                    0.005971            0.047503          0.460919           1.000000           0.192895          0.532537                0.033426             0.146373                  0.125084       0.017350              0.009367                  -0.032536\n",
      "product_height_cm                           0.011746        -0.004904        -0.006957    0.005519       0.029323  0.223602       0.391831            -0.028170                    0.072724           -0.033399          0.583277           0.192895           1.000000          0.281224                0.012570             0.224190                  0.127021       0.007379             -0.038836                  -0.041108\n",
      "product_width_cm                           -0.008723        -0.019519        -0.005889    0.007312      -0.010732  0.172467       0.323777             0.064442                   -0.068419            0.010367          0.506609           0.532537           0.281224          1.000000               -0.016362             0.156538                  0.145728       0.016667              0.059227                  -0.020779\n",
      "seller_zip_code_prefix                      0.076078        -0.036425        -0.035775    0.024215      -0.018721  0.086416       0.150204            -0.031412                    0.059581           -0.045357          0.009709           0.033426           0.012570         -0.016362                1.000000             0.071455                  0.040971       0.000372             -0.067969                   0.036790\n",
      "payment_value_total                         0.054596         0.060718         0.015405    0.006758       0.266002  0.761235       0.386453             0.002254                    0.159276            0.009791          0.316895           0.146373           0.224190          0.156538                0.071455             1.000000                  0.263164      -0.001580             -0.050235                  -0.082840\n",
      "payment_installments_max                    0.055849         0.060907         0.035644   -0.001415       0.072769  0.285593       0.192770             0.020483                    0.036357           -0.000272          0.186941           0.125084           0.127021          0.145728                0.040971             0.263164                  1.000000      -0.041108             -0.020432                  -0.035805\n",
      "payment_count                              -0.006180         0.004291         0.009003    0.000141      -0.007368  0.002500       0.009487            -0.002390                   -0.003803           -0.002954          0.014920           0.017350           0.007379          0.016667                0.000372            -0.001580                 -0.041108       1.000000             -0.010720                  -0.000936\n",
      "review_count_product                        0.015302         0.012204         0.017569   -0.010202      -0.005996 -0.060756      -0.037750             0.056451                   -0.030893            0.017089         -0.023441           0.009367          -0.038836          0.059227               -0.067969            -0.050235                 -0.020432      -0.010720              1.000000                   0.014737\n",
      "review_score_mean_product                  -0.008042        -0.019486        -0.023477   -0.001445      -0.100873 -0.014161      -0.037331            -0.013127                    0.024539            0.034204         -0.049110          -0.032536          -0.041108         -0.020779                0.036790            -0.082840                 -0.035805      -0.000936              0.014737                   1.000000\n",
      "\n",
      "=== Categorical Values (Top 20 per column) ===\n",
      "     column                            value  count  percent\n",
      "   order_id 8272b63d03f5f79c56e9e4120aec44ef     21 0.020000\n",
      "   order_id ab14fdcfbe524636d65ee38360e22ce8     20 0.020000\n",
      "   order_id 1b15974a0141d54e36626dca3fdc731a     20 0.020000\n",
      "   order_id 9ef13efd6949e4573a18964dd1bbe7f5     15 0.010000\n",
      "   order_id 428a2f660dc84138d969ccd69a0ab6d5     15 0.010000\n",
      "   order_id 9bdc4d4c71aa1de4606060929dee888c     14 0.010000\n",
      "   order_id 73c8ab38f07dc94389065f7eba4f297a     14 0.010000\n",
      "   order_id 37ee401157a3a0b28c9c6d0ed8c3b24b     13 0.010000\n",
      "   order_id 2c2a19b5703863c908512d135aa6accc     12 0.010000\n",
      "   order_id af822dacd6f5cff7376413c03a388bb7     12 0.010000\n",
      "   order_id 637617b3ffe9e2f7a2411243829226d0     12 0.010000\n",
      "   order_id 3a213fcdfe7d98be74ea0dc05a8b31ae     12 0.010000\n",
      "   order_id c05d6a79e55da72ca780ce90364abed9     12 0.010000\n",
      "   order_id 5a3b1c29a49756e75f1ef513383c0c12     11 0.010000\n",
      "   order_id 71dab1155600756af6de79de92e712e3     11 0.010000\n",
      "   order_id 6c355e2913545fa6f72c40cbca57729e     11 0.010000\n",
      "   order_id 7f2c22c54cbae55091a09a9653fd2b8a     11 0.010000\n",
      "   order_id a483ffe0ce133740ab12ebcba8a3ccf9     10 0.010000\n",
      "   order_id ca3625898fbd48669d50701aba51cd5f     10 0.010000\n",
      "   order_id e8fa22c3673b1dd17ea315021b1f0f61     10 0.010000\n",
      "customer_id fc3d1daec319d62d49bfb5e1f83123e9     21 0.020000\n",
      "customer_id bd5d39761aa56689a265d95d8d32b8be     20 0.020000\n",
      "customer_id be1b70680b9f9694d8c70f41fa3dc92b     20 0.020000\n",
      "customer_id adb32467ecc74b53576d9d13a5a55891     15 0.010000\n",
      "customer_id 10de381f8a8d23fff822753305f71cae     15 0.010000\n",
      "\n",
      "=== Unique Counts Per Column ===\n",
      "                       column  unique_values\n",
      "                     order_id          99441\n",
      "                  customer_id          99441\n",
      "                 order_status              8\n",
      "     order_purchase_timestamp          98875\n",
      "            order_approved_at          90734\n",
      " order_delivered_carrier_date          81019\n",
      "order_delivered_customer_date          95665\n",
      "order_estimated_delivery_date            459\n",
      "           customer_unique_id          96096\n",
      "     customer_zip_code_prefix          14994\n",
      "                customer_city           4119\n",
      "               customer_state             27\n",
      "              geolocation_lat          14838\n",
      "              geolocation_lng          14838\n",
      "                   geo_points            484\n",
      "                order_item_id             22\n",
      "                   product_id          32952\n",
      "                    seller_id           3096\n",
      "          shipping_limit_date          93319\n",
      "                        price           5969\n",
      "                freight_value           7000\n",
      "        product_category_name             74\n",
      "          product_name_lenght             67\n",
      "   product_description_lenght           2961\n",
      "           product_photos_qty             20\n",
      "\n",
      "=== Head (10 rows) ===\n",
      "                        order_id                      customer_id order_status order_purchase_timestamp   order_approved_at order_delivered_carrier_date order_delivered_customer_date order_estimated_delivery_date               customer_unique_id  customer_zip_code_prefix           customer_city customer_state  geolocation_lat  geolocation_lng  geo_points  order_item_id                       product_id                        seller_id shipping_limit_date      price  freight_value product_category_name  product_name_lenght  product_description_lenght  product_photos_qty  product_weight_g  product_length_cm  product_height_cm  product_width_cm product_category_name_english  seller_zip_code_prefix           seller_city seller_state  payment_value_total  payment_installments_max  payment_count  review_count_product  review_score_mean_product\n",
      "e481f51cbdc54678b7cc49136f2d6af7 9ef432eb6251297304e76186b10a928d    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15          2017-10-04 19:55:00           2017-10-10 21:25:13                    2017-10-18 7c396fd4830fd04220f754e42b4e5bff                      3149               sao paulo             SP       -23.576983       -46.587161   24.000000       1.000000 87285b34884572647811a353c7ac498a 3504c0cb71d7fa48d967e0e4c94d59d9 2017-10-06 11:07:15  29.990000       8.720000 utilidades_domesticas            40.000000                  268.000000            4.000000        500.000000          19.000000           8.000000         13.000000                    housewares             9350.000000                  maua           SP            38.710000                  1.000000       3.000000              4.000000                   4.000000\n",
      "53cdb2fc8bc7dce0b6741e2150273451 b0830fb4747a6c6d20dea0b8c802d7ef    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27          2018-07-26 14:31:00           2018-08-07 15:27:45                    2018-08-13 af07308b275d755c9edb36a90c618231                     47813               barreiras             BA       -12.177924       -44.660711   19.000000       1.000000 595fac2a385ac33a80bd5114aec74eb8 289cdb325fb7e7f891c38608bf9e0962 2018-07-30 03:24:27 118.700000      22.760000            perfumaria            29.000000                  178.000000            1.000000        400.000000          19.000000          13.000000         19.000000                     perfumery            31570.000000        belo horizonte           SP           141.460000                  1.000000       1.000000            100.000000                   4.440000\n",
      "47770eb9100c2d0c44946d9cf07ec65d 41ce2a54c0b03bf3443c3d931a367089    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23          2018-08-08 13:50:00           2018-08-17 18:06:29                    2018-09-04 3a653a41f6f9fc3d2a113cf8398680e8                     75265              vianopolis             GO       -16.745150       -48.514783   25.000000       1.000000 aa4383b373c6aca5d8797843e5594415 4869f7a5dfa277a7dca6462dcf3b52b2 2018-08-13 08:55:23 159.900000      19.220000            automotivo            46.000000                  232.000000            1.000000        420.000000          24.000000          19.000000         21.000000                          auto            14840.000000               guariba           SP           179.120000                  3.000000       1.000000              3.000000                   5.000000\n",
      "949d5b44dbf5de918fe9c16f97b45f8a f88197465ea7920adcdbec7375364d82    delivered      2017-11-18 19:28:06 2017-11-18 19:45:59          2017-11-22 13:39:59           2017-12-02 00:28:42                    2017-12-15 7c142cf63193a1473d2e66489a9ae977                     59296 sao goncalo do amarante             RN        -5.774190       -35.271143   18.000000       1.000000 d0b61bfb1de832b15ba9d266ca96e5b0 66922902710d126a0e7d26b0e3805106 2017-11-23 19:45:59  45.000000      27.200000              pet_shop            59.000000                  468.000000            3.000000        450.000000          30.000000          10.000000         20.000000                      pet_shop            31842.000000        belo horizonte           MG            72.200000                  1.000000       1.000000              4.000000                   4.250000\n",
      "ad21c59c0840e6cb83a9ceb5573f8159 8ab97904e6daea8866dbdbc4fb7aad2c    delivered      2018-02-13 21:18:39 2018-02-13 22:20:29          2018-02-14 19:46:34           2018-02-16 18:17:02                    2018-02-26 72632f0f9dd73dfee390c9b22eb56dd6                      9195             santo andre             SP       -23.676370       -46.514627  222.000000       1.000000 65266b2da20d04dbe00c5c2d3bb7859e 2c9e548be18521d1c43cde1c582c6de8 2018-02-19 20:31:37  19.900000       8.720000             papelaria            38.000000                  316.000000            4.000000        250.000000          51.000000          15.000000         15.000000                    stationery             8752.000000       mogi das cruzes           SP            28.620000                  1.000000       1.000000             31.000000                   4.258065\n",
      "a4591c265e18cb1dcee52889e2d8acc3 503740e9ca751ccdda7ba28e9ab8f608    delivered      2017-07-09 21:57:05 2017-07-09 22:10:13          2017-07-11 14:58:04           2017-07-26 10:57:55                    2017-08-01 80bb27c7c16e8f973207a5086ab329e2                     86320            congonhinhas             PR       -23.553522       -50.549924   33.000000       1.000000 060cb19345d90064d1015407193c233d 8581055ce74af1daba164fdbd55a40de 2017-07-13 22:10:13 147.900000      27.360000            automotivo            49.000000                  608.000000            1.000000       7150.000000          65.000000          10.000000         65.000000                          auto             7112.000000             guarulhos           SP           175.260000                  6.000000       1.000000             25.000000                   4.240000\n",
      "136cce7faa42fdb2cefd53fdc79a6098 ed0271e0b7da060a393796590e7b737a     invoiced      2017-04-11 12:22:08 2017-04-13 13:25:17                          NaT                           NaT                    2017-05-09 36edbb3fb164b1f16485364b6fb04c73                     98900              santa rosa             RS       -27.865358       -54.470128  182.000000       1.000000 a1804276d9941ac0733cfd409f5206eb dc8798cbf453b7e0f98745e396cc5616 2017-04-19 13:25:17  49.900000      16.050000                   NaN                  NaN                         NaN                 NaN        600.000000          35.000000          35.000000         15.000000                           NaN             5455.000000             sao paulo           SP            65.950000                  1.000000       1.000000              2.000000                   1.500000\n",
      "6514b8ad8028c9f2cc2374ded245783f 9bdf08b4b3b52b5526ff42d37d47f222    delivered      2017-05-16 13:10:30 2017-05-16 13:22:11          2017-05-22 10:07:46           2017-05-26 12:55:51                    2017-06-07 932afa1e708222e5821dac9cd5db4cae                     26525               nilopolis             RJ       -22.805707       -43.423079  163.000000       1.000000 4520766ec412348b8d4caa5e8a18c464 16090f2ca825584b5a147ab24aa30c86 2017-05-22 13:22:11  59.990000      15.170000            automotivo            59.000000                  956.000000            1.000000         50.000000          16.000000          16.000000         17.000000                          auto            12940.000000               atibaia           SP            75.160000                  3.000000       1.000000             52.000000                   4.134615\n",
      "76c6e866289321a7c93b82b54852dc33 f54a9f0e6b351c431402b8461ea51999    delivered      2017-01-23 18:29:09 2017-01-25 02:50:47          2017-01-26 14:16:31           2017-02-02 14:08:10                    2017-03-06 39382392765b6dc74812866ee5ee92a7                     99655            faxinalzinho             RS       -27.421769       -52.675022    3.000000       1.000000 ac1789e492dcd698c5c10b97a671243a 63b9ae557efed31d1f7687917d248a8d 2017-01-27 18:29:09  19.900000      16.050000      moveis_decoracao            41.000000                  432.000000            2.000000        300.000000          35.000000          35.000000         15.000000               furniture_decor            13720.000000 sao jose do rio pardo           SP            35.950000                  1.000000       1.000000              2.000000                   3.000000\n",
      "e69bfb5eb88e0ed6a785585b27e16dbf 31ad1d1b63eb9962463f764d4e6e0c9d    delivered      2017-07-29 11:55:02 2017-07-29 12:05:32          2017-08-10 19:45:24           2017-08-16 17:14:30                    2017-08-23 299905e3934e9e181bfb2e164dd4b4f8                     18075                sorocaba             SP       -23.474030       -47.467397  153.000000       1.000000 9a78fb9862b10749a117f7fc3c31f051 7c67e1448b00f6e969d365cea6b010ab 2017-08-11 12:05:32 149.990000      19.770000     moveis_escritorio            45.000000                  527.000000            1.000000       9750.000000          42.000000          41.000000         42.000000              office_furniture             8577.000000       itaquaquecetuba           SP           169.760000                  1.000000       2.000000              8.000000                   3.375000\n",
      "\n",
      "=== End of EDA Report ===\n",
      "=== Robust EDA Report: sales ===\n",
      "\n",
      "=== Info ===\n",
      " rows  columns  memory_bytes\n",
      "55792       16      23481940\n",
      "\n",
      "=== Dtypes ===\n",
      "       column   dtype\n",
      " Critic_Score float64\n",
      "    Developer  object\n",
      "  ESRB_Rating  object\n",
      "        Genre  object\n",
      " Global_Sales float64\n",
      "     JP_Sales float64\n",
      "     NA_Sales float64\n",
      "         Name  object\n",
      "  Other_Sales float64\n",
      "    PAL_Sales float64\n",
      "     Platform  object\n",
      "    Publisher  object\n",
      "         Rank   int64\n",
      "Total_Shipped float64\n",
      "   User_Score float64\n",
      "         Year float64\n",
      "\n",
      "=== Missing Values ===\n",
      "       column  missing_count  missing_percent\n",
      "   User_Score          55457        99.400000\n",
      "Total_Shipped          53965        96.730000\n",
      " Critic_Score          49256        88.290000\n",
      "     JP_Sales          48749        87.380000\n",
      "     NA_Sales          42828        76.760000\n",
      "    PAL_Sales          42603        76.360000\n",
      "  Other_Sales          40270        72.180000\n",
      " Global_Sales          36377        65.200000\n",
      "  ESRB_Rating          32169        57.660000\n",
      "         Year            979         1.750000\n",
      "    Developer             17         0.030000\n",
      "         Rank              0         0.000000\n",
      "         Name              0         0.000000\n",
      "        Genre              0         0.000000\n",
      "     Platform              0         0.000000\n",
      "    Publisher              0         0.000000\n",
      "\n",
      "=== Duplicates Summary ===\n",
      " duplicate_rows\n",
      "              0\n",
      "\n",
      "=== Duplicates Preview (up to 20 rows) ===\n",
      "(No duplicate rows found.)\n",
      "\n",
      "=== Numeric Summary (5%..95%) ===\n",
      "       column        count         mean          std         min          5%          25%          50%          75%          95%          max\n",
      "         Rank 55792.000000 27896.500000 16105.907446    1.000000 2790.550000 13948.750000 27896.500000 41844.250000 53002.450000 55792.000000\n",
      " Critic_Score  6536.000000     7.213709     1.454079    1.000000    4.400000     6.400000     7.500000     8.300000     9.100000    10.000000\n",
      "   User_Score   335.000000     8.253433     1.401489    2.000000    5.640000     7.800000     8.500000     9.100000     9.860000    10.000000\n",
      "Total_Shipped  1827.000000     1.887258     4.195693    0.030000    0.110000     0.200000     0.590000     1.800000     7.445000    82.860000\n",
      " Global_Sales 19415.000000     0.365503     0.833022    0.000000    0.000000     0.030000     0.120000     0.360000     1.460000    20.320000\n",
      "     NA_Sales 12964.000000     0.275541     0.512809    0.000000    0.010000     0.050000     0.120000     0.290000     1.050000     9.760000\n",
      "    PAL_Sales 13189.000000     0.155263     0.399257    0.000000    0.000000     0.010000     0.040000     0.140000     0.630000     9.850000\n",
      "     JP_Sales  7043.000000     0.110402     0.184673    0.000000    0.000000     0.020000     0.050000     0.120000     0.420000     2.690000\n",
      "  Other_Sales 15522.000000     0.044719     0.129554    0.000000    0.000000     0.000000     0.010000     0.040000     0.190000     3.120000\n",
      "         Year 54813.000000  2005.659095     8.355585 1970.000000 1991.000000  2000.000000  2008.000000  2011.000000  2017.000000  2020.000000\n",
      "\n",
      "=== Skew and Kurtosis ===\n",
      "       column      skew   kurtosis\n",
      "         Rank  0.000000  -1.200000\n",
      " Critic_Score -0.903200   0.820715\n",
      "   User_Score -1.659555   3.675138\n",
      "Total_Shipped  7.541958  96.283206\n",
      " Global_Sales  8.201137 110.282870\n",
      "     NA_Sales  6.469175  68.747988\n",
      "    PAL_Sales  9.079713 134.422202\n",
      "     JP_Sales  4.619332  32.861846\n",
      "  Other_Sales  9.356241 132.367023\n",
      "         Year -0.735907   0.297730\n",
      "\n",
      "=== IQR Outlier Counts ===\n",
      "       column  outlier_count\n",
      "         Rank              0\n",
      " Critic_Score            145\n",
      "   User_Score             19\n",
      "Total_Shipped            186\n",
      " Global_Sales           1986\n",
      "     NA_Sales           1273\n",
      "    PAL_Sales           1517\n",
      "     JP_Sales            734\n",
      "  Other_Sales           1482\n",
      "         Year            757\n",
      "\n",
      "=== Correlation (first 30 numeric columns) ===\n",
      "                   Rank  Critic_Score  User_Score  Total_Shipped  Global_Sales  NA_Sales  PAL_Sales  JP_Sales  Other_Sales      Year\n",
      "column                                                                                                                              \n",
      "Rank           1.000000     -0.137650   -0.293034      -0.441132     -0.554659 -0.550922  -0.438841 -0.443212    -0.427737 -0.097345\n",
      "Critic_Score  -0.137650      1.000000    0.582673       0.203425      0.295941  0.314285   0.253431  0.174933     0.254755  0.015670\n",
      "User_Score    -0.293034      0.582673    1.000000      -0.025732      0.241650  0.234039   0.190490  0.108721     0.224679 -0.116728\n",
      "Total_Shipped -0.441132      0.203425   -0.025732       1.000000           NaN       NaN        NaN       NaN          NaN -0.169701\n",
      "Global_Sales  -0.554659      0.295941    0.241650            NaN      1.000000  0.914964   0.904582  0.228782     0.856798 -0.041354\n",
      "NA_Sales      -0.550922      0.314285    0.234039            NaN      0.914964  1.000000   0.683959  0.075239     0.687831 -0.059352\n",
      "PAL_Sales     -0.438841      0.253431    0.190490            NaN      0.904582  0.683959   1.000000  0.123954     0.814068  0.082548\n",
      "JP_Sales      -0.443212      0.174933    0.108721            NaN      0.228782  0.075239   0.123954  1.000000     0.082254 -0.351626\n",
      "Other_Sales   -0.427737      0.254755    0.224679            NaN      0.856798  0.687831   0.814068  0.082254     1.000000  0.089282\n",
      "Year          -0.097345      0.015670   -0.116728      -0.169701     -0.041354 -0.059352   0.082548 -0.351626     0.089282  1.000000\n",
      "\n",
      "=== Categorical Values (Top 20 per column) ===\n",
      "column                               value  count   percent\n",
      "  Name                  Plants vs. Zombies     20  0.040000\n",
      "  Name                            Monopoly     15  0.030000\n",
      "  Name                       Double Dragon     14  0.030000\n",
      "  Name                     Samurai Shodown     13  0.020000\n",
      "  Name Pier Solar and the Great Architects     12  0.020000\n",
      "  Name                            Lemmings     12  0.020000\n",
      "  Name                    Wheel of Fortune     12  0.020000\n",
      "  Name                               Elite     12  0.020000\n",
      "  Name                           Jeopardy!     12  0.020000\n",
      "  Name                      Space Invaders     12  0.020000\n",
      "  Name                 The Incredible Hulk     11  0.020000\n",
      "  Name                     Resident Evil 4     11  0.020000\n",
      "  Name      LEGO Batman 2: DC Super Heroes     11  0.020000\n",
      "  Name                         Angry Birds     11  0.020000\n",
      "  Name                         Final Fight     11  0.020000\n",
      "  Name              Angry Birds: Star Wars     11  0.020000\n",
      "  Name                        After Burner     11  0.020000\n",
      "  Name                  Shin Megami Tensei     11  0.020000\n",
      "  Name                             FIFA 14     11  0.020000\n",
      "  Name                      Micro Machines     11  0.020000\n",
      " Genre                                Misc   9476 16.980000\n",
      " Genre                              Action   7667 13.740000\n",
      " Genre                           Adventure   5293  9.490000\n",
      " Genre                              Sports   5244  9.400000\n",
      " Genre                             Shooter   4586  8.220000\n",
      "\n",
      "=== Unique Counts Per Column ===\n",
      "       column  unique_values\n",
      "         Rank          55792\n",
      "         Name          37102\n",
      "        Genre             20\n",
      "  ESRB_Rating              9\n",
      "     Platform             74\n",
      "    Publisher           3069\n",
      "    Developer           8065\n",
      " Critic_Score             90\n",
      "   User_Score             51\n",
      "Total_Shipped            447\n",
      " Global_Sales            511\n",
      "     NA_Sales            337\n",
      "    PAL_Sales            268\n",
      "     JP_Sales            132\n",
      "  Other_Sales            139\n",
      "         Year             48\n",
      "\n",
      "=== Head (10 rows) ===\n",
      " Rank                               Name        Genre ESRB_Rating Platform        Publisher             Developer  Critic_Score  User_Score  Total_Shipped  Global_Sales  NA_Sales  PAL_Sales  JP_Sales  Other_Sales        Year\n",
      "    1                         Wii Sports       Sports           E      Wii         Nintendo          Nintendo EAD      7.700000         NaN      82.860000           NaN       NaN        NaN       NaN          NaN 2006.000000\n",
      "    2                  Super Mario Bros.     Platform         NaN      NES         Nintendo          Nintendo EAD     10.000000         NaN      40.240000           NaN       NaN        NaN       NaN          NaN 1985.000000\n",
      "    3                     Mario Kart Wii       Racing           E      Wii         Nintendo          Nintendo EAD      8.200000    9.100000      37.140000           NaN       NaN        NaN       NaN          NaN 2008.000000\n",
      "    4      PlayerUnknown's Battlegrounds      Shooter         NaN       PC PUBG Corporation      PUBG Corporation           NaN         NaN      36.600000           NaN       NaN        NaN       NaN          NaN 2017.000000\n",
      "    5                  Wii Sports Resort       Sports           E      Wii         Nintendo          Nintendo EAD      8.000000    8.800000      33.090000           NaN       NaN        NaN       NaN          NaN 2009.000000\n",
      "    6 Pokemon Red / Green / Blue Version Role-Playing           E       GB         Nintendo            Game Freak      9.400000         NaN      31.380000           NaN       NaN        NaN       NaN          NaN 1998.000000\n",
      "    7              New Super Mario Bros.     Platform           E       DS         Nintendo          Nintendo EAD      9.100000    8.100000      30.800000           NaN       NaN        NaN       NaN          NaN 2006.000000\n",
      "    8                             Tetris       Puzzle           E       GB         Nintendo Bullet Proof Software           NaN         NaN      30.260000           NaN       NaN        NaN       NaN          NaN 1989.000000\n",
      "    9          New Super Mario Bros. Wii     Platform           E      Wii         Nintendo          Nintendo EAD      8.600000    9.200000      30.220000           NaN       NaN        NaN       NaN          NaN 2009.000000\n",
      "   10                          Minecraft         Misc         NaN       PC           Mojang             Mojang AB     10.000000         NaN      30.010000           NaN       NaN        NaN       NaN          NaN 2010.000000\n",
      "\n",
      "=== End of EDA Report ===\n"
     ]
    }
   ],
   "source": [
    "robust_eda(steam, name=\"steam\")\n",
    "robust_eda(olist, name=\"olist\")\n",
    "robust_eda(sales, name=\"sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3177e9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e30164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] steam split: 0.33 s\n",
      "[info] steam post split | X_train shape=(800000, 22) mem=0.438 GB | X_test shape=(200001, 22) mem=0.109 GB\n",
      "[timer] steam datetime to numeric: 0.20 s\n",
      "[timer] steam text features: 4.03 s\n",
      "[info] steam after text | X_train shape=(800000, 22) mem=0.438 GB | X_test shape=(200001, 43) mem=0.119 GB\n",
      "[info] steam tags unique=441, kept=100 (min_count=5, top_k=100)\n",
      "[timer] steam tags multi-hot: 11.02 s\n",
      "[info] steam after tags | X_train shape=(800000, 121) mem=0.399 GB | X_test shape=(200001, 142) mem=0.109 GB\n",
      "[info] OHE auto-excluded (steam): ['description', 'title']\n",
      "[timer] steam OHE: 0.33 s\n",
      "[info] steam after OHE | X_train shape=(800000, 119) mem=0.194 GB | X_test shape=(200001, 119) mem=0.048 GB\n",
      "steam non-numeric after OHE: []\n",
      "[timer] steam impute: 3.96 s\n",
      "[info] steam after impute+downcast | X_train shape=(800000, 119) mem=0.361 GB | X_test shape=(200001, 119) mem=0.090 GB\n",
      "[timer] steam poly: 0.34 s\n",
      "[info] steam after poly | X_train shape=(800000, 126) mem=0.381 GB | X_test shape=(200001, 126) mem=0.095 GB\n",
      "[timer] steam outlier filter: 0.58 s\n",
      "[info] steam after outlier | X_train shape=(522321, 126) mem=0.249 GB | X_test shape=(130675, 126) mem=0.062 GB\n",
      "[timer] steam scale: 0.47 s\n",
      "[timer] steam select features: 123.76 s\n",
      "[info] steam after select | X_train shape=(522321, 50) mem=0.101 GB | X_test shape=(130675, 50) mem=0.025 GB\n",
      "[timer] olist split: 0.07 s\n",
      "[info] olist post split | X_train shape=(90740, 37) mem=0.072 GB | X_test shape=(22685, 37) mem=0.018 GB\n",
      "[timer] olist datetime to numeric: 0.07 s\n",
      "[timer] olist text features: 0.22 s\n",
      "[info] OHE auto-excluded (olist): ['customer_city', 'order_approved_at', 'product_id', 'seller_city', 'seller_id']\n",
      "[timer] olist OHE: 0.24 s\n",
      "[info] olist after OHE | X_train shape=(90740, 186) mem=0.032 GB | X_test shape=(22685, 186) mem=0.008 GB\n",
      "[timer] olist impute: 0.15 s\n",
      "[info] olist after impute+downcast | X_train shape=(90740, 27) mem=0.010 GB | X_test shape=(22685, 27) mem=0.002 GB\n",
      "[timer] olist poly: 0.01 s\n",
      "[info] olist after poly | X_train shape=(90740, 35) mem=0.013 GB | X_test shape=(22685, 35) mem=0.003 GB\n",
      "[timer] olist outlier filter: 0.08 s\n",
      "[info] olist after outlier | X_train shape=(42838, 35) mem=0.006 GB | X_test shape=(10740, 35) mem=0.001 GB\n",
      "[timer] olist scale: 0.01 s\n",
      "[timer] olist select features: 15.40 s\n",
      "[info] olist after select | X_train shape=(42838, 35) mem=0.006 GB | X_test shape=(10740, 35) mem=0.001 GB\n",
      "[timer] sales split: 0.02 s\n",
      "[info] sales post split | X_train shape=(44633, 15) mem=0.018 GB | X_test shape=(11159, 15) mem=0.005 GB\n",
      "[timer] sales datetime to numeric: 0.02 s\n",
      "[timer] sales text features: 0.23 s\n",
      "[info] OHE auto-excluded (sales): ['Developer', 'Name', 'Publisher']\n",
      "[timer] sales OHE: 0.07 s\n",
      "[info] sales after OHE | X_train shape=(44633, 89) mem=0.007 GB | X_test shape=(11159, 89) mem=0.002 GB\n",
      "[timer] sales impute: 0.02 s\n",
      "[info] sales after impute+downcast | X_train shape=(44633, 9) mem=0.002 GB | X_test shape=(11159, 9) mem=0.000 GB\n",
      "[timer] sales poly: 0.00 s\n",
      "[info] sales after poly | X_train shape=(44633, 14) mem=0.003 GB | X_test shape=(11159, 14) mem=0.001 GB\n",
      "[timer] sales outlier filter: 0.02 s\n",
      "[info] sales after outlier | X_train shape=(35892, 14) mem=0.002 GB | X_test shape=(8937, 14) mem=0.001 GB\n",
      "[timer] sales scale: 0.00 s\n",
      "[timer] sales select features: 2.24 s\n",
      "[info] sales after select | X_train shape=(35892, 14) mem=0.002 GB | X_test shape=(8937, 14) mem=0.001 GB\n",
      "[steam] X_train: (522321, 50) | X_test: (130675, 50) | y_train: (522321,) | y_test: (130675,)\n",
      "[olist] X_train: (42838, 35) | X_test: (10740, 35) | y_train: (42838,) | y_test: (10740,)\n",
      "[sales] X_train: (35892, 14) | X_test: (8937, 14) | y_train: (35892,) | y_test: (8937,)\n",
      "\n",
      "=== STEAM Dataset ===\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=5 | F1_macro=0.425628 ± 0.002794\n",
      "[2/40] GBT | k=13 | F1_macro=0.425628 ± 0.002794\n",
      "[3/40] GBT | k=25 | F1_macro=0.425628 ± 0.002794\n",
      "[4/40] GBT | k=38 | F1_macro=0.425628 ± 0.002794\n",
      "[5/40] GBT | k=50 | F1_macro=0.425628 ± 0.002794\n",
      "[6/40] RandomForest | k=5 | F1_macro=0.468882 ± 0.000441\n",
      "[7/40] RandomForest | k=13 | F1_macro=0.468882 ± 0.000441\n",
      "[8/40] RandomForest | k=25 | F1_macro=0.468882 ± 0.000441\n",
      "[9/40] RandomForest | k=38 | F1_macro=0.468882 ± 0.000441\n",
      "[10/40] RandomForest | k=50 | F1_macro=0.468882 ± 0.000441\n",
      "[11/40] DecisionTree | k=5 | F1_macro=0.499772 ± 0.000870\n",
      "[12/40] DecisionTree | k=13 | F1_macro=0.499772 ± 0.000870\n",
      "[13/40] DecisionTree | k=25 | F1_macro=0.499772 ± 0.000870\n",
      "[14/40] DecisionTree | k=38 | F1_macro=0.499772 ± 0.000870\n",
      "[15/40] DecisionTree | k=50 | F1_macro=0.499772 ± 0.000870\n",
      "[16/40] LogisticRegression | k=5 | F1_macro=0.378253 ± 0.023563\n",
      "[17/40] LogisticRegression | k=13 | F1_macro=0.380573 ± 0.042214\n",
      "[18/40] LogisticRegression | k=25 | F1_macro=0.413930 ± 0.019622\n",
      "[19/40] LogisticRegression | k=38 | F1_macro=0.419943 ± 0.015349\n",
      "[20/40] LogisticRegression | k=50 | F1_macro=0.425292 ± 0.015598\n",
      "[21/40] LinearSVM | k=5 | F1_macro=0.378254 ± 0.023563\n",
      "[22/40] LinearSVM | k=13 | F1_macro=0.380569 ± 0.042216\n",
      "[23/40] LinearSVM | k=25 | F1_macro=0.413930 ± 0.019641\n",
      "[24/40] LinearSVM | k=38 | F1_macro=0.419931 ± 0.015354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\u001b[33m\"\u001b[39m\u001b[33msales\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== STEAM Dataset ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m best_steam_model = \u001b[43mbuild_and_tune_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_steam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_steam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclassification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moversample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m score_steam = evaluate_on_holdout(best_steam_model, X_test_steam, y_test_steam, task_type=\u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msteam threshold:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(best_steam_model, \u001b[33m\"\u001b[39m\u001b[33mbest_threshold_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 250\u001b[39m, in \u001b[36mbuild_and_tune_models\u001b[39m\u001b[34m(X_train, y_train, task_type, num_folds, num_iterations, oversample, oversample_method)\u001b[39m\n\u001b[32m    248\u001b[39m k_print = total_features \u001b[38;5;28;01mif\u001b[39;00m k_val == \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(k_val)\n\u001b[32m    249\u001b[39m pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m mean_score = \u001b[38;5;28mfloat\u001b[39m(np.mean(scores))\n\u001b[32m    252\u001b[39m std_score = \u001b[38;5;28mfloat\u001b[39m(np.std(scores))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:684\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    682\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:411\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:866\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    864\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    870\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/imblearn/pipeline.py:526\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    521\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    522\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    523\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    524\u001b[39m             all_params=params,\n\u001b[32m    525\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/svm/_classes.py:321\u001b[39m, in \u001b[36mLinearSVC.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n\u001b[32m    317\u001b[39m _dual = _validate_dual_parameter(\n\u001b[32m    318\u001b[39m     \u001b[38;5;28mself\u001b[39m.dual, \u001b[38;5;28mself\u001b[39m.loss, \u001b[38;5;28mself\u001b[39m.penalty, \u001b[38;5;28mself\u001b[39m.multi_class, X\n\u001b[32m    319\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[38;5;28mself\u001b[39m.coef_, \u001b[38;5;28mself\u001b[39m.intercept_, n_iter_ = \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_dual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = n_iter_.max().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/svm/_base.py:1184\u001b[39m, in \u001b[36m_fit_liblinear\u001b[39m\u001b[34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[39m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mepsilon_insensitive\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msquared_epsilon_insensitive\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1183\u001b[39m     enc = LabelEncoder()\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     y_ind = \u001b[43menc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m     classes_ = enc.classes_\n\u001b[32m   1186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes_) < \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:111\u001b[39m, in \u001b[36mLabelEncoder.fit_transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m \u001b[33;03m    Encoded labels.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m y = column_or_1d(y, warn=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_, y = \u001b[43m_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/_encode.py:56\u001b[39m, in \u001b[36m_unique\u001b[39m\u001b[34m(values, return_inverse, return_counts)\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_python(\n\u001b[32m     53\u001b[39m         values, return_inverse=return_inverse, return_counts=return_counts\n\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# numerical\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unique_np\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_counts\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/_encode.py:71\u001b[39m, in \u001b[36m_unique_np\u001b[39m\u001b[34m(values, return_inverse, return_counts)\u001b[39m\n\u001b[32m     69\u001b[39m     uniques, _, inverse, counts = xp.unique_all(values)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_inverse:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     uniques, inverse = \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_counts:\n\u001b[32m     73\u001b[39m     uniques, counts = xp.unique_counts(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/_array_api.py:410\u001b[39m, in \u001b[36m_NumPyAPIWrapper.unique_inverse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique_inverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/numpy/lib/_arraysetops_impl.py:294\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan, sorted)\u001b[39m\n\u001b[32m    292\u001b[39m ar = np.asanyarray(ar)\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/numpy/lib/_arraysetops_impl.py:379\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan, inverse_shape, axis, sorted)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# If we don't use the hash map, we use the slower sorting method.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_indices:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     perm = \u001b[43mar\u001b[49m\u001b[43m.\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmergesort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquicksort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     aux = ar[perm]\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Classification call\n",
    "splits = prepare_data(\n",
    "    steam, olist, sales,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    feature_selection=\"tree\",  # try none first\n",
    "    max_features=50,\n",
    "    task_type=\"classification\",\n",
    "    scale_method=\"standard\",\n",
    "    tag_min_count=5, tag_top_k=100,\n",
    "    verbose=True,\n",
    "    ohe_top_k_per_col=50,        # cap per column\n",
    "    ohe_min_freq_per_col=5,\n",
    "    ohe_auto_exclude=True,       # auto skip long/free-text\n",
    "    ohe_high_card_threshold=500, # tune if needed\n",
    "    ohe_long_text_avglen=25\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "print(\"\\n=== STEAM Dataset ===\")\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ")\n",
    "\n",
    "score_steam = evaluate_on_holdout(best_steam_model, X_test_steam, y_test_steam, task_type=\"classification\")\n",
    "\n",
    "print(\"steam threshold:\", getattr(best_steam_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== OLIST Dataset ===\")\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ")\n",
    "\n",
    "score_olist = evaluate_on_holdout(best_olist_model, X_test_olist, y_test_olist, task_type=\"classification\")\n",
    "\n",
    "print(\"olist threshold:\", getattr(best_olist_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== SALES Dataset ===\")\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ") \n",
    "\n",
    "score_sales = evaluate_on_holdout(best_sales_model, X_test_sales, y_test_sales, task_type=\"classification\")\n",
    "\n",
    "print(\"sales threshold:\", getattr(best_sales_model, \"best_threshold_\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec77e4c",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc10f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Regression call\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m splits = \u001b[43mprepare_all\u001b[49m(\n\u001b[32m      3\u001b[39m     steam, olist, sales,\n\u001b[32m      4\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mregression\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     test_size=\u001b[32m0.2\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\u001b[33m\"\u001b[39m\u001b[33msteam\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\u001b[33m\"\u001b[39m\u001b[33molist\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'prepare_all' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regression call\n",
    "splits = prepare_data(\n",
    "    steam, olist, sales,\n",
    "    task_type=\"regression\",\n",
    "    random_state=42,\n",
    "    feature_selection=\"none\",  # try none first\n",
    "    max_features=None,\n",
    "    scale_method=\"standard\",\n",
    "    tag_min_count=5, tag_top_k=200,\n",
    "    verbose=True,\n",
    "    ohe_top_k_per_col=50,        # cap per column\n",
    "    ohe_min_freq_per_col=5,\n",
    "    ohe_auto_exclude=True,       # auto skip long/free-text\n",
    "    ohe_high_card_threshold=500, # tune if needed\n",
    "    ohe_long_text_avglen=25,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
