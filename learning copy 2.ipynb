{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e204c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTS AND GLOBALS ####\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import (\n",
    "    f_regression,      # for regression\n",
    "    f_classif,         # for classification\n",
    "    SelectKBest,\n",
    "    mutual_info_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Global Settings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random_state = RANDOM_STATE\n",
    "N_ROWS = 10_000\n",
    "\n",
    "# No scientific notation in pandas display\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "# Reproducibility for NumPy-based randomness\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892baa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Small Helpers (updated)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def robust_eda(df):\n",
    "    \"\"\"\n",
    "    Performs robust Exploratory Data Analysis (EDA) on a pandas DataFrame and prints the results.\n",
    "    \n",
    "    This function includes:\n",
    "    - DataFrame shape and data types\n",
    "    - Missing values and duplicates (with handling for unhashable types)\n",
    "    - Head of the DataFrame\n",
    "    - Numerical summaries with robust percentiles (5%, 25%, 50%, 75%, 95%)\n",
    "    - Skewness and kurtosis for numerical columns\n",
    "    - Outlier counts using IQR method\n",
    "    - Correlation matrix for numerical columns\n",
    "    - Value counts and unique counts for categorical columns (with handling for unhashable types)\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to analyze.\n",
    "    \"\"\"\n",
    "    print(\"=== Robust EDA Report ===\")\n",
    "    \n",
    "    # Basic Info\n",
    "    print(\"\\nDataFrame Shape:\", df.shape)\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "    print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "    \n",
    "    # Duplicate Rows with try-except for unhashable types\n",
    "    try:\n",
    "        print(\"\\nDuplicate Rows:\", df.duplicated().sum())\n",
    "    except TypeError:\n",
    "        print(\"\\nDuplicate Rows: Cannot compute due to unhashable types in DataFrame (e.g., lists in columns).\")\n",
    "    \n",
    "    print(\"\\nHead of DataFrame:\\n\", df.head())\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    \n",
    "    # Numerical Analysis\n",
    "    if num_cols:\n",
    "        print(\"\\n=== Numerical Columns Analysis ===\")\n",
    "        # Robust summary with additional percentiles for outlier insight\n",
    "        print(\"\\nSummary Statistics (with 5%, 25%, 50%, 75%, 95% percentiles):\\n\", \n",
    "              df[num_cols].describe(percentiles=[.05, .25, .5, .75, .95]))\n",
    "        \n",
    "        # Skewness and Kurtosis\n",
    "        print(\"\\nSkewness:\\n\", df[num_cols].skew())\n",
    "        print(\"\\nKurtosis:\\n\", df[num_cols].kurtosis())\n",
    "        \n",
    "        # Outlier Detection using IQR (robust to outliers)\n",
    "        Q1 = df[num_cols].quantile(0.25)\n",
    "        Q3 = df[num_cols].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((df[num_cols] < (Q1 - 1.5 * IQR)) | (df[num_cols] > (Q3 + 1.5 * IQR))).sum()\n",
    "        print(\"\\nOutlier Counts (IQR method):\\n\", outliers)\n",
    "        \n",
    "        # Correlation Matrix (using Pearson; consider Spearman for non-normal data if needed)\n",
    "        if len(num_cols) > 1:\n",
    "            print(\"\\nCorrelation Matrix:\\n\", df[num_cols].corr())\n",
    "    \n",
    "    # Categorical Analysis\n",
    "    if cat_cols:\n",
    "        print(\"\\n=== Categorical Columns Analysis ===\")\n",
    "        for col in cat_cols:\n",
    "            try:\n",
    "                print(f\"\\nValue Counts for '{col}':\\n\", df[col].value_counts())\n",
    "                print(f\"Unique Values in '{col}': {df[col].nunique()}\")\n",
    "                print(f\"Most Common Value in '{col}': {df[col].mode()[0] if not df[col].mode().empty else 'N/A'}\")\n",
    "            except TypeError:\n",
    "                print(f\"\\nCannot analyze '{col}' due to unhashable types (e.g., lists).\")\n",
    "    \n",
    "    print(\"\\n=== End of EDA Report ===\")\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "    return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bd5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb965d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Helpers\n",
    "# =============================\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KeepTrainColumns(BaseEstimator, TransformerMixin):\n",
    "    # remembers training columns and reindexes any input to match\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.keep_columns_ = list(X.columns)\n",
    "        else:\n",
    "            self.keep_columns_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.keep_columns_ is None:\n",
    "            return X\n",
    "        if hasattr(X, \"reindex\"):\n",
    "            return X.reindex(columns=self.keep_columns_, fill_value=0)\n",
    "        return X\n",
    "\n",
    "\n",
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    # turn scores into 0/1 using a chosen threshold\n",
    "    import numpy as np\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        raw = model.decision_function(X)\n",
    "        raw_min, raw_max = float(raw.min()), float(raw.max())\n",
    "        scores = (raw - raw_min) / (raw_max - raw_min + 1e-9)\n",
    "    else:\n",
    "        scores = model.predict(X).astype(float)\n",
    "    return (scores >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model builder + tuner (with oversampling + threshold tuning)\n",
    "# =============================\n",
    "def build_and_tune_models(\n",
    "    X_train, y_train,\n",
    "    task_type,\n",
    "    num_folds,\n",
    "    num_iterations,\n",
    "    oversample=False,\n",
    "    oversample_method=\"random\"\n",
    "):\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV, cross_val_predict\n",
    "    from sklearn.pipeline import Pipeline as SKPipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, f_classif, VarianceThreshold\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "    # optional oversampling tools\n",
    "    ImbPipeline = None\n",
    "    RandomOverSampler = None\n",
    "    SMOTE = None\n",
    "    if oversample and str(task_type).strip().lower() == \"classification\":\n",
    "        try:\n",
    "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "        except Exception:\n",
    "            print(\"imblearn not available. Oversampling disabled.\")\n",
    "            oversample = False\n",
    "\n",
    "    # neat prints\n",
    "    np.set_printoptions(suppress=True)\n",
    "    pd.options.display.float_format = lambda x: f\"{x:.6f}\"\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    # drop constant columns once on train\n",
    "    non_constant_columns = X_train.columns[X_train.nunique(dropna=False) > 1]\n",
    "    if len(non_constant_columns) < X_train.shape[1]:\n",
    "        dropped = X_train.shape[1] - len(non_constant_columns)\n",
    "        print(f\"Removed {dropped} constant feature(s).\")\n",
    "        X_train = X_train[non_constant_columns]\n",
    "\n",
    "    # task settings\n",
    "    task = str(task_type).strip().lower()\n",
    "    if task == \"classification\":\n",
    "        scoring = \"f1_macro\"\n",
    "        selector_score_func = f_classif\n",
    "        min_class = int(y_train.value_counts().min())\n",
    "        eff_folds = max(2, min(int(num_folds), min_class))\n",
    "        baseline_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        sampler_obj = None\n",
    "        if oversample:\n",
    "            if oversample_method == \"smote\":\n",
    "                k_neighbors_for_smote = max(1, min(5, min_class - 1))\n",
    "                if k_neighbors_for_smote < 1:\n",
    "                    print(\"SMOTE not possible (minority class too small). Using RandomOverSampler.\")\n",
    "                    sampler_obj = RandomOverSampler(random_state=42)\n",
    "                else:\n",
    "                    sampler_obj = SMOTE(random_state=42, k_neighbors=k_neighbors_for_smote)\n",
    "            else:\n",
    "                sampler_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "        class_weight_choice = None if oversample else \"balanced\"\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingClassifier(random_state=42),\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=42, class_weight=class_weight_choice, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeClassifier(random_state=42, class_weight=class_weight_choice),\n",
    "            \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"LinearSVM\": LinearSVC(max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"NaiveBayes\": GaussianNB(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"Dummy\": DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "        }\n",
    "        metric_name = \"F1_macro\"\n",
    "        higher_is_better = True\n",
    "    elif task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "        selector_score_func = f_regression\n",
    "        eff_folds = max(2, int(num_folds))\n",
    "        baseline_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingRegressor(random_state=42),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(max_iter=5000),\n",
    "            \"Lasso\": Lasso(max_iter=5000),\n",
    "            \"ElasticNet\": ElasticNet(max_iter=5000),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"Dummy\": DummyRegressor(strategy=\"mean\"),\n",
    "        }\n",
    "        metric_name = \"CV_MAE\"\n",
    "        higher_is_better = False\n",
    "        sampler_obj = None\n",
    "    else:\n",
    "        raise ValueError('task_type must be \"classification\" or \"regression\"')\n",
    "\n",
    "    total_features = X_train.shape[1]\n",
    "    feature_fractions = [0.10, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "    # which models need scaling and selection\n",
    "    needs_scaling = {\"LogisticRegression\", \"LinearSVM\", \"KNN\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"NaiveBayes\"}\n",
    "    skip_selection = {\"Dummy\"}\n",
    "    tree_like = {\"RandomForest\", \"DecisionTree\", \"GBT\"}\n",
    "\n",
    "    def k_from_fraction(frac, total_cols):\n",
    "        if frac >= 1.0:\n",
    "            return \"all\"\n",
    "        k = int(max(1, math.ceil(frac * total_cols)))\n",
    "        return min(k, total_cols)\n",
    "\n",
    "    # dynamic KNN neighbors cap\n",
    "    per_fold_train = int(len(X_train) * (eff_folds - 1) / eff_folds)\n",
    "    max_knn_k = max(3, min(101, per_fold_train - 1))\n",
    "    knn_ks = list(range(3, max_knn_k + 1, 2))\n",
    "\n",
    "    def logspace_list(low_exp, high_exp, num):\n",
    "        return list(np.logspace(low_exp, high_exp, num))\n",
    "\n",
    "    def linspace_list(low_val, high_val, num):\n",
    "        return list(np.linspace(low_val, high_val, num))\n",
    "\n",
    "    param_spaces_classification = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LogisticRegression\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__solver\": [\"saga\"],\n",
    "        },\n",
    "        \"LinearSVM\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        },\n",
    "        \"NaiveBayes\": {\n",
    "            \"model__var_smoothing\": list(10 ** np.linspace(-11, -7, 9))\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"most_frequent\", \"stratified\", \"uniform\"]},\n",
    "    }\n",
    "\n",
    "    param_spaces_regression = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LinearRegression\": {},\n",
    "        \"Ridge\": {\"model__alpha\": logspace_list(-3, 3, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"Lasso\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"ElasticNet\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__l1_ratio\": linspace_list(0.1, 0.9, 9), \"model__fit_intercept\": [True, False]},\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"mean\", \"median\"]},\n",
    "    }\n",
    "\n",
    "    # build a pipeline for a given k\n",
    "    def make_pipeline_for_k(model_name, model_obj, k_value):\n",
    "        # order: align -> impute -> variance -> select -> scale -> sampler -> model\n",
    "        align_step = (\"align\", KeepTrainColumns())\n",
    "        impute_step = (\"impute\", SimpleImputer(strategy=\"median\"))\n",
    "        variance_step = (\"variance\", VarianceThreshold(threshold=0.0))\n",
    "\n",
    "        if model_name in skip_selection or model_name in tree_like:\n",
    "            select_step = (\"select\", \"passthrough\")\n",
    "        else:\n",
    "            select_step = (\"select\", SelectKBest(score_func=selector_score_func, k=k_value))\n",
    "\n",
    "        scale_step = (\"scale\", StandardScaler() if model_name in needs_scaling else \"passthrough\")\n",
    "\n",
    "        steps = [align_step, impute_step, variance_step, select_step, scale_step]\n",
    "\n",
    "        if task == \"classification\" and oversample and sampler_obj is not None:\n",
    "            steps.append((\"sampler\", sampler_obj))\n",
    "\n",
    "        steps.append((\"model\", model_obj))\n",
    "\n",
    "        if oversample and ImbPipeline is not None and task == \"classification\":\n",
    "            return ImbPipeline(steps)\n",
    "        else:\n",
    "            return SKPipeline(steps)\n",
    "\n",
    "    # baseline sweep across models × k\n",
    "    rows = []\n",
    "    total_steps = len(model_space) * len(feature_fractions)\n",
    "    step = 0\n",
    "    print(\"Streaming results (each line is one model × feature count):\")\n",
    "    for model_name, model_obj in model_space.items():\n",
    "        for frac in feature_fractions:\n",
    "            step += 1\n",
    "            k_val = k_from_fraction(frac, total_features)\n",
    "            k_print = total_features if k_val == \"all\" else int(k_val)\n",
    "            pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=baseline_cv, scoring=scoring, n_jobs=1)\n",
    "            mean_score = float(np.mean(scores))\n",
    "            std_score = float(np.std(scores))\n",
    "            if task == \"regression\":\n",
    "                display_mean = -mean_score\n",
    "                display_std = float(np.std(-scores))\n",
    "            else:\n",
    "                display_mean = mean_score\n",
    "                display_std = std_score\n",
    "            rows.append({\"Model\": model_name, \"K_features\": k_print, \"MeanScore\": display_mean, \"StdDev\": display_std, \"Metric\": metric_name})\n",
    "            print(f\"[{step}/{total_steps}] {model_name} | k={k_print} | {metric_name}={display_mean:.6f} ± {display_std:.6f}\", flush=True)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    if task == \"classification\":\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    else:\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Baseline results (CV) ===\")\n",
    "    print(results_df[[\"Model\", \"K_features\", \"MeanScore\", \"StdDev\", \"Metric\"]])\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    best_model_name = str(best_row[\"Model\"])\n",
    "    best_k = int(best_row[\"K_features\"])\n",
    "    best_model_obj = model_space[best_model_name]\n",
    "    k_val_for_search = \"all\" if best_k >= total_features else best_k\n",
    "    best_pipeline = make_pipeline_for_k(best_model_name, best_model_obj, k_val_for_search)\n",
    "\n",
    "    # pick search space\n",
    "    search_space = (param_spaces_classification if task == \"classification\" else param_spaces_regression).get(best_model_name, {})\n",
    "    if len(search_space) == 0:\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        try:\n",
    "            best_pipeline.input_columns_ = list(X_train.columns)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # threshold tuning only for classification\n",
    "        if task == \"classification\":\n",
    "            try:\n",
    "                _tune_threshold_inplace(best_pipeline, X_train, y_train, search_cv)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] threshold tuning failed: {e}\")\n",
    "        print(\"\\nBest model had no tunable params. Returning fitted pipeline.\")\n",
    "        return best_pipeline\n",
    "\n",
    "    # hyperparameter search\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=best_pipeline,\n",
    "        param_distributions=search_space,\n",
    "        n_iter=int(max(1, num_iterations)),\n",
    "        scoring=scoring,\n",
    "        cv=search_cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # print tuned CV result\n",
    "    if task == \"regression\":\n",
    "        tuned_score_display = -float(search.best_score_)\n",
    "        tuned_metric_name = \"CV MAE\"\n",
    "    else:\n",
    "        tuned_score_display = float(search.best_score_)\n",
    "        tuned_metric_name = \"F1 macro\"\n",
    "\n",
    "    print(\"\\n=== Best model after randomized search ===\")\n",
    "    print(f\"Model name: {best_model_name}\")\n",
    "    print(f\"Number of features: {best_k}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best CV score ({tuned_metric_name}): {tuned_score_display:.6f}\")\n",
    "\n",
    "    # remember training columns\n",
    "    try:\n",
    "        search.best_estimator_.input_columns_ = list(X_train.columns)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # threshold tuning only for classification\n",
    "    if task == \"classification\":\n",
    "        try:\n",
    "            _tune_threshold_inplace(search.best_estimator_, X_train, y_train, search_cv)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] threshold tuning failed: {e}\")\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "def _tune_threshold_inplace(fitted_estimator, X, y, cv):\n",
    "    \"\"\"\n",
    "    Finds a good decision threshold using out-of-fold scores on the training set.\n",
    "    Stores results on the estimator as .best_threshold_ and .best_threshold_cv_f1_.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    # try probabilities first\n",
    "    scores = None\n",
    "    try:\n",
    "        proba_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"predict_proba\", n_jobs=1)  # shape (n, 2)\n",
    "        scores = proba_oof[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback to decision function\n",
    "    if scores is None:\n",
    "        try:\n",
    "            decision_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"decision_function\", n_jobs=1)\n",
    "            dec_min, dec_max = float(decision_oof.min()), float(decision_oof.max())\n",
    "            scores = (decision_oof - dec_min) / (dec_max - dec_min + 1e-9)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # if no scores available, keep default threshold\n",
    "    if scores is None:\n",
    "        fitted_estimator.best_threshold_ = 0.5\n",
    "        fitted_estimator.best_threshold_cv_f1_ = None\n",
    "        print(\"[info] model does not expose scores for thresholding. Using 0.5.\")\n",
    "        return\n",
    "\n",
    "    # sweep thresholds\n",
    "    best_threshold = 0.5\n",
    "    best_f1_macro = -1.0\n",
    "    thresholds_to_try = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    for t in thresholds_to_try:\n",
    "        y_hat = (scores >= t).astype(int)\n",
    "        f1_macro_val = float(f1_score(y, y_hat, average=\"macro\"))\n",
    "        if f1_macro_val > best_f1_macro:\n",
    "            best_f1_macro = f1_macro_val\n",
    "            best_threshold = float(t)\n",
    "\n",
    "    # show OOF result at best threshold\n",
    "    y_hat_final = (scores >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Threshold tuning (OOF on train) ===\")\n",
    "    print(f\"Best threshold: {best_threshold:.2f} | F1_macro: {best_f1_macro:.6f}\")\n",
    "    print(\"Confusion matrix at best threshold:\")\n",
    "    print(confusion_matrix(y, y_hat_final))\n",
    "\n",
    "    # store on estimator\n",
    "    fitted_estimator.best_threshold_ = best_threshold\n",
    "    fitted_estimator.best_threshold_cv_f1_ = best_f1_macro\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Holdout evaluation helper (uses tuned threshold if available)\n",
    "# =============================\n",
    "def evaluate_on_holdout(model, X_test, y_test, task_type, threshold=None):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import f1_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "    # align columns to what the model saw at fit\n",
    "    try:\n",
    "        if hasattr(model, \"input_columns_\") and hasattr(X_test, \"reindex\"):\n",
    "            X_test = X_test.reindex(columns=model.input_columns_, fill_value=0)\n",
    "        elif hasattr(model, \"named_steps\") and \"align\" in getattr(model, \"named_steps\", {}):\n",
    "            keep_cols = getattr(model.named_steps[\"align\"], \"keep_columns_\", None)\n",
    "            if keep_cols is not None and hasattr(X_test, \"reindex\"):\n",
    "                X_test = X_test.reindex(columns=list(keep_cols), fill_value=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not align columns: {e}\")\n",
    "\n",
    "    # choose prediction path\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        final_threshold = threshold\n",
    "        if final_threshold is None and hasattr(model, \"best_threshold_\"):\n",
    "            final_threshold = float(model.best_threshold_)\n",
    "        if final_threshold is not None:\n",
    "            y_pred = predict_with_threshold(model, X_test, threshold=final_threshold)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n=== Holdout (time split) ===\")\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(f\"F1 macro: {f1:.6f}\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        return f1\n",
    "    else:\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a9032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, mean_absolute_error\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ---------------------------------\n",
    "# config\n",
    "# ---------------------------------\n",
    "FEATURE_SELECTION_OPTIONS = {\"none\", \"tree\", \"forward\", \"mutual_info\"}\n",
    "\n",
    "# ---------------------------------\n",
    "# small helpers\n",
    "# ---------------------------------\n",
    "def _scale_fit(method, X_train_num):\n",
    "    if method == \"standard\":\n",
    "        return StandardScaler().fit(X_train_num)\n",
    "    if method == \"minmax\":\n",
    "        return MinMaxScaler().fit(X_train_num)\n",
    "    return None\n",
    "\n",
    "def _select_features(method, max_features, task_type, random_state, X_train, y_train):\n",
    "    if method is None or method == \"none\":\n",
    "        return X_train.columns.tolist()\n",
    "    k = min(max_features, X_train.shape[1]) if max_features else X_train.shape[1]\n",
    "    if k < 1:\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    if method == \"tree\":\n",
    "        est = RandomForestClassifier(random_state=random_state) if task_type == \"classification\" else RandomForestRegressor(random_state=random_state)\n",
    "        est.fit(X_train, y_train)\n",
    "        imp = pd.Series(est.feature_importances_, index=X_train.columns)\n",
    "        return imp.nlargest(k).index.tolist()\n",
    "\n",
    "    if method == \"forward\":\n",
    "        if task_type == \"classification\":\n",
    "            est = RandomForestClassifier(random_state=random_state)\n",
    "            scoring = \"f1_macro\"\n",
    "        else:\n",
    "            est = RandomForestRegressor(random_state=random_state)\n",
    "            scoring = \"neg_mean_absolute_error\"\n",
    "        sfs = SequentialFeatureSelector(est, n_features_to_select=k, direction=\"forward\", scoring=scoring, cv=5, n_jobs=-1)\n",
    "        sfs.fit(X_train, y_train)\n",
    "        return X_train.columns[sfs.get_support()].tolist()\n",
    "\n",
    "    if method == \"mutual_info\":\n",
    "        sel = SelectKBest(mutual_info_classif if task_type == \"classification\" else mutual_info_regression, k=k)\n",
    "        sel.fit(X_train, y_train)\n",
    "        return X_train.columns[sel.get_support()].tolist()\n",
    "\n",
    "    return X_train.columns.tolist()\n",
    "\n",
    "def _cap_fit(X_train, q=0.95):\n",
    "    caps = {}\n",
    "    for c in X_train.select_dtypes(include=[\"number\"]).columns:\n",
    "        caps[c] = np.nanpercentile(X_train[c], q * 100.0)\n",
    "    return caps\n",
    "\n",
    "def _cap_apply(X, caps):\n",
    "    for c, v in caps.items():\n",
    "        if c in X.columns:\n",
    "            X[c] = np.clip(X[c], None, v)\n",
    "    return X\n",
    "\n",
    "def _dummify_fit(X_train, cols):\n",
    "    if not cols:\n",
    "        return X_train.columns\n",
    "    Xd = pd.get_dummies(X_train, columns=cols, drop_first=True)\n",
    "    return Xd.columns\n",
    "\n",
    "def _dummify_apply(X, cols, schema_cols):\n",
    "    if cols:\n",
    "        Xd = pd.get_dummies(X, columns=cols, drop_first=True)\n",
    "    else:\n",
    "        Xd = X.copy()\n",
    "    Xd = Xd.reindex(columns=schema_cols, fill_value=0)\n",
    "    return Xd\n",
    "\n",
    "def _datetimes_to_numeric_inplace(X):\n",
    "    # convert datetimes to float seconds since epoch\n",
    "    for c in X.columns:\n",
    "        if np.issubdtype(X[c].dtype, np.datetime64):\n",
    "            mask = X[c].isna()\n",
    "            arr = X[c].astype(\"int64\").astype(\"float64\") / 1e9\n",
    "            if mask.any():\n",
    "                arr[mask.values] = np.nan\n",
    "            X[c] = arr\n",
    "    return X\n",
    "\n",
    "def _freq_encode_objects_inplace(X_train, X_test):\n",
    "    # replace object/category columns with frequency ratios\n",
    "    obj_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        vc = X_train[c].value_counts(dropna=False)\n",
    "        total = vc.sum() if vc.sum() > 0 else 1.0\n",
    "        mapping = (vc / total).to_dict()\n",
    "        default_val = 0.0\n",
    "        X_train[c] = X_train[c].map(mapping).fillna(default_val)\n",
    "        X_test[c] = X_test[c].map(mapping).fillna(default_val)\n",
    "    return X_train, X_test\n",
    "\n",
    "def _scale_numeric_only(X_train, X_test, scale_method):\n",
    "    # scale numeric columns only\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return X_train, X_test\n",
    "    scaler = _scale_fit(scale_method, X_train[num_cols])\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "    return X_train, X_test\n",
    "\n",
    "def _find_time_col(X):\n",
    "    # try a few common time columns\n",
    "    preferred = [\n",
    "        \"date\",\n",
    "        \"order_purchase_timestamp\",\n",
    "        \"order_delivered_customer_date\",\n",
    "        \"order_estimated_delivery_date\",\n",
    "        \"date_release\",\n",
    "        \"Year\",\n",
    "        \"year\",\n",
    "    ]\n",
    "    for c in preferred:\n",
    "        if c in X.columns:\n",
    "            return c\n",
    "    # fallback: use index order\n",
    "    return None\n",
    "\n",
    "def _sort_by_time(X, y):\n",
    "    tcol = _find_time_col(X)\n",
    "    if tcol is None:\n",
    "        return X.reset_index(drop=True), y.reset_index(drop=True), None\n",
    "    key = X[tcol].values\n",
    "    order = np.argsort(key)\n",
    "    return X.iloc[order].reset_index(drop=True), y.iloc[order].reset_index(drop=True), tcol\n",
    "\n",
    "def _tune_f1_threshold(y_true, y_prob, grid=None):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return best_t, best_f1\n",
    "\n",
    "# ---------------------------------\n",
    "# main prep\n",
    "# ---------------------------------\n",
    "def prepare_data(steam_df, olist_df, sales_df, test_size, random_state,\n",
    "                 feature_selection, max_features, task_type, scale_method):\n",
    "    feature_selection = (feature_selection or \"none\").lower()\n",
    "    if feature_selection not in FEATURE_SELECTION_OPTIONS:\n",
    "        feature_selection = \"none\"\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    # ===================== STEAM =====================\n",
    "    steam = steam_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        steam[\"target\"] = (steam[\"positive_ratio\"] >= 80).astype(int)\n",
    "    else:\n",
    "        steam[\"target\"] = steam[\"positive_ratio\"]\n",
    "    steam.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # drop only clear leakage and IDs\n",
    "    steam.drop(columns=[\"app_id\", \"user_id\", \"review_id\", \"title\", \"description\", \"tags\", \"positive_ratio\", \"rating\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # keep dates as features and add days_since_release\n",
    "    if {\"date\", \"date_release\"}.issubset(steam.columns):\n",
    "        steam[\"days_since_release\"] = (steam[\"date\"] - steam[\"date_release\"]).dt.days\n",
    "\n",
    "    # keep these binary flags\n",
    "    if \"is_recommended\" in steam.columns:\n",
    "        steam[\"is_recommended\"] = steam[\"is_recommended\"].astype(int)\n",
    "    if \"mac\" in steam.columns:\n",
    "        steam[\"mac\"] = steam[\"mac\"].astype(int)\n",
    "    if \"linux\" in steam.columns:\n",
    "        steam[\"linux\"] = steam[\"linux\"].astype(int)\n",
    "\n",
    "    # basic feature ideas\n",
    "    if \"hours\" in steam.columns:\n",
    "        steam[\"log_hours\"] = np.log1p(steam[\"hours\"])\n",
    "    if {\"hours\", \"user_reviews\"}.issubset(steam.columns):\n",
    "        steam[\"reviews_per_hour\"] = steam[\"user_reviews\"] / (steam[\"hours\"] + 1e-9)\n",
    "\n",
    "    X = steam.drop(columns=[\"target\"])\n",
    "    y = steam[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=strat)\n",
    "\n",
    "    _datetimes_to_numeric_inplace(X_train)\n",
    "    _datetimes_to_numeric_inplace(X_test)\n",
    "    X_train, X_test = _freq_encode_objects_inplace(X_train, X_test)\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "\n",
    "    caps = _cap_fit(X_train.select_dtypes(include=[\"number\"]), q=0.95)\n",
    "    X_train = _cap_apply(X_train, caps)\n",
    "    X_test = _cap_apply(X_test, caps)\n",
    "\n",
    "    # no oversampling here; it happens inside CV if you enable it\n",
    "\n",
    "    X_train, X_test = _scale_numeric_only(X_train, X_test, scale_method)\n",
    "\n",
    "    keep_cols = _select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    outputs[\"steam\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ===================== OLIST =====================\n",
    "    olist = olist_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        olist[\"target\"] = (olist[\"review_score_mean_product\"] >= 4.0).astype(int)\n",
    "    else:\n",
    "        olist[\"target\"] = olist[\"review_score_mean_product\"]\n",
    "    olist.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # drop only clear IDs\n",
    "    olist.drop(columns=[\"order_id\", \"customer_id\", \"customer_unique_id\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # keep order_status (not direct leakage)\n",
    "    if {\"order_purchase_timestamp\", \"order_estimated_delivery_date\"}.issubset(olist.columns):\n",
    "        olist[\"delivery_delay\"] = (olist[\"order_estimated_delivery_date\"] - olist[\"order_purchase_timestamp\"]).dt.days\n",
    "\n",
    "    if {\"payment_value_total\", \"payment_installments_max\"}.issubset(olist.columns):\n",
    "        olist[\"avg_installment\"] = olist[\"payment_value_total\"] / olist[\"payment_installments_max\"].replace(0, 1)\n",
    "\n",
    "    X = olist.drop(columns=[\"review_score_mean_product\", \"target\"], errors=\"ignore\")\n",
    "    y = olist[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=strat)\n",
    "\n",
    "    _datetimes_to_numeric_inplace(X_train)\n",
    "    _datetimes_to_numeric_inplace(X_test)\n",
    "    X_train, X_test = _freq_encode_objects_inplace(X_train, X_test)\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "\n",
    "    dims = [\"product_length_cm\", \"product_height_cm\", \"product_width_cm\"]\n",
    "    if all(d in X_train.columns for d in dims):\n",
    "        X_train[\"volume\"] = X_train[\"product_length_cm\"] * X_train[\"product_height_cm\"] * X_train[\"product_width_cm\"]\n",
    "        X_test[\"volume\"] = X_test[\"product_length_cm\"] * X_test[\"product_height_cm\"] * X_test[\"product_width_cm\"]\n",
    "        X_train.drop(columns=dims, inplace=True, errors=\"ignore\")\n",
    "        X_test.drop(columns=dims, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    small_cats = []  # none after freq-encode\n",
    "    schema_cols = _dummify_fit(X_train.copy(), small_cats)\n",
    "    X_train = _dummify_apply(X_train, small_cats, schema_cols)\n",
    "    X_test = _dummify_apply(X_test, small_cats, schema_cols)\n",
    "\n",
    "    caps = _cap_fit(X_train.select_dtypes(include=[\"number\"]), q=0.95)\n",
    "    X_train = _cap_apply(X_train, caps)\n",
    "    X_test = _cap_apply(X_test, caps)\n",
    "\n",
    "    # no oversampling here\n",
    "\n",
    "    X_train, X_test = _scale_numeric_only(X_train, X_test, scale_method)\n",
    "\n",
    "    keep_cols = _select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    outputs[\"olist\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ===================== SALES =====================\n",
    "    sales = sales_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        sales[\"target\"] = (sales[\"Critic_Score\"] >= 8.0).astype(int)\n",
    "    else:\n",
    "        sales[\"target\"] = sales[\"Critic_Score\"]\n",
    "    sales.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    # drop only obvious leakage and outcomes not usable at predict time\n",
    "    sales.drop(columns=[\"Rank\", \"Name\", \"Publisher\", \"Developer\", \"Total_Shipped\", \"Global_Sales\", \"NA_Sales\", \"PAL_Sales\", \"JP_Sales\", \"Other_Sales\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    for c in [\"ESRB_Rating\", \"Genre\", \"Platform\"]:\n",
    "        if c in sales.columns:\n",
    "            sales[c] = sales[c].fillna(\"Unknown\")\n",
    "\n",
    "    X = sales.drop(columns=[\"target\", \"Critic_Score\"], errors=\"ignore\")\n",
    "    y = sales[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=strat)\n",
    "\n",
    "    _datetimes_to_numeric_inplace(X_train)\n",
    "    _datetimes_to_numeric_inplace(X_test)\n",
    "    X_train, X_test = _freq_encode_objects_inplace(X_train, X_test)\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "\n",
    "    caps = _cap_fit(X_train.select_dtypes(include=[\"number\"]), q=0.95)\n",
    "    X_train = _cap_apply(X_train, caps)\n",
    "    X_test = _cap_apply(X_test, caps)\n",
    "\n",
    "    # no oversampling here\n",
    "\n",
    "    X_train, X_test = _scale_numeric_only(X_train, X_test, scale_method)\n",
    "\n",
    "    keep_cols = _select_features(feature_selection, max_features, task_type, random_state, X_train, y_train)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    outputs[\"sales\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b18a3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: start downloads\n",
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.361 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.217 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.186 sec\n",
      "main: downloads finished\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 10000 of 41154794 rows in 5.885 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(10000, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: picked 10000 of 99441 rows in 0.001 sec\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (11444, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: picked 10000 of 55792 rows in 0.022 sec\n",
      "vg2019: done shape=(10000, 16)\n",
      "main: load all done in 17.211 sec (00:00:17)\n",
      "download: shapes summary\n",
      "download: steam shape = (10000, 24)\n",
      "download: olist shape = (11444, 38)\n",
      "download: sales shape = (10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Download Paths\n",
    "# =============================\n",
    "print(\"main: start downloads\")\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "print(\"main: downloads finished\")\n",
    "\n",
    "# =============================\n",
    "# Load All\n",
    "# =============================\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# =============================\n",
    "# Download Shapes\n",
    "# =============================\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e087b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBUST EDA ON STEAM\n",
      "=== Robust EDA Report ===\n",
      "\n",
      "DataFrame Shape: (10000, 24)\n",
      "\n",
      "Data Types:\n",
      " app_id                     int64\n",
      "helpful                    int64\n",
      "funny                      int64\n",
      "date              datetime64[ns]\n",
      "is_recommended              bool\n",
      "hours                    float64\n",
      "user_id                    int64\n",
      "review_id                  int64\n",
      "title                     object\n",
      "date_release      datetime64[ns]\n",
      "win                         bool\n",
      "mac                         bool\n",
      "linux                       bool\n",
      "rating                    object\n",
      "positive_ratio             int64\n",
      "user_reviews               int64\n",
      "price_final              float64\n",
      "price_original           float64\n",
      "discount                 float64\n",
      "steam_deck                  bool\n",
      "description               object\n",
      "tags                      object\n",
      "products                   int64\n",
      "reviews                    int64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      " app_id            0\n",
      "helpful           0\n",
      "funny             0\n",
      "date              0\n",
      "is_recommended    0\n",
      "hours             0\n",
      "user_id           0\n",
      "review_id         0\n",
      "title             0\n",
      "date_release      0\n",
      "win               0\n",
      "mac               0\n",
      "linux             0\n",
      "rating            0\n",
      "positive_ratio    0\n",
      "user_reviews      0\n",
      "price_final       0\n",
      "price_original    0\n",
      "discount          0\n",
      "steam_deck        0\n",
      "description       0\n",
      "tags              0\n",
      "products          0\n",
      "reviews           0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows: Cannot compute due to unhashable types in DataFrame (e.g., lists in columns).\n",
      "\n",
      "Head of DataFrame:\n",
      "     app_id  helpful  funny       date  is_recommended      hours  user_id  \\\n",
      "0  1222670        0      0 2022-09-18            True 118.800000  3860966   \n",
      "1  1091500        0      0 2022-12-28           False   4.400000  3320671   \n",
      "2  1237320        0      0 2022-12-31            True  27.900000  4887817   \n",
      "3   233860        3      0 2018-07-13            True  82.300000  5220359   \n",
      "4   284160        0      0 2020-08-28            True  73.000000  8726467   \n",
      "\n",
      "   review_id            title date_release  ...  positive_ratio  user_reviews  \\\n",
      "0       1499      The Sims™ 4   2020-06-18  ...              87        110750   \n",
      "1       2802   Cyberpunk 2077   2020-12-09  ...              80        557051   \n",
      "2       3069  Sonic Frontiers   2022-11-07  ...              93         14850   \n",
      "3       4145           Kenshi   2018-12-06  ...              95         62332   \n",
      "4       6337     BeamNG.drive   2015-05-29  ...              97        178635   \n",
      "\n",
      "   price_final price_original  discount  steam_deck  description  tags  \\\n",
      "0     0.000000       0.000000  0.000000        True                 []   \n",
      "1    60.000000       0.000000  0.000000        True                 []   \n",
      "2    60.000000       0.000000  0.000000        True                 []   \n",
      "3    30.000000       0.000000  0.000000        True                 []   \n",
      "4    25.000000       0.000000  0.000000        True                 []   \n",
      "\n",
      "   products  reviews  \n",
      "0         5        1  \n",
      "1        68        5  \n",
      "2       148        5  \n",
      "3       103       16  \n",
      "4       119       27  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "=== Numerical Columns Analysis ===\n",
      "\n",
      "Summary Statistics (with 5%, 25%, 50%, 75%, 95% percentiles):\n",
      "               app_id      helpful        funny        hours         user_id  \\\n",
      "count   10000.000000 10000.000000 10000.000000 10000.000000    10000.000000   \n",
      "mean   599491.220200     3.086700     0.984800    98.613400  7469461.674500   \n",
      "std    472179.661750    38.879842    16.931764   172.462586  3995032.126820   \n",
      "min        10.000000     0.000000     0.000000     0.000000     1072.000000   \n",
      "5%       8847.500000     0.000000     0.000000     0.800000   772104.400000   \n",
      "25%    252490.000000     0.000000     0.000000     7.700000  4358313.250000   \n",
      "50%    433340.000000     0.000000     0.000000    26.900000  7545213.000000   \n",
      "75%    916952.500000     0.000000     0.000000    97.500000 10984898.750000   \n",
      "95%   1549180.000000     8.000000     2.000000   496.095000 13573287.500000   \n",
      "max   2206340.000000  3259.000000  1070.000000   998.800000 14304565.000000   \n",
      "\n",
      "            review_id  positive_ratio   user_reviews  price_final  \\\n",
      "count    10000.000000    10000.000000   10000.000000 10000.000000   \n",
      "mean  20545984.688300       86.208500  174295.604600    18.622725   \n",
      "std   11869151.354655       11.123498  579917.401950    16.560762   \n",
      "min       1499.000000       13.000000      10.000000     0.000000   \n",
      "5%     2126884.450000       64.000000     653.950000     0.000000   \n",
      "25%   10165465.750000       82.000000   10514.750000     4.990000   \n",
      "50%   20557149.000000       89.000000   45796.000000    15.000000   \n",
      "75%   30850872.000000       94.000000  136055.000000    29.990000   \n",
      "95%   39147358.100000       97.000000  637341.000000    59.990000   \n",
      "max   41152293.000000      100.000000 7494460.000000    70.000000   \n",
      "\n",
      "       price_original     discount     products      reviews  \n",
      "count    10000.000000 10000.000000 10000.000000 10000.000000  \n",
      "mean         7.442697     3.321200   274.908500    26.350200  \n",
      "std         12.041573    15.152227   674.205738   164.819288  \n",
      "min          0.000000     0.000000     0.000000     1.000000  \n",
      "5%           0.000000     0.000000    12.000000     1.000000  \n",
      "25%          0.000000     0.000000    49.000000     2.000000  \n",
      "50%          0.000000     0.000000   116.000000     5.000000  \n",
      "75%         14.990000     0.000000   264.000000    15.000000  \n",
      "95%         29.990000     0.000000   926.000000    75.050000  \n",
      "max         79.980000    90.000000 19774.000000  6045.000000  \n",
      "\n",
      "Skewness:\n",
      " app_id            0.945135\n",
      "helpful          62.508928\n",
      "funny            44.665099\n",
      "hours             2.801352\n",
      "user_id          -0.115645\n",
      "review_id         0.009927\n",
      "positive_ratio   -1.917753\n",
      "user_reviews     10.241628\n",
      "price_final       0.902062\n",
      "price_original    1.875139\n",
      "discount          4.543284\n",
      "products         13.023190\n",
      "reviews          22.010897\n",
      "dtype: float64\n",
      "\n",
      "Kurtosis:\n",
      " app_id              0.089466\n",
      "helpful          4983.159851\n",
      "funny            2368.319774\n",
      "hours               8.186668\n",
      "user_id            -1.089530\n",
      "review_id          -1.199204\n",
      "positive_ratio      5.259712\n",
      "user_reviews      122.591565\n",
      "price_final         0.180778\n",
      "price_original      3.697629\n",
      "discount           19.337049\n",
      "products          281.904033\n",
      "reviews           603.980879\n",
      "dtype: float64\n",
      "\n",
      "Outlier Counts (IQR method):\n",
      " app_id             110\n",
      "helpful           2107\n",
      "funny              643\n",
      "hours             1277\n",
      "user_id              0\n",
      "review_id            0\n",
      "positive_ratio     498\n",
      "user_reviews      1209\n",
      "price_final         10\n",
      "price_original     353\n",
      "discount           487\n",
      "products           970\n",
      "reviews           1173\n",
      "dtype: int64\n",
      "\n",
      "Correlation Matrix:\n",
      "                   app_id   helpful     funny     hours   user_id  review_id  \\\n",
      "app_id          1.000000 -0.002930 -0.007099 -0.129953 -0.038656   0.018711   \n",
      "helpful        -0.002930  1.000000  0.408794  0.002932 -0.003928  -0.004807   \n",
      "funny          -0.007099  0.408794  1.000000  0.001283 -0.004810  -0.002500   \n",
      "hours          -0.129953  0.002932  0.001283  1.000000 -0.016284  -0.233563   \n",
      "user_id        -0.038656 -0.003928 -0.004810 -0.016284  1.000000   0.010005   \n",
      "review_id       0.018711 -0.004807 -0.002500 -0.233563  0.010005   1.000000   \n",
      "positive_ratio -0.161827 -0.012311  0.004805  0.019357  0.010324  -0.117824   \n",
      "user_reviews   -0.155326  0.005315  0.009511  0.266046 -0.029112  -0.183874   \n",
      "price_final     0.112610  0.006903  0.007895  0.038505  0.001255  -0.226662   \n",
      "price_original  0.024748  0.002336 -0.000077 -0.157804  0.037213   0.198994   \n",
      "discount       -0.030042 -0.003899 -0.003765 -0.078525  0.015539   0.097526   \n",
      "products       -0.005167  0.020429  0.012144 -0.067875  0.050912   0.089226   \n",
      "reviews        -0.000123  0.008610  0.000962 -0.054228  0.019403   0.075168   \n",
      "\n",
      "                positive_ratio  user_reviews  price_final  price_original  \\\n",
      "app_id               -0.161827     -0.155326     0.112610        0.024748   \n",
      "helpful              -0.012311      0.005315     0.006903        0.002336   \n",
      "funny                 0.004805      0.009511     0.007895       -0.000077   \n",
      "hours                 0.019357      0.266046     0.038505       -0.157804   \n",
      "user_id               0.010324     -0.029112     0.001255        0.037213   \n",
      "review_id            -0.117824     -0.183874    -0.226662        0.198994   \n",
      "positive_ratio        1.000000      0.027136     0.020754       -0.100965   \n",
      "user_reviews          0.027136      1.000000    -0.053319       -0.161077   \n",
      "price_final           0.020754     -0.053319     1.000000        0.207524   \n",
      "price_original       -0.100965     -0.161077     0.207524        1.000000   \n",
      "discount              0.001847     -0.054650    -0.171852        0.246890   \n",
      "products             -0.042591     -0.056194    -0.031359        0.086113   \n",
      "reviews              -0.059781     -0.030664    -0.052080        0.007127   \n",
      "\n",
      "                discount  products   reviews  \n",
      "app_id         -0.030042 -0.005167 -0.000123  \n",
      "helpful        -0.003899  0.020429  0.008610  \n",
      "funny          -0.003765  0.012144  0.000962  \n",
      "hours          -0.078525 -0.067875 -0.054228  \n",
      "user_id         0.015539  0.050912  0.019403  \n",
      "review_id       0.097526  0.089226  0.075168  \n",
      "positive_ratio  0.001847 -0.042591 -0.059781  \n",
      "user_reviews   -0.054650 -0.056194 -0.030664  \n",
      "price_final    -0.171852 -0.031359 -0.052080  \n",
      "price_original  0.246890  0.086113  0.007127  \n",
      "discount        1.000000  0.027869  0.025135  \n",
      "products        0.027869  1.000000  0.335938  \n",
      "reviews         0.025135  0.335938  1.000000  \n",
      "\n",
      "=== Categorical Columns Analysis ===\n",
      "\n",
      "Value Counts for 'date':\n",
      " date\n",
      "2019-06-29    76\n",
      "2021-11-24    42\n",
      "2019-06-30    35\n",
      "2020-11-25    32\n",
      "2021-11-25    32\n",
      "              ..\n",
      "2013-12-31     1\n",
      "2012-10-05     1\n",
      "2015-12-21     1\n",
      "2015-05-29     1\n",
      "2013-06-23     1\n",
      "Name: count, Length: 2762, dtype: int64\n",
      "Unique Values in 'date': 2762\n",
      "Most Common Value in 'date': 2019-06-29 00:00:00\n",
      "\n",
      "Value Counts for 'is_recommended':\n",
      " is_recommended\n",
      "True     8578\n",
      "False    1422\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'is_recommended': 2\n",
      "Most Common Value in 'is_recommended': True\n",
      "\n",
      "Value Counts for 'title':\n",
      " title\n",
      "Team Fortress 2              77\n",
      "Cyberpunk 2077               61\n",
      "Paladins®                    60\n",
      "Wallpaper Engine             58\n",
      "The Witcher® 3: Wild Hunt    53\n",
      "                             ..\n",
      "Pixel Painter                 1\n",
      "A Year Of Rain                1\n",
      "Locoland                      1\n",
      "Jaws Of Extinction™           1\n",
      "Tainted Grail: Conquest       1\n",
      "Name: count, Length: 2878, dtype: int64\n",
      "Unique Values in 'title': 2878\n",
      "Most Common Value in 'title': Team Fortress 2\n",
      "\n",
      "Value Counts for 'date_release':\n",
      " date_release\n",
      "2007-10-10    119\n",
      "2018-05-08     81\n",
      "2018-11-01     62\n",
      "2020-12-09     61\n",
      "2015-04-13     58\n",
      "             ... \n",
      "2022-04-07      1\n",
      "2020-04-10      1\n",
      "2016-09-22      1\n",
      "2022-11-02      1\n",
      "2010-09-27      1\n",
      "Name: count, Length: 1831, dtype: int64\n",
      "Unique Values in 'date_release': 1831\n",
      "Most Common Value in 'date_release': 2007-10-10 00:00:00\n",
      "\n",
      "Value Counts for 'win':\n",
      " win\n",
      "True     9997\n",
      "False       3\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'win': 2\n",
      "Most Common Value in 'win': True\n",
      "\n",
      "Value Counts for 'mac':\n",
      " mac\n",
      "False    6040\n",
      "True     3960\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'mac': 2\n",
      "Most Common Value in 'mac': False\n",
      "\n",
      "Value Counts for 'linux':\n",
      " linux\n",
      "False    7213\n",
      "True     2787\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'linux': 2\n",
      "Most Common Value in 'linux': False\n",
      "\n",
      "Value Counts for 'rating':\n",
      " rating\n",
      "Very Positive              5722\n",
      "Overwhelmingly Positive    2397\n",
      "Mostly Positive            1088\n",
      "Mixed                       714\n",
      "Mostly Negative              46\n",
      "Positive                     26\n",
      "Overwhelmingly Negative       6\n",
      "Negative                      1\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'rating': 8\n",
      "Most Common Value in 'rating': Very Positive\n",
      "\n",
      "Value Counts for 'steam_deck':\n",
      " steam_deck\n",
      "True     9997\n",
      "False       3\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'steam_deck': 2\n",
      "Most Common Value in 'steam_deck': True\n",
      "\n",
      "Value Counts for 'description':\n",
      " description\n",
      "                                                                                                                                                                                                                                                                                                       5060\n",
      "Tomb Raider explores the intense origin story of Lara Croft and her ascent from a young woman to a hardened survivor.                                                                                                                                                                                    35\n",
      "You're a survivor in the zombie infested ruins of society, and must work with your friends and forge alliances to remain among the living.                                                                                                                                                               33\n",
      "Episode 1 now FREE! Life is Strange is an award-winning and critically acclaimed episodic adventure game that allows the player to rewind time and affect the past, present and future.                                                                                                                  27\n",
      "Deep within the SCP Foundation during a containment breach, many of the anomalies have bypassed security and escaped from their chambers - without peaceful intentions. Become site personnel, a re-containment agent, or an anomalous entity and fight to take control of or escape the facility!       26\n",
      "                                                                                                                                                                                                                                                                                                       ... \n",
      "Fortify is a base designer for Rust. Quickly plan your base with lots of helpful tools and up to 3 other friends. See stability, raid durability and get a resource count with upkeep. Frequently updated with Rust changes and new features. Share designs easily and browse the workshop.               1\n",
      "Shank is the cult-classic revival of the sidescrolling beat-em-up. Play as Shank in an over-the-top grindhouse game, packed to the rim with enemies, bosses, combos, and more by the award-winning team at Klei Entertainment.                                                                            1\n",
      "Welcome to Automachef, a resource management puzzler where you design kitchens, program machinery and watch your genius come to life! It’s time to engineer tomorrow’s kitchen, today!                                                                                                                    1\n",
      "Ice Lakes is a modern ice fishing simulator with different single and multiplayer game modes and sandbox approach to wintertime fishing. Use and customize wide selection of fishing gear and learn how changing season, bottom topology, time of day and weather conditions affects fish behavior.       1\n",
      "A unique, infinitely replayable, story-driven hybrid between a deck-building Roguelike and an RPG game. Explore the ever-changing maps, fight with deadly enemies, and learn what happened to the cursed island of Avalon.                                                                                1\n",
      "Name: count, Length: 2390, dtype: int64\n",
      "Unique Values in 'description': 2390\n",
      "Most Common Value in 'description': \n",
      "\n",
      "Value Counts for 'tags':\n",
      " tags\n",
      "[]                                                                                                                                                                                                                                                                     5060\n",
      "[Adventure, Action, Female Protagonist, Third Person, Singleplayer, Story Rich, Third-Person Shooter, Multiplayer, Exploration, Action-Adventure, Quick-Time Events, Atmospheric, Shooter, Puzzle, Stealth, Cinematic, Platformer, RPG, Reboot, 3D Vision]               35\n",
      "[Free to Play, Survival, Zombies, Multiplayer, Open World, Co-op, Crafting, Sandbox, Adventure, Shooter, First-Person, Post-apocalyptic, Looter Shooter, Action, FPS, Singleplayer, Massively Multiplayer, Atmospheric, Indie, Casual]                                   33\n",
      "[Story Rich, Choices Matter, Great Soundtrack, Female Protagonist, Time Travel, Atmospheric, Episodic, Singleplayer, Adventure, Time Manipulation, Multiple Endings, Mystery, Third Person, Point & Click, Indie, Walking Simulator, Casual, Action, Memes, LGBTQ+]      27\n",
      "[Free to Play, Horror, Multiplayer, First-Person, Co-op, Survival Horror, Shooter, Online Co-Op, Action, FPS, Memes, Sci-fi, Survival, Psychological Horror, Atmospheric, Strategy, Difficult, Indie, Adventure, Fantasy]                                                26\n",
      "                                                                                                                                                                                                                                                                       ... \n",
      "[Anime, RPG, Strategy, JRPG, Female Protagonist, Cute, Singleplayer, Strategy RPG, Nudity, Memes, Visual Novel, Turn-Based Tactics, Turn-Based, Sexual Content, Tactical RPG, Funny, Comedy]                                                                              1\n",
      "[Indie, Building, Base Building, Destruction, Physics, Family Friendly, Multiplayer, Online Co-Op, Simulation]                                                                                                                                                            1\n",
      "[Action, Beat 'em up, Indie, 2D, Side Scroller, Local Co-Op, Platformer, Gore, Singleplayer, Co-op, Short, Controller, Adventure, Hack and Slash, Multiplayer, Local Multiplayer, Old School, Violent, Great Soundtrack, Cartoon]                                         1\n",
      "[Simulation, Indie, Puzzle, Automation, Resource Management, Funny, Programming, Cooking, Management, Singleplayer, Building, Casual]                                                                                                                                     1\n",
      "[Deckbuilding, Dark Fantasy, Card Game, Roguelike, Story Rich, RPG, Turn-Based, Choices Matter, Singleplayer, Base Building, Indie, CRPG, Board Game, Replay Value, Roguelite, Card Battler, Isometric, Turn-Based Combat, Action, Survival]                              1\n",
      "Name: count, Length: 2395, dtype: int64\n",
      "\n",
      "Cannot analyze 'tags' due to unhashable types (e.g., lists).\n",
      "\n",
      "=== End of EDA Report ===\n",
      "END OF ROBUST EDA ON STEAM\n",
      "\n",
      "ROBUST EDA ON OLIST\n",
      "=== Robust EDA Report ===\n",
      "\n",
      "DataFrame Shape: (11444, 38)\n",
      "\n",
      "Data Types:\n",
      " order_id                                 object\n",
      "customer_id                              object\n",
      "order_status                             object\n",
      "order_purchase_timestamp         datetime64[ns]\n",
      "order_approved_at                        object\n",
      "order_delivered_carrier_date     datetime64[ns]\n",
      "order_delivered_customer_date    datetime64[ns]\n",
      "order_estimated_delivery_date    datetime64[ns]\n",
      "customer_unique_id                       object\n",
      "customer_zip_code_prefix                  int64\n",
      "customer_city                            object\n",
      "customer_state                           object\n",
      "geolocation_lat                         float64\n",
      "geolocation_lng                         float64\n",
      "geo_points                              float64\n",
      "order_item_id                           float64\n",
      "product_id                               object\n",
      "seller_id                                object\n",
      "shipping_limit_date              datetime64[ns]\n",
      "price                                   float64\n",
      "freight_value                           float64\n",
      "product_category_name                    object\n",
      "product_name_lenght                     float64\n",
      "product_description_lenght              float64\n",
      "product_photos_qty                      float64\n",
      "product_weight_g                        float64\n",
      "product_length_cm                       float64\n",
      "product_height_cm                       float64\n",
      "product_width_cm                        float64\n",
      "product_category_name_english            object\n",
      "seller_zip_code_prefix                  float64\n",
      "seller_city                              object\n",
      "seller_state                             object\n",
      "payment_value_total                     float64\n",
      "payment_installments_max                  int64\n",
      "payment_count                             int64\n",
      "review_count_product                    float64\n",
      "review_score_mean_product               float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      " order_id                           0\n",
      "customer_id                        0\n",
      "order_status                       0\n",
      "order_purchase_timestamp           0\n",
      "order_approved_at                 21\n",
      "order_delivered_carrier_date     213\n",
      "order_delivered_customer_date    339\n",
      "order_estimated_delivery_date      0\n",
      "customer_unique_id                 0\n",
      "customer_zip_code_prefix           0\n",
      "customer_city                      0\n",
      "customer_state                     0\n",
      "geolocation_lat                   28\n",
      "geolocation_lng                   28\n",
      "geo_points                        28\n",
      "order_item_id                     80\n",
      "product_id                        80\n",
      "seller_id                         80\n",
      "shipping_limit_date               80\n",
      "price                             80\n",
      "freight_value                     80\n",
      "product_category_name            245\n",
      "product_name_lenght              245\n",
      "product_description_lenght       245\n",
      "product_photos_qty               245\n",
      "product_weight_g                  82\n",
      "product_length_cm                 82\n",
      "product_height_cm                 82\n",
      "product_width_cm                  82\n",
      "product_category_name_english    249\n",
      "seller_zip_code_prefix            80\n",
      "seller_city                       80\n",
      "seller_state                      80\n",
      "payment_value_total                0\n",
      "payment_installments_max           0\n",
      "payment_count                      0\n",
      "review_count_product             128\n",
      "review_score_mean_product        128\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Head of DataFrame:\n",
      "                            order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  5acce57f8d9dfd55fa48e212a641a69d  295ae9b35379e077273387ff64354b6f   \n",
      "2  138849fd84dff2fb4ca70a0a34c4aa1c  9b18f3fc296990b97854e351334a32f6   \n",
      "3  e425680f760cbc130be3e53a9773c584  f178c1827f67a8467b0385b7378d951a   \n",
      "4  2edfd6d1f0b4cd0db4bf37b1b224d855  241e78de29b3090cfa1b5d73a8130c72   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2017-07-31 21:37:10  2017-08-02 02:56:02   \n",
      "2    delivered      2018-02-01 14:02:19  2018-02-03 02:53:07   \n",
      "3    delivered      2017-08-31 08:15:24  2017-08-31 08:30:17   \n",
      "4    delivered      2017-06-13 21:11:26  2017-06-15 03:05:45   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2017-08-03 18:32:48           2017-08-08 21:24:41   \n",
      "2          2018-02-06 19:13:26           2018-02-14 13:41:59   \n",
      "3          2017-08-31 20:06:14           2017-09-04 20:59:55   \n",
      "4          2017-06-16 14:55:37           2017-06-19 18:51:28   \n",
      "\n",
      "  order_estimated_delivery_date                customer_unique_id  \\\n",
      "0                    2017-10-18  7c396fd4830fd04220f754e42b4e5bff   \n",
      "1                    2017-08-22  f1f4f45c8602d0db1329eed1c8e935d4   \n",
      "2                    2018-02-23  b2cac0b16835dabf811b204127f58afa   \n",
      "3                    2017-09-20  9d9ab3b77f0416765b3fbedf94a942a4   \n",
      "4                    2017-07-06  c63e44efa43f3947087aee96b388d949   \n",
      "\n",
      "   customer_zip_code_prefix  ... product_width_cm  \\\n",
      "0                      3149  ...        13.000000   \n",
      "1                     19780  ...        28.000000   \n",
      "2                      6330  ...        11.000000   \n",
      "3                     12070  ...        20.000000   \n",
      "4                      4658  ...        80.000000   \n",
      "\n",
      "     product_category_name_english  seller_zip_code_prefix     seller_city  \\\n",
      "0                       housewares             9350.000000            maua   \n",
      "1            computers_accessories            31255.000000  belo horizonte   \n",
      "2  construction_tools_construction            13360.000000        capivari   \n",
      "3                  furniture_decor             3204.000000       sao paulo   \n",
      "4                   sports_leisure            83323.000000         pinhais   \n",
      "\n",
      "   seller_state  payment_value_total payment_installments_max payment_count  \\\n",
      "0            SP            38.710000                        1             3   \n",
      "1            MG            43.000000                        1             1   \n",
      "2            SP            52.840000                        1             1   \n",
      "3            SP            50.250000                        1             1   \n",
      "4            PR           141.150000                        1             1   \n",
      "\n",
      "  review_count_product  review_score_mean_product  \n",
      "0             2.000000                   4.000000  \n",
      "1             1.000000                   5.000000  \n",
      "2             1.000000                   5.000000  \n",
      "3             2.000000                   4.000000  \n",
      "4             3.000000                   4.666667  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "\n",
      "=== Numerical Columns Analysis ===\n",
      "\n",
      "Summary Statistics (with 5%, 25%, 50%, 75%, 95% percentiles):\n",
      "        customer_zip_code_prefix  geolocation_lat  geolocation_lng  \\\n",
      "count              11444.000000     11416.000000     11416.000000   \n",
      "mean               34626.971164       -21.253642       -46.188584   \n",
      "std                29513.915008         5.450153         3.991135   \n",
      "min                 1004.000000       -33.689948       -68.502935   \n",
      "5%                  3338.050000       -28.264439       -52.271901   \n",
      "25%                11295.000000       -23.586323       -48.049677   \n",
      "50%                24030.000000       -22.916897       -46.632292   \n",
      "75%                57240.000000       -20.252688       -43.704435   \n",
      "95%                90040.000000        -8.038037       -38.543398   \n",
      "max                99965.000000         3.842508       -34.823063   \n",
      "\n",
      "        geo_points  order_item_id        price  freight_value  \\\n",
      "count 11416.000000   11364.000000 11364.000000   11364.000000   \n",
      "mean    157.252278       1.201250   120.279328      19.744683   \n",
      "std     158.108211       0.663923   186.394305      15.323438   \n",
      "min       1.000000       1.000000     2.200000       0.000000   \n",
      "5%       18.000000       1.000000    17.000000       7.780000   \n",
      "25%      54.000000       1.000000    39.000000      13.080000   \n",
      "50%     103.000000       1.000000    72.900000      16.160000   \n",
      "75%     202.000000       1.000000   134.900000      20.980000   \n",
      "95%     481.000000       2.000000   349.900000      44.098000   \n",
      "max    1146.000000      11.000000  4690.000000     294.760000   \n",
      "\n",
      "       product_name_lenght  product_description_lenght  product_photos_qty  \\\n",
      "count         11199.000000                11199.000000        11199.000000   \n",
      "mean             48.880346                  786.621216            2.197071   \n",
      "std              10.047190                  650.790272            1.693417   \n",
      "min               5.000000                    4.000000            1.000000   \n",
      "5%               29.000000                  161.000000            1.000000   \n",
      "25%              43.000000                  345.000000            1.000000   \n",
      "50%              52.000000                  600.000000            1.000000   \n",
      "75%              57.000000                  986.500000            3.000000   \n",
      "95%              60.000000                 2104.100000            6.000000   \n",
      "max              72.000000                 3976.000000           15.000000   \n",
      "\n",
      "       product_weight_g  product_length_cm  product_height_cm  \\\n",
      "count      11362.000000       11362.000000       11362.000000   \n",
      "mean        2022.428622          29.937863          16.337529   \n",
      "std         3610.452785          16.201601          13.235647   \n",
      "min            0.000000           7.000000           2.000000   \n",
      "5%           119.050000          16.000000           3.000000   \n",
      "25%          300.000000          18.000000           8.000000   \n",
      "50%          650.000000          25.000000          13.000000   \n",
      "75%         1750.000000          37.000000          20.000000   \n",
      "95%         9400.000000          61.000000          44.000000   \n",
      "max        30000.000000         105.000000         105.000000   \n",
      "\n",
      "       product_width_cm  seller_zip_code_prefix  payment_value_total  \\\n",
      "count      11362.000000            11364.000000         11444.000000   \n",
      "mean          22.673209            24627.663675           180.862915   \n",
      "std           11.517267            27594.655486           275.003076   \n",
      "min            6.000000             1001.000000             9.590000   \n",
      "5%            11.000000             2955.000000            33.230000   \n",
      "25%           15.000000             6653.000000            64.335000   \n",
      "50%           20.000000            13843.000000           113.800000   \n",
      "75%           30.000000            29157.000000           194.500000   \n",
      "95%           45.000000            88303.000000           540.760000   \n",
      "max          100.000000            99730.000000          7274.880000   \n",
      "\n",
      "       payment_installments_max  payment_count  review_count_product  \\\n",
      "count              11444.000000   11444.000000          11316.000000   \n",
      "mean                   3.019748       1.042905              3.981707   \n",
      "std                    2.805307       0.348770              6.840892   \n",
      "min                    1.000000       1.000000              1.000000   \n",
      "5%                     1.000000       1.000000              1.000000   \n",
      "25%                    1.000000       1.000000              1.000000   \n",
      "50%                    2.000000       1.000000              1.000000   \n",
      "75%                    4.000000       1.000000              3.000000   \n",
      "95%                   10.000000       1.000000             17.000000   \n",
      "max                   24.000000      13.000000             47.000000   \n",
      "\n",
      "       review_score_mean_product  \n",
      "count               11316.000000  \n",
      "mean                    3.995122  \n",
      "std                     1.185562  \n",
      "min                     1.000000  \n",
      "5%                      1.000000  \n",
      "25%                     3.500000  \n",
      "50%                     4.333333  \n",
      "75%                     5.000000  \n",
      "95%                     5.000000  \n",
      "max                     5.000000  \n",
      "\n",
      "Skewness:\n",
      " customer_zip_code_prefix      0.811566\n",
      "geolocation_lat               1.660797\n",
      "geolocation_lng              -0.024661\n",
      "geo_points                    2.246842\n",
      "order_item_id                 5.189536\n",
      "price                         7.605577\n",
      "freight_value                 5.459946\n",
      "product_name_lenght          -0.929388\n",
      "product_description_lenght    1.993265\n",
      "product_photos_qty            1.846079\n",
      "product_weight_g              3.668109\n",
      "product_length_cm             1.824148\n",
      "product_height_cm             2.257122\n",
      "product_width_cm              1.699819\n",
      "seller_zip_code_prefix        1.542412\n",
      "payment_value_total           9.899611\n",
      "payment_installments_max      1.491603\n",
      "payment_count                17.649948\n",
      "review_count_product          3.782292\n",
      "review_score_mean_product    -1.309752\n",
      "dtype: float64\n",
      "\n",
      "Kurtosis:\n",
      " customer_zip_code_prefix      -0.727678\n",
      "geolocation_lat                3.144177\n",
      "geolocation_lng                2.168439\n",
      "geo_points                     6.815536\n",
      "order_item_id                 38.346561\n",
      "price                         98.317205\n",
      "freight_value                 52.506008\n",
      "product_name_lenght            0.243588\n",
      "product_description_lenght     4.887734\n",
      "product_photos_qty             4.303060\n",
      "product_weight_g              17.269872\n",
      "product_length_cm              4.064315\n",
      "product_height_cm              7.683492\n",
      "product_width_cm               4.228914\n",
      "seller_zip_code_prefix         0.901833\n",
      "payment_value_total          185.842563\n",
      "payment_installments_max       1.717324\n",
      "payment_count                455.503815\n",
      "review_count_product          16.040168\n",
      "review_score_mean_product      0.848219\n",
      "dtype: float64\n",
      "\n",
      "Outlier Counts (IQR method):\n",
      " customer_zip_code_prefix         0\n",
      "geolocation_lat               1786\n",
      "geolocation_lng                554\n",
      "geo_points                     775\n",
      "order_item_id                 1444\n",
      "price                          857\n",
      "freight_value                 1168\n",
      "product_name_lenght            124\n",
      "product_description_lenght     703\n",
      "product_photos_qty             269\n",
      "product_weight_g              1600\n",
      "product_length_cm              446\n",
      "product_height_cm              767\n",
      "product_width_cm               244\n",
      "seller_zip_code_prefix        1751\n",
      "payment_value_total            929\n",
      "payment_installments_max       830\n",
      "payment_count                  331\n",
      "review_count_product          1644\n",
      "review_score_mean_product      910\n",
      "dtype: int64\n",
      "\n",
      "Correlation Matrix:\n",
      "                             customer_zip_code_prefix  geolocation_lat  \\\n",
      "customer_zip_code_prefix                    1.000000         0.127568   \n",
      "geolocation_lat                             0.127568         1.000000   \n",
      "geolocation_lng                            -0.303235         0.440411   \n",
      "geo_points                                 -0.061367        -0.123759   \n",
      "order_item_id                               0.002355         0.003490   \n",
      "price                                       0.027093         0.056296   \n",
      "freight_value                               0.230137         0.267724   \n",
      "product_name_lenght                        -0.002643         0.010754   \n",
      "product_description_lenght                  0.024913         0.040220   \n",
      "product_photos_qty                          0.019439         0.035102   \n",
      "product_weight_g                           -0.001004         0.001493   \n",
      "product_length_cm                           0.010634         0.000578   \n",
      "product_height_cm                           0.010078        -0.002581   \n",
      "product_width_cm                           -0.028543        -0.029676   \n",
      "seller_zip_code_prefix                      0.070782        -0.032522   \n",
      "payment_value_total                         0.047757         0.085453   \n",
      "payment_installments_max                    0.065098         0.059068   \n",
      "payment_count                               0.001930         0.012950   \n",
      "review_count_product                        0.000585         0.016956   \n",
      "review_score_mean_product                  -0.029763        -0.036610   \n",
      "\n",
      "                            geolocation_lng  geo_points  order_item_id  \\\n",
      "customer_zip_code_prefix          -0.303235   -0.061367       0.002355   \n",
      "geolocation_lat                    0.440411   -0.123759       0.003490   \n",
      "geolocation_lng                    1.000000    0.067078      -0.010417   \n",
      "geo_points                         0.067078    1.000000       0.031513   \n",
      "order_item_id                     -0.010417    0.031513       1.000000   \n",
      "price                              0.023436    0.000657      -0.064469   \n",
      "freight_value                      0.070631   -0.031276      -0.002930   \n",
      "product_name_lenght                0.008685   -0.005950      -0.000208   \n",
      "product_description_lenght         0.019450    0.002488       0.008972   \n",
      "product_photos_qty                 0.004794   -0.018113      -0.041938   \n",
      "product_weight_g                  -0.007583   -0.012367       0.001829   \n",
      "product_length_cm                 -0.012891   -0.000781       0.068192   \n",
      "product_height_cm                 -0.008259   -0.005108       0.051762   \n",
      "product_width_cm                  -0.012075   -0.007496      -0.024374   \n",
      "seller_zip_code_prefix            -0.021762    0.048534      -0.005437   \n",
      "payment_value_total                0.036664    0.016667       0.257934   \n",
      "payment_installments_max           0.029819   -0.004632       0.070963   \n",
      "payment_count                      0.007939   -0.005720      -0.016215   \n",
      "review_count_product               0.016562   -0.020561      -0.012781   \n",
      "review_score_mean_product         -0.038726   -0.021148      -0.140012   \n",
      "\n",
      "                               price  freight_value  product_name_lenght  \\\n",
      "customer_zip_code_prefix    0.027093       0.230137            -0.002643   \n",
      "geolocation_lat             0.056296       0.267724             0.010754   \n",
      "geolocation_lng             0.023436       0.070631             0.008685   \n",
      "geo_points                  0.000657      -0.031276            -0.005950   \n",
      "order_item_id              -0.064469      -0.002930            -0.000208   \n",
      "price                       1.000000       0.397203             0.010826   \n",
      "freight_value               0.397203       1.000000             0.010814   \n",
      "product_name_lenght         0.010826       0.010814             1.000000   \n",
      "product_description_lenght  0.203839       0.095370             0.096362   \n",
      "product_photos_qty          0.045801       0.021218             0.139280   \n",
      "product_weight_g            0.357744       0.619779             0.013846   \n",
      "product_length_cm           0.154958       0.318175             0.042195   \n",
      "product_height_cm           0.233589       0.395066            -0.027161   \n",
      "product_width_cm            0.175308       0.322149             0.057028   \n",
      "seller_zip_code_prefix      0.073792       0.147544            -0.031519   \n",
      "payment_value_total         0.791285       0.387347             0.005388   \n",
      "payment_installments_max    0.298205       0.200667             0.030048   \n",
      "payment_count               0.011418       0.002480             0.012477   \n",
      "review_count_product       -0.048035      -0.016032             0.066203   \n",
      "review_score_mean_product  -0.013628      -0.050094            -0.015235   \n",
      "\n",
      "                            product_description_lenght  product_photos_qty  \\\n",
      "customer_zip_code_prefix                      0.024913            0.019439   \n",
      "geolocation_lat                               0.040220            0.035102   \n",
      "geolocation_lng                               0.019450            0.004794   \n",
      "geo_points                                    0.002488           -0.018113   \n",
      "order_item_id                                 0.008972           -0.041938   \n",
      "price                                         0.203839            0.045801   \n",
      "freight_value                                 0.095370            0.021218   \n",
      "product_name_lenght                           0.096362            0.139280   \n",
      "product_description_lenght                    1.000000            0.110989   \n",
      "product_photos_qty                            0.110989            1.000000   \n",
      "product_weight_g                              0.080193            0.022353   \n",
      "product_length_cm                             0.026101            0.038727   \n",
      "product_height_cm                             0.089622           -0.042768   \n",
      "product_width_cm                             -0.057006            0.014635   \n",
      "seller_zip_code_prefix                        0.057174           -0.047356   \n",
      "payment_value_total                           0.183002            0.017747   \n",
      "payment_installments_max                      0.068933            0.028265   \n",
      "payment_count                                 0.017202            0.009471   \n",
      "review_count_product                         -0.016824            0.001383   \n",
      "review_score_mean_product                    -0.000606            0.006793   \n",
      "\n",
      "                            product_weight_g  product_length_cm  \\\n",
      "customer_zip_code_prefix           -0.001004           0.010634   \n",
      "geolocation_lat                     0.001493           0.000578   \n",
      "geolocation_lng                    -0.007583          -0.012891   \n",
      "geo_points                         -0.012367          -0.000781   \n",
      "order_item_id                       0.001829           0.068192   \n",
      "price                               0.357744           0.154958   \n",
      "freight_value                       0.619779           0.318175   \n",
      "product_name_lenght                 0.013846           0.042195   \n",
      "product_description_lenght          0.080193           0.026101   \n",
      "product_photos_qty                  0.022353           0.038727   \n",
      "product_weight_g                    1.000000           0.457371   \n",
      "product_length_cm                   0.457371           1.000000   \n",
      "product_height_cm                   0.583286           0.201756   \n",
      "product_width_cm                    0.501546           0.511297   \n",
      "seller_zip_code_prefix              0.004458           0.017917   \n",
      "payment_value_total                 0.328457           0.200939   \n",
      "payment_installments_max            0.176960           0.147323   \n",
      "payment_count                       0.004179           0.013729   \n",
      "review_count_product                0.003909           0.018398   \n",
      "review_score_mean_product          -0.035127          -0.049406   \n",
      "\n",
      "                            product_height_cm  product_width_cm  \\\n",
      "customer_zip_code_prefix             0.010078         -0.028543   \n",
      "geolocation_lat                     -0.002581         -0.029676   \n",
      "geolocation_lng                     -0.008259         -0.012075   \n",
      "geo_points                          -0.005108         -0.007496   \n",
      "order_item_id                        0.051762         -0.024374   \n",
      "price                                0.233589          0.175308   \n",
      "freight_value                        0.395066          0.322149   \n",
      "product_name_lenght                 -0.027161          0.057028   \n",
      "product_description_lenght           0.089622         -0.057006   \n",
      "product_photos_qty                  -0.042768          0.014635   \n",
      "product_weight_g                     0.583286          0.501546   \n",
      "product_length_cm                    0.201756          0.511297   \n",
      "product_height_cm                    1.000000          0.279064   \n",
      "product_width_cm                     0.279064          1.000000   \n",
      "seller_zip_code_prefix               0.014638         -0.023015   \n",
      "payment_value_total                  0.237978          0.141443   \n",
      "payment_installments_max             0.123094          0.131071   \n",
      "payment_count                       -0.011133          0.029566   \n",
      "review_count_product                -0.030847          0.088317   \n",
      "review_score_mean_product           -0.035764         -0.001738   \n",
      "\n",
      "                            seller_zip_code_prefix  payment_value_total  \\\n",
      "customer_zip_code_prefix                  0.070782             0.047757   \n",
      "geolocation_lat                          -0.032522             0.085453   \n",
      "geolocation_lng                          -0.021762             0.036664   \n",
      "geo_points                                0.048534             0.016667   \n",
      "order_item_id                            -0.005437             0.257934   \n",
      "price                                     0.073792             0.791285   \n",
      "freight_value                             0.147544             0.387347   \n",
      "product_name_lenght                      -0.031519             0.005388   \n",
      "product_description_lenght                0.057174             0.183002   \n",
      "product_photos_qty                       -0.047356             0.017747   \n",
      "product_weight_g                          0.004458             0.328457   \n",
      "product_length_cm                         0.017917             0.200939   \n",
      "product_height_cm                         0.014638             0.237978   \n",
      "product_width_cm                         -0.023015             0.141443   \n",
      "seller_zip_code_prefix                    1.000000             0.069138   \n",
      "payment_value_total                       0.069138             1.000000   \n",
      "payment_installments_max                  0.057798             0.277456   \n",
      "payment_count                            -0.008886             0.005832   \n",
      "review_count_product                     -0.052583            -0.040692   \n",
      "review_score_mean_product                 0.014069            -0.095713   \n",
      "\n",
      "                            payment_installments_max  payment_count  \\\n",
      "customer_zip_code_prefix                    0.065098       0.001930   \n",
      "geolocation_lat                             0.059068       0.012950   \n",
      "geolocation_lng                             0.029819       0.007939   \n",
      "geo_points                                 -0.004632      -0.005720   \n",
      "order_item_id                               0.070963      -0.016215   \n",
      "price                                       0.298205       0.011418   \n",
      "freight_value                               0.200667       0.002480   \n",
      "product_name_lenght                         0.030048       0.012477   \n",
      "product_description_lenght                  0.068933       0.017202   \n",
      "product_photos_qty                          0.028265       0.009471   \n",
      "product_weight_g                            0.176960       0.004179   \n",
      "product_length_cm                           0.147323       0.013729   \n",
      "product_height_cm                           0.123094      -0.011133   \n",
      "product_width_cm                            0.131071       0.029566   \n",
      "seller_zip_code_prefix                      0.057798      -0.008886   \n",
      "payment_value_total                         0.277456       0.005832   \n",
      "payment_installments_max                    1.000000      -0.042488   \n",
      "payment_count                              -0.042488       1.000000   \n",
      "review_count_product                       -0.022269       0.011722   \n",
      "review_score_mean_product                  -0.066786      -0.001229   \n",
      "\n",
      "                            review_count_product  review_score_mean_product  \n",
      "customer_zip_code_prefix                0.000585                  -0.029763  \n",
      "geolocation_lat                         0.016956                  -0.036610  \n",
      "geolocation_lng                         0.016562                  -0.038726  \n",
      "geo_points                             -0.020561                  -0.021148  \n",
      "order_item_id                          -0.012781                  -0.140012  \n",
      "price                                  -0.048035                  -0.013628  \n",
      "freight_value                          -0.016032                  -0.050094  \n",
      "product_name_lenght                     0.066203                  -0.015235  \n",
      "product_description_lenght             -0.016824                  -0.000606  \n",
      "product_photos_qty                      0.001383                   0.006793  \n",
      "product_weight_g                        0.003909                  -0.035127  \n",
      "product_length_cm                       0.018398                  -0.049406  \n",
      "product_height_cm                      -0.030847                  -0.035764  \n",
      "product_width_cm                        0.088317                  -0.001738  \n",
      "seller_zip_code_prefix                 -0.052583                   0.014069  \n",
      "payment_value_total                    -0.040692                  -0.095713  \n",
      "payment_installments_max               -0.022269                  -0.066786  \n",
      "payment_count                           0.011722                  -0.001229  \n",
      "review_count_product                    1.000000                   0.018501  \n",
      "review_score_mean_product               0.018501                   1.000000  \n",
      "\n",
      "=== Categorical Columns Analysis ===\n",
      "\n",
      "Value Counts for 'order_id':\n",
      " order_id\n",
      "71dab1155600756af6de79de92e712e3    11\n",
      "6c355e2913545fa6f72c40cbca57729e    11\n",
      "c52c7fbe316b5b9d549e8a25206b8a1f     9\n",
      "ad850e69fce9a512ada84086651a2e7d     7\n",
      "df36d28de846593cdbc153e28f195cd5     7\n",
      "                                    ..\n",
      "e79f873315375aad54aa74ee8119dc40     1\n",
      "a8b7b51948bf9deabad95734d8470c5f     1\n",
      "1f18ee0a409056c8fb31ec0e16fbeb40     1\n",
      "a8bf3c34d62fdbb261cda157deee9b0f     1\n",
      "c81f74e50f0496fa39716cc77cacd460     1\n",
      "Name: count, Length: 10000, dtype: int64\n",
      "Unique Values in 'order_id': 10000\n",
      "Most Common Value in 'order_id': 6c355e2913545fa6f72c40cbca57729e\n",
      "\n",
      "Value Counts for 'customer_id':\n",
      " customer_id\n",
      "8c20d9bfbc96c5d39025d77a3ba83d7f    11\n",
      "d95ca02ab50105ccce682bdf9ffdc3b4    11\n",
      "88324c93ce11436ae046563bf0da285c     9\n",
      "30bb84b541c96af98ba7d90b9ebf35d0     7\n",
      "2f6d72365267ce6048d9bbc1a9bd977e     7\n",
      "                                    ..\n",
      "bb4a5a4ffe14ca07ec7a100091914d93     1\n",
      "b933dffd7bce914ecf44a45630277fdb     1\n",
      "22acdc5a401b5ccb535fb27cfa9cfffe     1\n",
      "b167b852c85288c36b6dab4a96635f59     1\n",
      "7b270ebc87c25c8404348c10ff80a80e     1\n",
      "Name: count, Length: 10000, dtype: int64\n",
      "Unique Values in 'customer_id': 10000\n",
      "Most Common Value in 'customer_id': 8c20d9bfbc96c5d39025d77a3ba83d7f\n",
      "\n",
      "Value Counts for 'order_status':\n",
      " order_status\n",
      "delivered      11102\n",
      "shipped          120\n",
      "canceled          80\n",
      "unavailable       62\n",
      "processing        40\n",
      "invoiced          39\n",
      "created            1\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'order_status': 7\n",
      "Most Common Value in 'order_status': delivered\n",
      "\n",
      "Value Counts for 'order_purchase_timestamp':\n",
      " order_purchase_timestamp\n",
      "2018-02-01 18:32:02    11\n",
      "2018-07-25 14:47:45    11\n",
      "2018-03-19 18:58:49     9\n",
      "2017-11-21 17:23:04     7\n",
      "2018-04-19 20:53:13     7\n",
      "                       ..\n",
      "2018-07-17 21:09:56     1\n",
      "2018-07-16 11:04:37     1\n",
      "2018-01-03 14:32:04     1\n",
      "2018-02-09 13:47:08     1\n",
      "2018-03-04 22:48:38     1\n",
      "Name: count, Length: 9997, dtype: int64\n",
      "Unique Values in 'order_purchase_timestamp': 9997\n",
      "Most Common Value in 'order_purchase_timestamp': 2018-02-01 18:32:02\n",
      "\n",
      "Value Counts for 'order_approved_at':\n",
      " order_approved_at\n",
      "2018-07-27 04:05:19    11\n",
      "2018-02-01 19:37:42    11\n",
      "2018-03-21 15:15:35     9\n",
      "2017-11-23 02:36:26     7\n",
      "2018-03-01 02:50:50     7\n",
      "                       ..\n",
      "2017-08-16 00:50:19     1\n",
      "2018-05-31 16:53:21     1\n",
      "2017-11-17 13:55:23     1\n",
      "2018-07-01 11:15:17     1\n",
      "2018-03-04 23:09:22     1\n",
      "Name: count, Length: 9892, dtype: int64\n",
      "Unique Values in 'order_approved_at': 9892\n",
      "Most Common Value in 'order_approved_at': 2018-02-01 19:37:42\n",
      "\n",
      "Value Counts for 'order_delivered_carrier_date':\n",
      " order_delivered_carrier_date\n",
      "2018-02-19 18:37:35    11\n",
      "2018-08-01 12:36:00    11\n",
      "2018-03-22 00:11:41     9\n",
      "2018-08-02 12:53:00     8\n",
      "2017-11-23 20:38:26     7\n",
      "                       ..\n",
      "2018-07-18 15:20:00     1\n",
      "2017-02-13 08:58:08     1\n",
      "2018-07-03 11:55:00     1\n",
      "2018-07-20 13:55:00     1\n",
      "2018-03-05 16:15:04     1\n",
      "Name: count, Length: 9525, dtype: int64\n",
      "Unique Values in 'order_delivered_carrier_date': 9525\n",
      "Most Common Value in 'order_delivered_carrier_date': 2018-02-19 18:37:35\n",
      "\n",
      "Value Counts for 'order_delivered_customer_date':\n",
      " order_delivered_customer_date\n",
      "2018-03-12 18:58:21    11\n",
      "2018-08-11 10:46:38    11\n",
      "2018-03-28 17:45:55     9\n",
      "2018-04-24 16:08:36     7\n",
      "2017-12-07 14:47:05     7\n",
      "                       ..\n",
      "2018-02-28 23:19:54     1\n",
      "2018-04-20 18:06:53     1\n",
      "2017-02-02 10:24:01     1\n",
      "2018-08-08 18:42:46     1\n",
      "2018-03-13 18:54:32     1\n",
      "Name: count, Length: 9678, dtype: int64\n",
      "Unique Values in 'order_delivered_customer_date': 9678\n",
      "Most Common Value in 'order_delivered_customer_date': 2018-03-12 18:58:21\n",
      "\n",
      "Value Counts for 'order_estimated_delivery_date':\n",
      " order_estimated_delivery_date\n",
      "2018-07-11    67\n",
      "2018-08-16    66\n",
      "2017-12-20    64\n",
      "2017-12-19    63\n",
      "2018-03-01    62\n",
      "              ..\n",
      "2016-12-16     1\n",
      "2018-02-13     1\n",
      "2017-02-17     1\n",
      "2016-10-28     1\n",
      "2017-02-01     1\n",
      "Name: count, Length: 421, dtype: int64\n",
      "Unique Values in 'order_estimated_delivery_date': 421\n",
      "Most Common Value in 'order_estimated_delivery_date': 2018-07-11 00:00:00\n",
      "\n",
      "Value Counts for 'customer_unique_id':\n",
      " customer_unique_id\n",
      "eae0a83d752b1dd32697e0e7b4221656    11\n",
      "a5c6335399140e986db84120c425adf0    11\n",
      "6411590d91c48640cb07e72fbb4a359e     9\n",
      "4e807d3a4553f2ba95698b27217b5352     7\n",
      "33de26d1fafbfd4945eb586f7136efe6     7\n",
      "                                    ..\n",
      "cdd84ebb59cbe647a3bbafabdec45d15     1\n",
      "e47f9a5d0e43e73cdbef0d5663dcac7a     1\n",
      "05395d5194f09d9fbd21d2215faaef08     1\n",
      "dd473d247facd88bb1d5a4816b99c7b1     1\n",
      "324a22205906aa2612a33e63c00ca8bb     1\n",
      "Name: count, Length: 9969, dtype: int64\n",
      "Unique Values in 'customer_unique_id': 9969\n",
      "Most Common Value in 'customer_unique_id': a5c6335399140e986db84120c425adf0\n",
      "\n",
      "Value Counts for 'customer_city':\n",
      " customer_city\n",
      "sao paulo          1808\n",
      "rio de janeiro      798\n",
      "belo horizonte      326\n",
      "brasilia            260\n",
      "curitiba            191\n",
      "                   ... \n",
      "miranorte             1\n",
      "brodowski             1\n",
      "jesupolis             1\n",
      "barra do garcas       1\n",
      "teolandia             1\n",
      "Name: count, Length: 1621, dtype: int64\n",
      "Unique Values in 'customer_city': 1621\n",
      "Most Common Value in 'customer_city': sao paulo\n",
      "\n",
      "Value Counts for 'customer_state':\n",
      " customer_state\n",
      "SP    4872\n",
      "RJ    1494\n",
      "MG    1374\n",
      "RS     588\n",
      "PR     575\n",
      "SC     415\n",
      "BA     376\n",
      "DF     261\n",
      "ES     231\n",
      "GO     222\n",
      "PE     183\n",
      "CE     146\n",
      "MT     111\n",
      "MS      90\n",
      "MA      87\n",
      "PA      84\n",
      "PB      71\n",
      "RN      58\n",
      "AL      42\n",
      "PI      41\n",
      "RO      30\n",
      "TO      26\n",
      "SE      26\n",
      "AM      18\n",
      "AC      10\n",
      "AP       9\n",
      "RR       4\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'customer_state': 27\n",
      "Most Common Value in 'customer_state': SP\n",
      "\n",
      "Value Counts for 'product_id':\n",
      " product_id\n",
      "389d119b48cf3043d311335e499d9c6b    53\n",
      "99a4788cb24856965c36a24e339b6058    48\n",
      "aca2eb7d00ea1a7b8ebd4e68314663af    48\n",
      "d1c427060a0f73f6b889a5c7c61f2ac4    44\n",
      "422879e10f46682990de24d770e7f83d    43\n",
      "                                    ..\n",
      "baded889837476e342ac1e93c87ce383     1\n",
      "ca83ea24493d1cdaa236098ae779b175     1\n",
      "4a96de21ccfb22ebe1c56992acbbd7fc     1\n",
      "95ed9ac528aa2547407e6dff356c12ba     1\n",
      "cb92637d91f8267856d2143b6fc7bd9e     1\n",
      "Name: count, Length: 6758, dtype: int64\n",
      "Unique Values in 'product_id': 6758\n",
      "Most Common Value in 'product_id': 389d119b48cf3043d311335e499d9c6b\n",
      "\n",
      "Value Counts for 'seller_id':\n",
      " seller_id\n",
      "4a3ca9315b744ce9f8e9374361493884    205\n",
      "1f50f920176fa81dab994f9023523100    180\n",
      "6560211a19b47992c3666cc44a7e94c0    176\n",
      "7c67e1448b00f6e969d365cea6b010ab    146\n",
      "da8622b14eb17ae2831f4ac5b9dab84a    145\n",
      "                                   ... \n",
      "15fa791d5e017f66402dc28c44480657      1\n",
      "3492e68f37fd1df87f4f2f2ea247f445      1\n",
      "f410c8873029fcc3809b9df6d0b28914      1\n",
      "b3e2faf2c1004d674f16a3aa3197f0e1      1\n",
      "985c92cc412091f8529c12d7f23d3fef      1\n",
      "Name: count, Length: 1630, dtype: int64\n",
      "Unique Values in 'seller_id': 1630\n",
      "Most Common Value in 'seller_id': 4a3ca9315b744ce9f8e9374361493884\n",
      "\n",
      "Value Counts for 'shipping_limit_date':\n",
      " shipping_limit_date\n",
      "2018-02-15 19:11:08    11\n",
      "2018-08-08 04:05:19    11\n",
      "2018-03-27 15:15:35     9\n",
      "2018-04-25 22:11:43     7\n",
      "2017-11-29 02:36:26     7\n",
      "                       ..\n",
      "2018-07-04 13:15:13     1\n",
      "2017-02-16 21:48:59     1\n",
      "2018-07-23 21:21:40     1\n",
      "2018-07-23 04:31:23     1\n",
      "2018-03-08 23:09:22     1\n",
      "Name: count, Length: 9904, dtype: int64\n",
      "Unique Values in 'shipping_limit_date': 9904\n",
      "Most Common Value in 'shipping_limit_date': 2018-02-15 19:11:08\n",
      "\n",
      "Value Counts for 'product_category_name':\n",
      " product_category_name\n",
      "cama_mesa_banho             1105\n",
      "beleza_saude                 940\n",
      "esporte_lazer                861\n",
      "moveis_decoracao             840\n",
      "informatica_acessorios       808\n",
      "                            ... \n",
      "artigos_de_festas              3\n",
      "flores                         2\n",
      "la_cuisine                     1\n",
      "artes_e_artesanato             1\n",
      "tablets_impressao_imagem       1\n",
      "Name: count, Length: 69, dtype: int64\n",
      "Unique Values in 'product_category_name': 69\n",
      "Most Common Value in 'product_category_name': cama_mesa_banho\n",
      "\n",
      "Value Counts for 'product_category_name_english':\n",
      " product_category_name_english\n",
      "bed_bath_table            1105\n",
      "health_beauty              940\n",
      "sports_leisure             861\n",
      "furniture_decor            840\n",
      "computers_accessories      808\n",
      "                          ... \n",
      "home_comfort_2               3\n",
      "flowers                      2\n",
      "la_cuisine                   1\n",
      "arts_and_craftmanship        1\n",
      "tablets_printing_image       1\n",
      "Name: count, Length: 68, dtype: int64\n",
      "Unique Values in 'product_category_name_english': 68\n",
      "Most Common Value in 'product_category_name_english': bed_bath_table\n",
      "\n",
      "Value Counts for 'seller_city':\n",
      " seller_city\n",
      "sao paulo               2819\n",
      "ibitinga                 811\n",
      "curitiba                 325\n",
      "belo horizonte           288\n",
      "santo andre              258\n",
      "                        ... \n",
      "sao bernardo do capo       1\n",
      "paincandu                  1\n",
      "presidente epitacio        1\n",
      "luziania                   1\n",
      "sao leopoldo               1\n",
      "Name: count, Length: 407, dtype: int64\n",
      "Unique Values in 'seller_city': 407\n",
      "Most Common Value in 'seller_city': sao paulo\n",
      "\n",
      "Value Counts for 'seller_state':\n",
      " seller_state\n",
      "SP    8018\n",
      "MG     942\n",
      "PR     893\n",
      "RJ     521\n",
      "SC     403\n",
      "RS     223\n",
      "DF      91\n",
      "BA      63\n",
      "GO      58\n",
      "PE      49\n",
      "ES      35\n",
      "MA      34\n",
      "MT      16\n",
      "CE       5\n",
      "RN       4\n",
      "PB       4\n",
      "SE       2\n",
      "PI       2\n",
      "MS       1\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'seller_state': 19\n",
      "Most Common Value in 'seller_state': SP\n",
      "\n",
      "=== End of EDA Report ===\n",
      "END OF ROBUST EDA ON OLIST\n",
      "\n",
      "ROBUST EDA ON SALES\n",
      "=== Robust EDA Report ===\n",
      "\n",
      "DataFrame Shape: (10000, 16)\n",
      "\n",
      "Data Types:\n",
      " Rank               int64\n",
      "Name              object\n",
      "Genre             object\n",
      "ESRB_Rating       object\n",
      "Platform          object\n",
      "Publisher         object\n",
      "Developer         object\n",
      "Critic_Score     float64\n",
      "User_Score       float64\n",
      "Total_Shipped    float64\n",
      "Global_Sales     float64\n",
      "NA_Sales         float64\n",
      "PAL_Sales        float64\n",
      "JP_Sales         float64\n",
      "Other_Sales      float64\n",
      "Year             float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      " Rank                0\n",
      "Name                0\n",
      "Genre               0\n",
      "ESRB_Rating      5805\n",
      "Platform            0\n",
      "Publisher           0\n",
      "Developer           6\n",
      "Critic_Score     8868\n",
      "User_Score       9951\n",
      "Total_Shipped    9687\n",
      "Global_Sales     6584\n",
      "NA_Sales         7681\n",
      "PAL_Sales        7651\n",
      "JP_Sales         8797\n",
      "Other_Sales      7235\n",
      "Year              184\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Head of DataFrame:\n",
      "     Rank                                Name         Genre ESRB_Rating  \\\n",
      "5      6  Pokemon Red / Green / Blue Version  Role-Playing           E   \n",
      "9     10                           Minecraft          Misc         NaN   \n",
      "10    11                           Duck Hunt       Shooter         NaN   \n",
      "27    28                 Super Mario Bros. 3      Platform           E   \n",
      "30    31         Grand Theft Auto: Vice City        Action           M   \n",
      "\n",
      "   Platform       Publisher       Developer  Critic_Score  User_Score  \\\n",
      "5        GB        Nintendo      Game Freak      9.400000         NaN   \n",
      "9        PC          Mojang       Mojang AB     10.000000         NaN   \n",
      "10      NES        Nintendo   Nintendo R&D1           NaN         NaN   \n",
      "27      NES        Nintendo   Nintendo R&D2           NaN         NaN   \n",
      "30      PS2  Rockstar Games  Rockstar North      9.600000         NaN   \n",
      "\n",
      "    Total_Shipped  Global_Sales  NA_Sales  PAL_Sales  JP_Sales  Other_Sales  \\\n",
      "5       31.380000           NaN       NaN        NaN       NaN          NaN   \n",
      "9       30.010000           NaN       NaN        NaN       NaN          NaN   \n",
      "10      28.310000           NaN       NaN        NaN       NaN          NaN   \n",
      "27      17.280000           NaN       NaN        NaN       NaN          NaN   \n",
      "30            NaN     16.150000  8.410000   5.490000  0.470000     1.780000   \n",
      "\n",
      "          Year  \n",
      "5  1998.000000  \n",
      "9  2010.000000  \n",
      "10 1985.000000  \n",
      "27 1990.000000  \n",
      "30 2002.000000  \n",
      "\n",
      "=== Numerical Columns Analysis ===\n",
      "\n",
      "Summary Statistics (with 5%, 25%, 50%, 75%, 95% percentiles):\n",
      "               Rank  Critic_Score  User_Score  Total_Shipped  Global_Sales  \\\n",
      "count 10000.000000   1132.000000   49.000000     313.000000   3416.000000   \n",
      "mean  28282.991200      7.166254    8.318367       1.727827      0.354359   \n",
      "std   16060.599677      1.462889    1.291910       3.602234      0.848933   \n",
      "min       6.000000      2.000000    2.000000       0.080000      0.000000   \n",
      "5%     3029.850000      4.300000    6.660000       0.110000      0.000000   \n",
      "25%   14369.750000      6.400000    8.000000       0.220000      0.030000   \n",
      "50%   28577.500000      7.500000    8.500000       0.590000      0.110000   \n",
      "75%   42212.000000      8.200000    9.200000       1.760000      0.340000   \n",
      "95%   53129.150000      9.100000    9.660000       6.708000      1.390000   \n",
      "max   55783.000000     10.000000   10.000000      31.380000     16.150000   \n",
      "\n",
      "         NA_Sales   PAL_Sales    JP_Sales  Other_Sales        Year  \n",
      "count 2319.000000 2349.000000 1203.000000  2765.000000 9816.000000  \n",
      "mean     0.261328    0.149515    0.110889     0.043089 2005.634780  \n",
      "std      0.512168    0.395121    0.189141     0.122784    8.412298  \n",
      "min      0.000000    0.000000    0.000000     0.000000 1970.000000  \n",
      "5%       0.010000    0.000000    0.000000     0.000000 1991.000000  \n",
      "25%      0.050000    0.010000    0.020000     0.000000 2000.000000  \n",
      "50%      0.110000    0.040000    0.040000     0.010000 2008.000000  \n",
      "75%      0.270000    0.130000    0.130000     0.030000 2011.000000  \n",
      "95%      1.000000    0.610000    0.438000     0.188000 2017.000000  \n",
      "max      9.060000    6.210000    2.170000     2.260000 2020.000000  \n",
      "\n",
      "Skewness:\n",
      " Rank            -0.029666\n",
      "Critic_Score    -0.846155\n",
      "User_Score      -2.620438\n",
      "Total_Shipped    5.448402\n",
      "Global_Sales     9.062350\n",
      "NA_Sales         7.536832\n",
      "PAL_Sales        7.902172\n",
      "JP_Sales         4.911197\n",
      "Other_Sales      8.054551\n",
      "Year            -0.754684\n",
      "dtype: float64\n",
      "\n",
      "Kurtosis:\n",
      " Rank             -1.203515\n",
      "Critic_Score      0.443551\n",
      "User_Score       11.100775\n",
      "Total_Shipped    36.652629\n",
      "Global_Sales    126.844127\n",
      "NA_Sales         91.867361\n",
      "PAL_Sales        87.505177\n",
      "JP_Sales         37.500071\n",
      "Other_Sales      91.500076\n",
      "Year              0.362760\n",
      "dtype: float64\n",
      "\n",
      "Outlier Counts (IQR method):\n",
      " Rank               0\n",
      "Critic_Score      29\n",
      "User_Score         2\n",
      "Total_Shipped     26\n",
      "Global_Sales     349\n",
      "NA_Sales         218\n",
      "PAL_Sales        270\n",
      "JP_Sales         113\n",
      "Other_Sales      342\n",
      "Year             142\n",
      "dtype: int64\n",
      "\n",
      "Correlation Matrix:\n",
      "                    Rank  Critic_Score  User_Score  Total_Shipped  \\\n",
      "Rank           1.000000     -0.156397   -0.278770      -0.466286   \n",
      "Critic_Score  -0.156397      1.000000    0.673779       0.364024   \n",
      "User_Score    -0.278770      0.673779    1.000000       0.188085   \n",
      "Total_Shipped -0.466286      0.364024    0.188085       1.000000   \n",
      "Global_Sales  -0.537923      0.312280    0.395085            NaN   \n",
      "NA_Sales      -0.536338      0.335187    0.436740            NaN   \n",
      "PAL_Sales     -0.440430      0.273022    0.486396            NaN   \n",
      "JP_Sales      -0.441845      0.206843    0.174519            NaN   \n",
      "Other_Sales   -0.447695      0.285556    0.458343            NaN   \n",
      "Year          -0.107046      0.004554   -0.280859      -0.285192   \n",
      "\n",
      "               Global_Sales  NA_Sales  PAL_Sales  JP_Sales  Other_Sales  \\\n",
      "Rank              -0.537923 -0.536338  -0.440430 -0.441845    -0.447695   \n",
      "Critic_Score       0.312280  0.335187   0.273022  0.206843     0.285556   \n",
      "User_Score         0.395085  0.436740   0.486396  0.174519     0.458343   \n",
      "Total_Shipped           NaN       NaN        NaN       NaN          NaN   \n",
      "Global_Sales       1.000000  0.936823   0.930517  0.211327     0.903602   \n",
      "NA_Sales           0.936823  1.000000   0.766901  0.062966     0.778130   \n",
      "PAL_Sales          0.930517  0.766901   1.000000  0.097555     0.882607   \n",
      "JP_Sales           0.211327  0.062966   0.097555  1.000000     0.058907   \n",
      "Other_Sales        0.903602  0.778130   0.882607  0.058907     1.000000   \n",
      "Year              -0.038429 -0.037921   0.078028 -0.374985     0.079306   \n",
      "\n",
      "                   Year  \n",
      "Rank          -0.107046  \n",
      "Critic_Score   0.004554  \n",
      "User_Score    -0.280859  \n",
      "Total_Shipped -0.285192  \n",
      "Global_Sales  -0.038429  \n",
      "NA_Sales      -0.037921  \n",
      "PAL_Sales      0.078028  \n",
      "JP_Sales      -0.374985  \n",
      "Other_Sales    0.079306  \n",
      "Year           1.000000  \n",
      "\n",
      "=== Categorical Columns Analysis ===\n",
      "\n",
      "Value Counts for 'Name':\n",
      " Name\n",
      "FIFA 14                                                                               5\n",
      "Samurai Shodown                                                                       5\n",
      "LEGO The Hobbit                                                                       5\n",
      "Pipe Mania                                                                            5\n",
      "Ben 10 Ultimate Alien: Cosmic Destruction                                             4\n",
      "                                                                                     ..\n",
      "Pacific Fighters                                                                      1\n",
      "Divine Divinity                                                                       1\n",
      "The Elder Scrolls IV: Oblivion - Game of the Year Edition                             1\n",
      "Tago Akira no Atama no Taisou Dai-3-Shuu: Fushigi no Kuni no Nazotoki Otogibanashi    1\n",
      "Ion Maiden                                                                            1\n",
      "Name: count, Length: 9052, dtype: int64\n",
      "Unique Values in 'Name': 9052\n",
      "Most Common Value in 'Name': FIFA 14\n",
      "\n",
      "Value Counts for 'Genre':\n",
      " Genre\n",
      "Misc                1698\n",
      "Action              1374\n",
      "Adventure            949\n",
      "Sports               940\n",
      "Shooter              822\n",
      "Role-Playing         816\n",
      "Platform             617\n",
      "Strategy             585\n",
      "Puzzle               567\n",
      "Racing               543\n",
      "Simulation           491\n",
      "Fighting             374\n",
      "Action-Adventure     109\n",
      "Visual Novel          47\n",
      "Music                 35\n",
      "MMO                   13\n",
      "Party                 13\n",
      "Board Game             3\n",
      "Sandbox                2\n",
      "Education              2\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'Genre': 20\n",
      "Most Common Value in 'Genre': Misc\n",
      "\n",
      "Value Counts for 'ESRB_Rating':\n",
      " ESRB_Rating\n",
      "E      1952\n",
      "T      1106\n",
      "M       539\n",
      "E10     521\n",
      "RP       66\n",
      "EC        9\n",
      "AO        1\n",
      "KA        1\n",
      "Name: count, dtype: int64\n",
      "Unique Values in 'ESRB_Rating': 8\n",
      "Most Common Value in 'ESRB_Rating': E\n",
      "\n",
      "Value Counts for 'Platform':\n",
      " Platform\n",
      "PC      2001\n",
      "PS2      634\n",
      "DS       575\n",
      "PS       467\n",
      "XBL      399\n",
      "        ... \n",
      "iQue       2\n",
      "ZXS        2\n",
      "CDi        2\n",
      "AST        1\n",
      "ACPC       1\n",
      "Name: count, Length: 64, dtype: int64\n",
      "Unique Values in 'Platform': 64\n",
      "Most Common Value in 'Platform': PC\n",
      "\n",
      "Value Counts for 'Publisher':\n",
      " Publisher\n",
      "Unknown                        873\n",
      "Sega                           376\n",
      "Konami                         287\n",
      "Ubisoft                        276\n",
      "Activision                     269\n",
      "                              ... \n",
      "Winter Wolves Game Studio        1\n",
      "Smosh LLC                        1\n",
      "Perfect World Entertainment      1\n",
      "Endgame Studios                  1\n",
      "3D Realms                        1\n",
      "Name: count, Length: 1431, dtype: int64\n",
      "Unique Values in 'Publisher': 1431\n",
      "Most Common Value in 'Publisher': Unknown\n",
      "\n",
      "Value Counts for 'Developer':\n",
      " Developer\n",
      "Unknown                           841\n",
      "Konami                            177\n",
      "Sega                              163\n",
      "Capcom                            108\n",
      "Hudson Soft                        89\n",
      "                                 ... \n",
      "Caravel Games                       1\n",
      "The Unallied                        1\n",
      "Celsius Game Studios                1\n",
      "Cryo Interactive Entertainment      1\n",
      "Voidpoint                           1\n",
      "Name: count, Length: 3276, dtype: int64\n",
      "Unique Values in 'Developer': 3276\n",
      "Most Common Value in 'Developer': Unknown\n",
      "\n",
      "=== End of EDA Report ===\n",
      "END OF ROBUST EDA ON SALES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ROBUST EDA ON STEAM\")\n",
    "robust_eda(steam)\n",
    "print(\"END OF ROBUST EDA ON STEAM\\n\")\n",
    "print(\"ROBUST EDA ON OLIST\")\n",
    "robust_eda(olist)\n",
    "print(\"END OF ROBUST EDA ON OLIST\\n\")\n",
    "print(\"ROBUST EDA ON SALES\")\n",
    "robust_eda(sales)\n",
    "print(\"END OF ROBUST EDA ON SALES\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3177e9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e30164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEAM Dataset ===\n",
      "Removed 1 constant feature(s).\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=2 | F1_macro=0.668788 ± 0.016833\n",
      "[2/40] GBT | k=5 | F1_macro=0.668788 ± 0.016833\n",
      "[3/40] GBT | k=9 | F1_macro=0.668788 ± 0.016833\n",
      "[4/40] GBT | k=14 | F1_macro=0.668788 ± 0.016833\n",
      "[5/40] GBT | k=18 | F1_macro=0.668788 ± 0.016833\n",
      "[6/40] RandomForest | k=2 | F1_macro=0.674434 ± 0.019483\n",
      "[7/40] RandomForest | k=5 | F1_macro=0.674434 ± 0.019483\n",
      "[8/40] RandomForest | k=9 | F1_macro=0.674434 ± 0.019483\n",
      "[9/40] RandomForest | k=14 | F1_macro=0.674434 ± 0.019483\n",
      "[10/40] RandomForest | k=18 | F1_macro=0.674434 ± 0.019483\n",
      "[11/40] DecisionTree | k=2 | F1_macro=0.663213 ± 0.015647\n",
      "[12/40] DecisionTree | k=5 | F1_macro=0.663213 ± 0.015647\n",
      "[13/40] DecisionTree | k=9 | F1_macro=0.663213 ± 0.015647\n",
      "[14/40] DecisionTree | k=14 | F1_macro=0.663213 ± 0.015647\n",
      "[15/40] DecisionTree | k=18 | F1_macro=0.663213 ± 0.015647\n",
      "[16/40] LogisticRegression | k=2 | F1_macro=0.456025 ± 0.019314\n",
      "[17/40] LogisticRegression | k=5 | F1_macro=0.526499 ± 0.030309\n",
      "[18/40] LogisticRegression | k=9 | F1_macro=0.556833 ± 0.043576\n",
      "[19/40] LogisticRegression | k=14 | F1_macro=0.575601 ± 0.059368\n",
      "[20/40] LogisticRegression | k=18 | F1_macro=0.571865 ± 0.057659\n",
      "[21/40] LinearSVM | k=2 | F1_macro=0.456025 ± 0.019314\n",
      "[22/40] LinearSVM | k=5 | F1_macro=0.525233 ± 0.027148\n",
      "[23/40] LinearSVM | k=9 | F1_macro=0.555144 ± 0.040455\n",
      "[24/40] LinearSVM | k=14 | F1_macro=0.575724 ± 0.060321\n",
      "[25/40] LinearSVM | k=18 | F1_macro=0.573391 ± 0.060093\n",
      "[26/40] NaiveBayes | k=2 | F1_macro=0.570126 ± 0.068597\n",
      "[27/40] NaiveBayes | k=5 | F1_macro=0.515143 ± 0.020860\n",
      "[28/40] NaiveBayes | k=9 | F1_macro=0.530428 ± 0.009397\n",
      "[29/40] NaiveBayes | k=14 | F1_macro=0.537361 ± 0.007718\n",
      "[30/40] NaiveBayes | k=18 | F1_macro=0.150417 ± 0.015293\n",
      "[31/40] KNN | k=2 | F1_macro=0.570985 ± 0.080979\n",
      "[32/40] KNN | k=5 | F1_macro=0.587730 ± 0.034863\n",
      "[33/40] KNN | k=9 | F1_macro=0.573615 ± 0.026770\n",
      "[34/40] KNN | k=14 | F1_macro=0.590550 ± 0.045696\n",
      "[35/40] KNN | k=18 | F1_macro=0.598847 ± 0.040634\n",
      "[36/40] Dummy | k=2 | F1_macro=0.149977 ± 0.015488\n",
      "[37/40] Dummy | k=5 | F1_macro=0.149977 ± 0.015488\n",
      "[38/40] Dummy | k=9 | F1_macro=0.149977 ± 0.015488\n",
      "[39/40] Dummy | k=14 | F1_macro=0.149977 ± 0.015488\n",
      "[40/40] Dummy | k=18 | F1_macro=0.149977 ± 0.015488\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "                 Model  K_features  MeanScore   StdDev    Metric\n",
      "0         RandomForest           2   0.674434 0.019483  F1_macro\n",
      "1         RandomForest           5   0.674434 0.019483  F1_macro\n",
      "2         RandomForest           9   0.674434 0.019483  F1_macro\n",
      "3         RandomForest          14   0.674434 0.019483  F1_macro\n",
      "4         RandomForest          18   0.674434 0.019483  F1_macro\n",
      "5                  GBT           2   0.668788 0.016833  F1_macro\n",
      "6                  GBT           5   0.668788 0.016833  F1_macro\n",
      "7                  GBT           9   0.668788 0.016833  F1_macro\n",
      "8                  GBT          14   0.668788 0.016833  F1_macro\n",
      "9                  GBT          18   0.668788 0.016833  F1_macro\n",
      "10        DecisionTree           2   0.663213 0.015647  F1_macro\n",
      "11        DecisionTree           5   0.663213 0.015647  F1_macro\n",
      "12        DecisionTree           9   0.663213 0.015647  F1_macro\n",
      "13        DecisionTree          14   0.663213 0.015647  F1_macro\n",
      "14        DecisionTree          18   0.663213 0.015647  F1_macro\n",
      "15                 KNN          18   0.598847 0.040634  F1_macro\n",
      "16                 KNN          14   0.590550 0.045696  F1_macro\n",
      "17                 KNN           5   0.587730 0.034863  F1_macro\n",
      "18           LinearSVM          14   0.575724 0.060321  F1_macro\n",
      "19  LogisticRegression          14   0.575601 0.059368  F1_macro\n",
      "20                 KNN           9   0.573615 0.026770  F1_macro\n",
      "21           LinearSVM          18   0.573391 0.060093  F1_macro\n",
      "22  LogisticRegression          18   0.571865 0.057659  F1_macro\n",
      "23                 KNN           2   0.570985 0.080979  F1_macro\n",
      "24          NaiveBayes           2   0.570126 0.068597  F1_macro\n",
      "25  LogisticRegression           9   0.556833 0.043576  F1_macro\n",
      "26           LinearSVM           9   0.555144 0.040455  F1_macro\n",
      "27          NaiveBayes          14   0.537361 0.007718  F1_macro\n",
      "28          NaiveBayes           9   0.530428 0.009397  F1_macro\n",
      "29  LogisticRegression           5   0.526499 0.030309  F1_macro\n",
      "30           LinearSVM           5   0.525233 0.027148  F1_macro\n",
      "31          NaiveBayes           5   0.515143 0.020860  F1_macro\n",
      "32           LinearSVM           2   0.456025 0.019314  F1_macro\n",
      "33  LogisticRegression           2   0.456025 0.019314  F1_macro\n",
      "34          NaiveBayes          18   0.150417 0.015293  F1_macro\n",
      "35               Dummy           2   0.149977 0.015488  F1_macro\n",
      "36               Dummy           5   0.149977 0.015488  F1_macro\n",
      "37               Dummy           9   0.149977 0.015488  F1_macro\n",
      "38               Dummy          14   0.149977 0.015488  F1_macro\n",
      "39               Dummy          18   0.149977 0.015488  F1_macro\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   0.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.0s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   1.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   1.8s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   1.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   3.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   3.6s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   2.8s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   3.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=   6.0s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   4.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   5.0s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   7.4s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   3.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   7.7s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   0.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   1.8s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   3.3s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   2.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   5.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   6.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   9.9s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   2.9s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   4.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=  10.6s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   4.5s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   2.0s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   3.1s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  13.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   6.9s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   2.9s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   6.4s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   5.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   0.9s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   4.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=   6.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   1.6s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   7.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   2.4s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   2.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=  17.8s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   6.0s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   1.3s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   3.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   6.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   2.4s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   3.2s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   4.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  12.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   9.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   9.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=  13.9s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  11.0s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=  10.0s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 2\n",
      "Best hyperparameters: {'model__n_estimators': 700, 'model__min_samples_split': 2, 'model__min_samples_leaf': 4, 'model__max_features': None, 'model__max_depth': 40}\n",
      "Best CV score (F1 macro): 0.746659\n",
      "\n",
      "=== Holdout ===\n",
      "F1 macro: 0.840768\n",
      "Confusion matrix:\n",
      "[[ 264  107]\n",
      " [  80 1549]]\n",
      "steam threshold: 0.44999999999999996\n",
      "\n",
      "=== OLIST Dataset ===\n",
      "Removed 1 constant feature(s).\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=4 | F1_macro=0.451257 ± 0.123317\n",
      "[2/40] GBT | k=9 | F1_macro=0.451257 ± 0.123317\n",
      "[3/40] GBT | k=17 | F1_macro=0.451257 ± 0.123317\n",
      "[4/40] GBT | k=25 | F1_macro=0.451257 ± 0.123317\n",
      "[5/40] GBT | k=33 | F1_macro=0.451257 ± 0.123317\n",
      "[6/40] RandomForest | k=4 | F1_macro=0.444899 ± 0.105429\n",
      "[7/40] RandomForest | k=9 | F1_macro=0.444899 ± 0.105429\n",
      "[8/40] RandomForest | k=17 | F1_macro=0.444899 ± 0.105429\n",
      "[9/40] RandomForest | k=25 | F1_macro=0.444899 ± 0.105429\n",
      "[10/40] RandomForest | k=33 | F1_macro=0.444899 ± 0.105429\n",
      "[11/40] DecisionTree | k=4 | F1_macro=0.544060 ± 0.026651\n",
      "[12/40] DecisionTree | k=9 | F1_macro=0.544060 ± 0.026651\n",
      "[13/40] DecisionTree | k=17 | F1_macro=0.544060 ± 0.026651\n",
      "[14/40] DecisionTree | k=25 | F1_macro=0.544060 ± 0.026651\n",
      "[15/40] DecisionTree | k=33 | F1_macro=0.544060 ± 0.026651\n",
      "[16/40] LogisticRegression | k=4 | F1_macro=0.525738 ± 0.062211\n",
      "[17/40] LogisticRegression | k=9 | F1_macro=0.571954 ± 0.019034\n",
      "[18/40] LogisticRegression | k=17 | F1_macro=0.575036 ± 0.017283\n",
      "[19/40] LogisticRegression | k=25 | F1_macro=0.566345 ± 0.016810\n",
      "[20/40] LogisticRegression | k=33 | F1_macro=0.569861 ± 0.013042\n",
      "[21/40] LinearSVM | k=4 | F1_macro=0.491450 ± 0.101522\n",
      "[22/40] LinearSVM | k=9 | F1_macro=0.561515 ± 0.019053\n",
      "[23/40] LinearSVM | k=17 | F1_macro=0.564681 ± 0.016486\n",
      "[24/40] LinearSVM | k=25 | F1_macro=0.553205 ± 0.020013\n",
      "[25/40] LinearSVM | k=33 | F1_macro=0.559016 ± 0.014554\n",
      "[26/40] NaiveBayes | k=4 | F1_macro=0.570420 ± 0.006324\n",
      "[27/40] NaiveBayes | k=9 | F1_macro=0.583955 ± 0.009306\n",
      "[28/40] NaiveBayes | k=17 | F1_macro=0.524381 ± 0.064259\n",
      "[29/40] NaiveBayes | k=25 | F1_macro=0.514005 ± 0.070868\n",
      "[30/40] NaiveBayes | k=33 | F1_macro=0.518254 ± 0.066597\n",
      "[31/40] KNN | k=4 | F1_macro=0.403458 ± 0.091088\n",
      "[32/40] KNN | k=9 | F1_macro=0.528982 ± 0.006394\n",
      "[33/40] KNN | k=17 | F1_macro=0.548071 ± 0.010541\n",
      "[34/40] KNN | k=25 | F1_macro=0.558698 ± 0.008024\n",
      "[35/40] KNN | k=33 | F1_macro=0.557896 ± 0.007352\n",
      "[36/40] Dummy | k=4 | F1_macro=0.249264 ± 0.027490\n",
      "[37/40] Dummy | k=9 | F1_macro=0.249264 ± 0.027490\n",
      "[38/40] Dummy | k=17 | F1_macro=0.249264 ± 0.027490\n",
      "[39/40] Dummy | k=25 | F1_macro=0.249264 ± 0.027490\n",
      "[40/40] Dummy | k=33 | F1_macro=0.249264 ± 0.027490\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "                 Model  K_features  MeanScore   StdDev    Metric\n",
      "0           NaiveBayes           9   0.583955 0.009306  F1_macro\n",
      "1   LogisticRegression          17   0.575036 0.017283  F1_macro\n",
      "2   LogisticRegression           9   0.571954 0.019034  F1_macro\n",
      "3           NaiveBayes           4   0.570420 0.006324  F1_macro\n",
      "4   LogisticRegression          33   0.569861 0.013042  F1_macro\n",
      "5   LogisticRegression          25   0.566345 0.016810  F1_macro\n",
      "6            LinearSVM          17   0.564681 0.016486  F1_macro\n",
      "7            LinearSVM           9   0.561515 0.019053  F1_macro\n",
      "8            LinearSVM          33   0.559016 0.014554  F1_macro\n",
      "9                  KNN          25   0.558698 0.008024  F1_macro\n",
      "10                 KNN          33   0.557896 0.007352  F1_macro\n",
      "11           LinearSVM          25   0.553205 0.020013  F1_macro\n",
      "12                 KNN          17   0.548071 0.010541  F1_macro\n",
      "13        DecisionTree           4   0.544060 0.026651  F1_macro\n",
      "14        DecisionTree           9   0.544060 0.026651  F1_macro\n",
      "15        DecisionTree          17   0.544060 0.026651  F1_macro\n",
      "16        DecisionTree          25   0.544060 0.026651  F1_macro\n",
      "17        DecisionTree          33   0.544060 0.026651  F1_macro\n",
      "18                 KNN           9   0.528982 0.006394  F1_macro\n",
      "19  LogisticRegression           4   0.525738 0.062211  F1_macro\n",
      "20          NaiveBayes          17   0.524381 0.064259  F1_macro\n",
      "21          NaiveBayes          33   0.518254 0.066597  F1_macro\n",
      "22          NaiveBayes          25   0.514005 0.070868  F1_macro\n",
      "23           LinearSVM           4   0.491450 0.101522  F1_macro\n",
      "24                 GBT           4   0.451257 0.123317  F1_macro\n",
      "25                 GBT           9   0.451257 0.123317  F1_macro\n",
      "26                 GBT          17   0.451257 0.123317  F1_macro\n",
      "27                 GBT          25   0.451257 0.123317  F1_macro\n",
      "28                 GBT          33   0.451257 0.123317  F1_macro\n",
      "29        RandomForest           4   0.444899 0.105429  F1_macro\n",
      "30        RandomForest           9   0.444899 0.105429  F1_macro\n",
      "31        RandomForest          17   0.444899 0.105429  F1_macro\n",
      "32        RandomForest          25   0.444899 0.105429  F1_macro\n",
      "33        RandomForest          33   0.444899 0.105429  F1_macro\n",
      "34                 KNN           4   0.403458 0.091088  F1_macro\n",
      "35               Dummy           4   0.249264 0.027490  F1_macro\n",
      "36               Dummy           9   0.249264 0.027490  F1_macro\n",
      "37               Dummy          17   0.249264 0.027490  F1_macro\n",
      "38               Dummy          25   0.249264 0.027490  F1_macro\n",
      "39               Dummy          33   0.249264 0.027490  F1_macro\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END .........................model__var_smoothing=1e-11; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683794e-11; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-11; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-10; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-11; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683794e-11; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-10; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683795e-10; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-09; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683794e-11; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-10; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683795e-10; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683795e-09; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-09; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683795e-10; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683795e-09; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-08; total time=   0.0s\n",
      "[CV] END .........model__var_smoothing=3.162277660168379e-08; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-09; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-07; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-08; total time=   0.0s\n",
      "[CV] END .........model__var_smoothing=3.162277660168379e-08; total time=   0.0s\n",
      "[CV] END ........model__var_smoothing=3.1622776601683795e-09; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-08; total time=   0.0s\n",
      "[CV] END .........model__var_smoothing=3.162277660168379e-08; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-07; total time=   0.0s\n",
      "[CV] END .........................model__var_smoothing=1e-07; total time=   0.0s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: NaiveBayes\n",
      "Number of features: 9\n",
      "Best hyperparameters: {'model__var_smoothing': np.float64(1e-11)}\n",
      "Best CV score (F1 macro): 0.583955\n",
      "\n",
      "=== Holdout ===\n",
      "F1 macro: 0.568472\n",
      "Confusion matrix:\n",
      "[[378 357]\n",
      " [559 995]]\n",
      "olist threshold: 0.7999999999999999\n",
      "\n",
      "=== SALES Dataset ===\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=1 | F1_macro=0.554258 ± 0.024924\n",
      "[2/40] GBT | k=2 | F1_macro=0.554258 ± 0.024924\n",
      "[3/40] GBT | k=3 | F1_macro=0.554258 ± 0.024924\n",
      "[4/40] GBT | k=4 | F1_macro=0.554258 ± 0.024924\n",
      "[5/40] GBT | k=5 | F1_macro=0.554258 ± 0.024924\n",
      "[6/40] RandomForest | k=1 | F1_macro=0.545626 ± 0.030510\n",
      "[7/40] RandomForest | k=2 | F1_macro=0.545626 ± 0.030510\n",
      "[8/40] RandomForest | k=3 | F1_macro=0.545626 ± 0.030510\n",
      "[9/40] RandomForest | k=4 | F1_macro=0.545626 ± 0.030510\n",
      "[10/40] RandomForest | k=5 | F1_macro=0.545626 ± 0.030510\n",
      "[11/40] DecisionTree | k=1 | F1_macro=0.556326 ± 0.019427\n",
      "[12/40] DecisionTree | k=2 | F1_macro=0.556326 ± 0.019427\n",
      "[13/40] DecisionTree | k=3 | F1_macro=0.556326 ± 0.019427\n",
      "[14/40] DecisionTree | k=4 | F1_macro=0.556326 ± 0.019427\n",
      "[15/40] DecisionTree | k=5 | F1_macro=0.556326 ± 0.019427\n",
      "[16/40] LogisticRegression | k=1 | F1_macro=0.434752 ± 0.038091\n",
      "[17/40] LogisticRegression | k=2 | F1_macro=0.356476 ± 0.143320\n",
      "[18/40] LogisticRegression | k=3 | F1_macro=0.389234 ± 0.120115\n",
      "[19/40] LogisticRegression | k=4 | F1_macro=0.397830 ± 0.107817\n",
      "[20/40] LogisticRegression | k=5 | F1_macro=0.397779 ± 0.107662\n",
      "[21/40] LinearSVM | k=1 | F1_macro=0.434752 ± 0.038091\n",
      "[22/40] LinearSVM | k=2 | F1_macro=0.383750 ± 0.105389\n",
      "[23/40] LinearSVM | k=3 | F1_macro=0.402911 ± 0.093396\n",
      "[24/40] LinearSVM | k=4 | F1_macro=0.406649 ± 0.087982\n",
      "[25/40] LinearSVM | k=5 | F1_macro=0.407075 ± 0.088191\n",
      "[26/40] NaiveBayes | k=1 | F1_macro=0.434752 ± 0.038091\n",
      "[27/40] NaiveBayes | k=2 | F1_macro=0.467503 ± 0.027134\n",
      "[28/40] NaiveBayes | k=3 | F1_macro=0.494038 ± 0.016499\n",
      "[29/40] NaiveBayes | k=4 | F1_macro=0.531199 ± 0.014024\n",
      "[30/40] NaiveBayes | k=5 | F1_macro=0.530881 ± 0.011614\n",
      "[31/40] KNN | k=1 | F1_macro=0.488071 ± 0.003744\n",
      "[32/40] KNN | k=2 | F1_macro=0.536616 ± 0.066310\n",
      "[33/40] KNN | k=3 | F1_macro=0.520251 ± 0.012422\n",
      "[34/40] KNN | k=4 | F1_macro=0.532880 ± 0.006763\n",
      "[35/40] KNN | k=5 | F1_macro=0.539498 ± 0.025416\n",
      "[36/40] Dummy | k=1 | F1_macro=0.488071 ± 0.003744\n",
      "[37/40] Dummy | k=2 | F1_macro=0.488071 ± 0.003744\n",
      "[38/40] Dummy | k=3 | F1_macro=0.488071 ± 0.003744\n",
      "[39/40] Dummy | k=4 | F1_macro=0.488071 ± 0.003744\n",
      "[40/40] Dummy | k=5 | F1_macro=0.488071 ± 0.003744\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "                 Model  K_features  MeanScore   StdDev    Metric\n",
      "0         DecisionTree           1   0.556326 0.019427  F1_macro\n",
      "1         DecisionTree           2   0.556326 0.019427  F1_macro\n",
      "2         DecisionTree           3   0.556326 0.019427  F1_macro\n",
      "3         DecisionTree           4   0.556326 0.019427  F1_macro\n",
      "4         DecisionTree           5   0.556326 0.019427  F1_macro\n",
      "5                  GBT           1   0.554258 0.024924  F1_macro\n",
      "6                  GBT           2   0.554258 0.024924  F1_macro\n",
      "7                  GBT           3   0.554258 0.024924  F1_macro\n",
      "8                  GBT           4   0.554258 0.024924  F1_macro\n",
      "9                  GBT           5   0.554258 0.024924  F1_macro\n",
      "10        RandomForest           1   0.545626 0.030510  F1_macro\n",
      "11        RandomForest           2   0.545626 0.030510  F1_macro\n",
      "12        RandomForest           3   0.545626 0.030510  F1_macro\n",
      "13        RandomForest           4   0.545626 0.030510  F1_macro\n",
      "14        RandomForest           5   0.545626 0.030510  F1_macro\n",
      "15                 KNN           5   0.539498 0.025416  F1_macro\n",
      "16                 KNN           2   0.536616 0.066310  F1_macro\n",
      "17                 KNN           4   0.532880 0.006763  F1_macro\n",
      "18          NaiveBayes           4   0.531199 0.014024  F1_macro\n",
      "19          NaiveBayes           5   0.530881 0.011614  F1_macro\n",
      "20                 KNN           3   0.520251 0.012422  F1_macro\n",
      "21          NaiveBayes           3   0.494038 0.016499  F1_macro\n",
      "22               Dummy           1   0.488071 0.003744  F1_macro\n",
      "23               Dummy           2   0.488071 0.003744  F1_macro\n",
      "24               Dummy           3   0.488071 0.003744  F1_macro\n",
      "25               Dummy           4   0.488071 0.003744  F1_macro\n",
      "26               Dummy           5   0.488071 0.003744  F1_macro\n",
      "27                 KNN           1   0.488071 0.003744  F1_macro\n",
      "28          NaiveBayes           2   0.467503 0.027134  F1_macro\n",
      "29           LinearSVM           1   0.434752 0.038091  F1_macro\n",
      "30  LogisticRegression           1   0.434752 0.038091  F1_macro\n",
      "31          NaiveBayes           1   0.434752 0.038091  F1_macro\n",
      "32           LinearSVM           5   0.407075 0.088191  F1_macro\n",
      "33           LinearSVM           4   0.406649 0.087982  F1_macro\n",
      "34           LinearSVM           3   0.402911 0.093396  F1_macro\n",
      "35  LogisticRegression           4   0.397830 0.107817  F1_macro\n",
      "36  LogisticRegression           5   0.397779 0.107662  F1_macro\n",
      "37  LogisticRegression           3   0.389234 0.120115  F1_macro\n",
      "38           LinearSVM           2   0.383750 0.105389  F1_macro\n",
      "39  LogisticRegression           2   0.356476 0.143320  F1_macro\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=2, model__min_samples_split=10, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=1, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=None, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=2, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=1, model__min_samples_split=2, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=1, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=40, model__min_samples_leaf=1, model__min_samples_split=5, model__splitter=best; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=4, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=1, model__min_samples_split=5, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=20, model__min_samples_leaf=4, model__min_samples_split=2, model__splitter=random; total time=   0.0s\n",
      "[CV] END model__max_depth=10, model__min_samples_leaf=1, model__min_samples_split=10, model__splitter=best; total time=   0.0s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: DecisionTree\n",
      "Number of features: 1\n",
      "Best hyperparameters: {'model__splitter': 'best', 'model__min_samples_split': 5, 'model__min_samples_leaf': 4, 'model__max_depth': 20}\n",
      "Best CV score (F1 macro): 0.562717\n",
      "\n",
      "=== Holdout ===\n",
      "F1 macro: 0.582536\n",
      "Confusion matrix:\n",
      "[[1747  171]\n",
      " [  50   32]]\n",
      "sales threshold: 0.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Classification call\n",
    "splits = prepare_data(\n",
    "    steam_df=steam,\n",
    "    olist_df=olist,\n",
    "    sales_df=sales,\n",
    "    test_size=0.2,\n",
    "    random_state=random_state,\n",
    "    feature_selection='tree',\n",
    "    max_features=100,\n",
    "    task_type='classification',\n",
    "    scale_method='standard'\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "print(\"\\n=== STEAM Dataset ===\")\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ")\n",
    "\n",
    "score_steam = evaluate_on_holdout(best_steam_model, X_test_steam, y_test_steam, task_type=\"classification\")\n",
    "\n",
    "print(\"steam threshold:\", getattr(best_steam_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== OLIST Dataset ===\")\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ")\n",
    "\n",
    "score_olist = evaluate_on_holdout(best_olist_model, X_test_olist, y_test_olist, task_type=\"classification\")\n",
    "\n",
    "print(\"olist threshold:\", getattr(best_olist_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== SALES Dataset ===\")\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True\n",
    ") \n",
    "\n",
    "score_sales = evaluate_on_holdout(best_sales_model, X_test_sales, y_test_sales, task_type=\"classification\")\n",
    "\n",
    "print(\"sales threshold:\", getattr(best_sales_model, \"best_threshold_\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec77e4c",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7bc10f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Regression call\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m splits = \u001b[43mprepare_all\u001b[49m(\n\u001b[32m      3\u001b[39m     steam, olist, sales,\n\u001b[32m      4\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mregression\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     test_size=\u001b[32m0.2\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\u001b[33m\"\u001b[39m\u001b[33msteam\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\u001b[33m\"\u001b[39m\u001b[33molist\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'prepare_all' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regression call\n",
    "splits = prepare_all(\n",
    "    steam, olist, sales,\n",
    "    task_type=\"regression\",\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
