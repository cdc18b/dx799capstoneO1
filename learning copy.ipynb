{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26fc83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.241 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.193 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.191 sec\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 50000 of 41154794 rows in 5.762 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(50000, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: picked 50000 of 99441 rows in 0.004 sec\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (57112, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: class counts too small for requested size, falling back to simple sample\n",
      "simple_random_sample: picked 50000 of 55792 rows in 0.003 sec\n",
      "vg2019: done shape=(50000, 16)\n",
      "main: load all done in 16.504 sec (00:00:16)\n",
      "download: shapes summary\n",
      "download: steam shape = (50000, 24)\n",
      "download: olist shape = (57112, 38)\n",
      "download: sales shape = (50000, 16)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Standard Library\n",
    "# =============================\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from itertools import chain, combinations\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# =============================\n",
    "# General / Utility (3rd party)\n",
    "# =============================\n",
    "import requests\n",
    "import kagglehub\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "# =============================\n",
    "# Data / Scientific\n",
    "# =============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "from scipy.special import expit, logit\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "# =============================\n",
    "# Visualization\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================\n",
    "# Imbalanced-Learn\n",
    "# =============================\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn (core / model selection)\n",
    "# =============================\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning, NotFittedError\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import (\n",
    "    SequentialFeatureSelector,\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    f_classif,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    "    mutual_info_regression,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import (\n",
    "    ElasticNet,\n",
    "    ElasticNetCV,\n",
    "    Lasso,\n",
    "    LassoCV,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    "    RidgeCV,\n",
    "    RidgeClassifier,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    get_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ParameterGrid,\n",
    "    ParameterSampler,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    "    TimeSeriesSplit,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "import sklearn.model_selection._search as sk_search\n",
    "import sklearn.model_selection._validation as sk_validation\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    MultiLabelBinarizer,\n",
    "    Normalizer,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn (ensembles)\n",
    "# =============================\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Global Settings\n",
    "# =============================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "random_state = 42\n",
    "N_ROWS = 50_000\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")  # no scientific notation\n",
    "\n",
    "def robust_eda(df, name):\n",
    "    # simple settings\n",
    "    top_k_categories = 20\n",
    "    max_corr_cols = 30\n",
    "    max_rows_to_show = 25\n",
    "\n",
    "    # make printing wide and avoid scientific notation\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\", max_rows_to_show,\n",
    "        \"display.max_columns\", None,\n",
    "        \"display.width\", 1000,\n",
    "        \"display.max_colwidth\", 200,\n",
    "        \"display.float_format\", lambda x: f\"{x:.6f}\"\n",
    "    ):\n",
    "        report_lines = []\n",
    "\n",
    "        # title\n",
    "        report_lines.append(f\"=== Robust EDA Report: {name} ===\")\n",
    "\n",
    "        # shapes and memory\n",
    "        info_df = pd.DataFrame(\n",
    "            {\n",
    "                \"rows\": [df.shape[0]],\n",
    "                \"columns\": [df.shape[1]],\n",
    "                \"memory_bytes\": [int(df.memory_usage(deep=True).sum())],\n",
    "            }\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Info ===\")\n",
    "        report_lines.append(info_df.to_string(index=False))\n",
    "\n",
    "        # dtypes\n",
    "        dtypes_df = (\n",
    "            df.dtypes.rename(\"dtype\")\n",
    "            .astype(str)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"column\"})\n",
    "            .sort_values(\"column\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Dtypes ===\")\n",
    "        report_lines.append(dtypes_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # missing values\n",
    "        total_rows = len(df)\n",
    "        missing_counts = df.isna().sum()\n",
    "        if total_rows > 0:\n",
    "            missing_percent = (missing_counts / total_rows * 100).round(2)\n",
    "        else:\n",
    "            missing_percent = pd.Series([0] * len(df.columns), index=df.columns)\n",
    "        missing_df = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": df.columns,\n",
    "                    \"missing_count\": missing_counts.values,\n",
    "                    \"missing_percent\": missing_percent.values,\n",
    "                }\n",
    "            )\n",
    "            .sort_values([\"missing_count\", \"missing_percent\"], ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Missing Values ===\")\n",
    "        report_lines.append(missing_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # duplicates (safe fallback for unhashable types)\n",
    "        try:\n",
    "            duplicate_count = int(df.duplicated().sum())\n",
    "            duplicate_index = df.index[df.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "        except TypeError:\n",
    "            df_hashable = df.astype(str)\n",
    "            duplicate_count = int(df_hashable.duplicated().sum())\n",
    "            duplicate_index = df_hashable.index[df_hashable.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "\n",
    "        duplicates_summary_df = pd.DataFrame({\"duplicate_rows\": [duplicate_count]})\n",
    "        report_lines.append(\"\\n=== Duplicates Summary ===\")\n",
    "        report_lines.append(duplicates_summary_df.to_string(index=False))\n",
    "        report_lines.append(\"\\n=== Duplicates Preview (up to 20 rows) ===\")\n",
    "        if len(duplicates_preview_df) > 0:\n",
    "            report_lines.append(duplicates_preview_df.to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"(No duplicate rows found.)\")\n",
    "\n",
    "        # column groups\n",
    "        numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        categorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "        # numeric summary\n",
    "        if len(numeric_columns) > 0:\n",
    "            percentiles = [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "            numeric_summary_df = (\n",
    "                df[numeric_columns]\n",
    "                .describe(percentiles=percentiles)\n",
    "                .T.reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Numeric Summary (5%..95%) ===\")\n",
    "            report_lines.append(numeric_summary_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # skew and kurtosis\n",
    "            skew_kurt_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": numeric_columns,\n",
    "                    \"skew\": df[numeric_columns].skew(numeric_only=True).values,\n",
    "                    \"kurtosis\": df[numeric_columns].kurtosis(numeric_only=True).values,\n",
    "                }\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Skew and Kurtosis ===\")\n",
    "            report_lines.append(skew_kurt_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # IQR outliers per column\n",
    "            q1 = df[numeric_columns].quantile(0.25)\n",
    "            q3 = df[numeric_columns].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outlier_mask = (df[numeric_columns] < (q1 - 1.5 * iqr)) | (df[numeric_columns] > (q3 + 1.5 * iqr))\n",
    "            iqr_outliers_df = (\n",
    "                outlier_mask.sum()\n",
    "                .rename(\"outlier_count\")\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== IQR Outlier Counts ===\")\n",
    "            report_lines.append(iqr_outliers_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # correlation on first N numeric columns\n",
    "            if len(numeric_columns) > 1:\n",
    "                selected_cols = numeric_columns[:max_corr_cols]\n",
    "                correlation_df = df[selected_cols].corr(method=\"pearson\", numeric_only=True)\n",
    "                correlation_df.index.name = \"column\"\n",
    "                report_lines.append(f\"\\n=== Correlation (first {max_corr_cols} numeric columns) ===\")\n",
    "                report_lines.append(correlation_df.to_string())\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No numeric columns found.)\")\n",
    "\n",
    "        # categorical value counts (top K each)\n",
    "        if len(categorical_columns) > 0:\n",
    "            cat_rows = []\n",
    "            for col in categorical_columns:\n",
    "                try:\n",
    "                    vc = df[col].value_counts(dropna=False).head(top_k_categories)\n",
    "                except TypeError:\n",
    "                    vc = df[col].astype(str).value_counts(dropna=False).head(top_k_categories)\n",
    "                for value, count in vc.items():\n",
    "                    percent = (count / total_rows * 100) if total_rows > 0 else 0\n",
    "                    cat_rows.append(\n",
    "                        {\"column\": col, \"value\": value, \"count\": int(count), \"percent\": round(percent, 2)}\n",
    "                    )\n",
    "            categorical_values_df = pd.DataFrame(cat_rows)\n",
    "            report_lines.append(f\"\\n=== Categorical Values (Top {top_k_categories} per column) ===\")\n",
    "            report_lines.append(categorical_values_df.head(max_rows_to_show).to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No categorical columns found.)\")\n",
    "\n",
    "        # unique counts per column\n",
    "        def _safe_nunique(series):\n",
    "            try:\n",
    "                return int(series.nunique(dropna=False))\n",
    "            except TypeError:\n",
    "                return np.nan\n",
    "\n",
    "        unique_counts_df = pd.DataFrame(\n",
    "            {\"column\": df.columns, \"unique_values\": [_safe_nunique(df[c]) for c in df.columns]}\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Unique Counts Per Column ===\")\n",
    "        report_lines.append(unique_counts_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # sample head\n",
    "        report_lines.append(\"\\n=== Head (10 rows) ===\")\n",
    "        report_lines.append(df.head(10).to_string(index=False))\n",
    "\n",
    "        # end\n",
    "        report_lines.append(\"\\n=== End of EDA Report ===\")\n",
    "\n",
    "        # one giant print\n",
    "        print(\"\\n\".join(report_lines))\n",
    "\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "        return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Timer + memory helpers\n",
    "# =========================\n",
    "class SimpleTimer:\n",
    "    def __init__(self, enabled=True):\n",
    "        self.enabled = enabled\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "    def tick(self, label):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        t = time.perf_counter() - self.t0\n",
    "        print(f\"[timer] {label}: {t:.2f} s\")\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "\n",
    "def df_mem_gb(df):\n",
    "    try:\n",
    "        return float(df.memory_usage(deep=True).sum()) / (1024 ** 3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "def show_shape_mem(label, X_train=None, X_test=None):\n",
    "    parts = [label]\n",
    "    if X_train is not None:\n",
    "        parts.append(f\"X_train shape={tuple(X_train.shape)} mem={df_mem_gb(X_train):.3f} GB\")\n",
    "    if X_test is not None:\n",
    "        parts.append(f\"X_test shape={tuple(X_test.shape)} mem={df_mem_gb(X_test):.3f} GB\")\n",
    "    print(\"[info]\", \" | \".join(parts))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Text feature helpers (fast)\n",
    "# =========================\n",
    "def clean_keyword_name(s):\n",
    "    s = str(s).lower().strip().replace(\" \", \"_\")\n",
    "    keep = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch == \"_\":\n",
    "            keep.append(ch)\n",
    "    return \"\".join(keep)[:60]\n",
    "\n",
    "\n",
    "def text_features_fit(X, keyword_map):\n",
    "    keyword_map = keyword_map or {}\n",
    "    new_cols = []\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        if col not in X.columns:\n",
    "            continue\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"]:\n",
    "            continue\n",
    "        if col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            safe = clean_keyword_name(kw)\n",
    "            name = f\"{col}_has_{safe}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        df_part = pd.DataFrame(part, index=X.index)\n",
    "        new_parts.append(df_part)\n",
    "        new_cols.extend(df_part.columns.tolist())\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    return {\"new_cols\": new_cols, \"keyword_map\": keyword_map}\n",
    "\n",
    "\n",
    "def text_features_apply(X, text_info):\n",
    "    keyword_map = text_info.get(\"keyword_map\") or {}\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        if col not in X.columns:\n",
    "            part = {\n",
    "                len_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "                wc_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "            }\n",
    "            for kw in keywords:\n",
    "                name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "                part[name] = pd.Series(0, index=X.index, dtype=np.uint8)\n",
    "            new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "            continue\n",
    "\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"] or col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    for c in text_info.get(\"new_cols\", []):\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_text_length_features_inplace(X, exclude_cols=None):\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    obj_cols = [c for c in X.columns if str(X[c].dtype) in [\"object\", \"category\"] and c not in exclude_cols]\n",
    "    for c in obj_cols:\n",
    "        X[f\"{c}_length\"] = X[c].fillna(\"\").astype(str).str.len().astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# General helpers\n",
    "# =========================\n",
    "def datetimes_to_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        if np.issubdtype(X[c].dtype, np.datetime64):\n",
    "            mask = X[c].isna()\n",
    "            vals_int = X[c].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "            arr = vals_int.astype(\"float64\") / 1000000000.0\n",
    "            if mask.any():\n",
    "                arr[mask.values] = np.nan\n",
    "            X[c] = arr\n",
    "    return X\n",
    "\n",
    "\n",
    "def downcast_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        dt = X[c].dtype\n",
    "        if np.issubdtype(dt, np.floating):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "        elif np.issubdtype(dt, np.integer) and X[c].nunique(dropna=True) > 2:\n",
    "            X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def scale_fit(method, X_train_num):\n",
    "    if method == \"standard\":\n",
    "        return StandardScaler().fit(X_train_num)\n",
    "    if method == \"minmax\":\n",
    "        return MinMaxScaler().fit(X_train_num)\n",
    "    return None\n",
    "\n",
    "\n",
    "def scale_numeric_only(X_train, X_test, scale_method):\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return X_train, X_test\n",
    "    scaler = scale_fit(scale_method, X_train[num_cols])\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols]).astype(\"float32\")\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols]).astype(\"float32\")\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Safer OHE with caps\n",
    "# =========================\n",
    "def _auto_exclude_mask(series, max_unique=500, max_avg_len=25):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    nunq = int(s.nunique(dropna=False))\n",
    "    avg_len = float(s.map(len).mean())\n",
    "    return (nunq > max_unique) or (avg_len > max_avg_len and nunq > 50)\n",
    "\n",
    "\n",
    "def _cap_categories(series, top_k=100, min_freq=1, other_label=\"Other\"):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    vc = s.value_counts()\n",
    "    kept = vc[vc >= min_freq].index.tolist()\n",
    "    if top_k is not None and len(kept) > top_k:\n",
    "        kept = vc.index[:top_k].tolist()\n",
    "    mapped = s.where(s.isin(kept), other_label)\n",
    "    return mapped.astype(\"category\"), kept\n",
    "\n",
    "\n",
    "def ohe_fit(\n",
    "    X,\n",
    "    exclude_cols=None,\n",
    "    top_k_per_col=100,\n",
    "    min_freq_per_col=1,\n",
    "    auto_exclude=False,\n",
    "    high_card_threshold=500,\n",
    "    long_text_avglen=25,\n",
    "):\n",
    "    exclude = set(exclude_cols or [])\n",
    "    value_map = {}\n",
    "\n",
    "    X_tmp = X.copy()\n",
    "    obj_cols = [c for c in X_tmp.select_dtypes(include=[\"object\", \"category\"]).columns]\n",
    "    excluded = list(exclude)\n",
    "\n",
    "    for c in obj_cols:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        s = X_tmp[c]\n",
    "        if auto_exclude:\n",
    "            if _auto_exclude_mask(s, max_unique=high_card_threshold, max_avg_len=long_text_avglen):\n",
    "                excluded.append(c)\n",
    "                continue\n",
    "        capped, kept = _cap_categories(s, top_k=top_k_per_col, min_freq=min_freq_per_col)\n",
    "        X_tmp[c] = capped\n",
    "        value_map[c] = kept\n",
    "\n",
    "    X_tmp = X_tmp.drop(columns=excluded, errors=\"ignore\")\n",
    "    obj_cols_final = [c for c in obj_cols if c not in excluded]\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=obj_cols_final, dummy_na=False)\n",
    "    schema_cols = X_ohe.columns.tolist()\n",
    "\n",
    "    return {\n",
    "        \"obj_cols\": obj_cols_final,\n",
    "        \"schema_cols\": schema_cols,\n",
    "        \"value_map\": value_map,\n",
    "        \"excluded\": excluded,\n",
    "        \"other_label\": \"Other\",\n",
    "    }\n",
    "\n",
    "\n",
    "def ohe_apply(X, ohe_info):\n",
    "    obj_cols = ohe_info[\"obj_cols\"]\n",
    "    schema_cols = ohe_info[\"schema_cols\"]\n",
    "    value_map = ohe_info[\"value_map\"]\n",
    "    other = ohe_info.get(\"other_label\", \"Other\")\n",
    "    excluded = ohe_info.get(\"excluded\", [])\n",
    "\n",
    "    X_tmp = X.drop(columns=excluded, errors=\"ignore\").copy()\n",
    "\n",
    "    for c in obj_cols:\n",
    "        if c in X_tmp.columns:\n",
    "            s = X_tmp[c].fillna(\"Unknown\").astype(str)\n",
    "            kept = set(value_map.get(c, []))\n",
    "            s = s.where(s.isin(kept), other).astype(\"category\")\n",
    "            X_tmp[c] = s\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=[c for c in obj_cols if c in X_tmp.columns], dummy_na=False)\n",
    "    X_ohe = X_ohe.reindex(columns=schema_cols, fill_value=0)\n",
    "    return X_ohe\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Outliers\n",
    "# =========================\n",
    "def outlier_bounds_fit(X_num, lower_q=0.025, upper_q=0.975, exclude_binary=True, sample_rows=200000):\n",
    "    bounds = {}\n",
    "    if lower_q is None or upper_q is None:\n",
    "        return bounds\n",
    "    X_use = X_num\n",
    "    if len(X_num) > sample_rows:\n",
    "        X_use = X_num.sample(n=sample_rows, random_state=123)\n",
    "    for c in X_use.columns:\n",
    "        vals = X_use[c].astype(\"float32\")\n",
    "        if exclude_binary and X_use[c].nunique(dropna=True) <= 2:\n",
    "            continue\n",
    "        lo = np.nanquantile(vals, lower_q)\n",
    "        hi = np.nanquantile(vals, upper_q)\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and hi >= lo:\n",
    "            bounds[c] = (float(lo), float(hi))\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def outlier_clip_inplace(X, bounds):\n",
    "    if not bounds:\n",
    "        return X\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].astype(\"float32\").clip(lo, hi)\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature selection\n",
    "# =========================\n",
    "def forward_feature_selection(X, y, model,\n",
    "                              scoring=\"neg_mean_absolute_error\",\n",
    "                              cv=5, tol=None, max_features=None, n_jobs=-1, verbose=False):\n",
    "    try:\n",
    "        feature_names = list(X.columns)\n",
    "        X_arr = X.values\n",
    "    except AttributeError:\n",
    "        X_arr = X\n",
    "        feature_names = [f\"f{i}\" for i in range(X_arr.shape[1])]\n",
    "\n",
    "    selected_idx = []\n",
    "    remaining_idx = list(range(X_arr.shape[1]))\n",
    "    best_scores = []\n",
    "    previous_score = float(\"inf\")\n",
    "    best_feature_set_idx = []\n",
    "    best_score = float(\"inf\")\n",
    "\n",
    "    while remaining_idx:\n",
    "        scores = {}\n",
    "        for idx in remaining_idx:\n",
    "            trial_idx = selected_idx + [idx]\n",
    "            cv_score = -cross_val_score(\n",
    "                model, X_arr[:, trial_idx], y,\n",
    "                scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "            ).mean()\n",
    "            scores[idx] = cv_score\n",
    "\n",
    "        best_idx = min(scores, key=scores.get)\n",
    "        current_score = scores[best_idx]\n",
    "\n",
    "        if tol is not None and previous_score - current_score < tol:\n",
    "            if verbose:\n",
    "                print(\"Stopping early (improvement < tol).\")\n",
    "            break\n",
    "\n",
    "        selected_idx.append(best_idx)\n",
    "        remaining_idx.remove(best_idx)\n",
    "        best_scores.append(current_score)\n",
    "        previous_score = current_score\n",
    "\n",
    "        if verbose:\n",
    "            name = feature_names[best_idx]\n",
    "            print(f\"Added {name} -> CV score = {current_score:.4f}\")\n",
    "\n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_feature_set_idx = selected_idx.copy()\n",
    "\n",
    "        if max_features is not None and len(selected_idx) >= max_features:\n",
    "            break\n",
    "\n",
    "    selected_features = [feature_names[i] for i in selected_idx]\n",
    "    best_feature_set = [feature_names[i] for i in best_feature_set_idx]\n",
    "\n",
    "    if not best_feature_set:\n",
    "        best_feature_set = selected_features[:]\n",
    "        best_score = best_scores[-1] if best_scores else float(\"inf\")\n",
    "\n",
    "    if verbose:\n",
    "        try:\n",
    "            index = np.argmax(np.array(selected_features) == best_feature_set[-1])\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(best_scores) + 1), best_scores, marker=\".\")\n",
    "            plt.plot([index + 1], [best_score], marker=\"x\")\n",
    "            plt.xticks(range(1, len(selected_features) + 1),\n",
    "                       selected_features, rotation=60, ha=\"right\", fontsize=6)\n",
    "            plt.title(\"Forward Feature Selection and CV Scores\")\n",
    "            plt.xlabel(\"Features Added\")\n",
    "            plt.ylabel(\"CV Score (MAE)\")\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"Best Features: {best_feature_set}\")\n",
    "        print(f\"Best CV MAE Score: {best_score:.4f}\")\n",
    "    return selected_features, best_scores, best_feature_set, best_score\n",
    "\n",
    "\n",
    "def select_features(method, max_features, task_type, random_state, X_train, y_train, verbose=False):\n",
    "    if method is None or method == \"none\":\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    k = min(max_features, X_train.shape[1]) if max_features else X_train.shape[1]\n",
    "    if k < 1:\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    if method == \"tree\":\n",
    "        est = RandomForestClassifier(random_state=random_state) if task_type == \"classification\" else RandomForestRegressor(random_state=random_state)\n",
    "        est.fit(X_train, y_train)\n",
    "        imp = pd.Series(est.feature_importances_, index=X_train.columns)\n",
    "        return imp.nlargest(k).index.tolist()\n",
    "\n",
    "    if method == \"forward\":\n",
    "        if task_type != \"regression\":\n",
    "            raise ValueError(\"Forward selection is only supported for regression tasks.\")\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        _, _, best_set, _ = forward_feature_selection(\n",
    "            X=X_train, y=y_train, model=model,\n",
    "            scoring=\"neg_mean_absolute_error\", cv=3,\n",
    "            tol=None, max_features=max_features, n_jobs=-1, verbose=verbose\n",
    "        )\n",
    "        return best_set\n",
    "\n",
    "    if method == \"mutual_info\":\n",
    "        sel = SelectKBest(mutual_info_classif if task_type == \"classification\" else mutual_info_regression, k=k)\n",
    "        sel.fit(X_train, y_train)\n",
    "        return X_train.columns[sel.get_support()].tolist()\n",
    "\n",
    "    return X_train.columns.tolist()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Batched poly features\n",
    "# =========================\n",
    "def add_poly_features_batched(X_train, X_test, squares, pairs):\n",
    "    train_parts = {}\n",
    "    for c in squares:\n",
    "        if c in X_train.columns:\n",
    "            train_parts[f\"{c}_sq\"] = X_train[c].astype(\"float32\") ** 2\n",
    "    for a, b in pairs:\n",
    "        if (a in X_train.columns) and (b in X_train.columns):\n",
    "            name = f\"{a}_x_{b}\"\n",
    "            train_parts[name] = (X_train[a].astype(\"float32\") * X_train[b].astype(\"float32\"))\n",
    "\n",
    "    test_parts = {}\n",
    "    for name in train_parts:\n",
    "        if name.endswith(\"_sq\"):\n",
    "            c = name[:-3]\n",
    "            if c in X_test.columns:\n",
    "                test_parts[name] = X_test[c].astype(\"float32\") ** 2\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "        else:\n",
    "            a, b = name.split(\"_x_\")\n",
    "            if (a in X_test.columns) and (b in X_test.columns):\n",
    "                test_parts[name] = (X_test[a].astype(\"float32\") * X_test[b].astype(\"float32\"))\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "\n",
    "    if train_parts:\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(train_parts, index=X_train.index)], axis=1)\n",
    "    if test_parts:\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(test_parts, index=X_test.index)], axis=1)\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train.copy())\n",
    "    X_test = downcast_numeric_inplace(X_test.copy())\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main prep (with best hyperparameters per dataset)\n",
    "# =========================\n",
    "def prepare_data(\n",
    "    steam_df,\n",
    "    olist_df,\n",
    "    sales_df,\n",
    "    test_size,\n",
    "    random_state,\n",
    "    feature_selection=None,\n",
    "    max_features=None,\n",
    "    task_type=\"classification\",\n",
    "    scale_method=None,\n",
    "    use_best_from_sweep=True,\n",
    "    verbose=False,\n",
    "):\n",
    "    outputs = {}\n",
    "    timer = SimpleTimer(enabled=verbose)\n",
    "\n",
    "    if use_best_from_sweep:\n",
    "        steam_hp = {\n",
    "            \"cutoff\": 95.0,\n",
    "            \"feature_selection\": None,\n",
    "            \"max_features\": None,\n",
    "            \"scale_method\": \"standard\",\n",
    "            \"ohe_auto_exclude\": False,\n",
    "            \"tag_min_count\": 5,\n",
    "            \"tag_top_k\": 200,\n",
    "            \"ohe_top_k_per_col\": 400,\n",
    "            \"ohe_min_freq_per_col\": 5,\n",
    "            \"outlier_lower_q\": 0.05,\n",
    "            \"outlier_upper_q\": 0.95,\n",
    "            \"ohe_high_card_threshold\": 300,\n",
    "            \"ohe_long_text_avglen\": 25,\n",
    "        }\n",
    "        olist_hp = {\n",
    "            \"cutoff\": 4.0,\n",
    "            \"feature_selection\": None,\n",
    "            \"max_features\": None,\n",
    "            \"scale_method\": \"standard\",\n",
    "            \"ohe_auto_exclude\": True,\n",
    "            \"tag_min_count\": 3,\n",
    "            \"tag_top_k\": 400,\n",
    "            \"ohe_top_k_per_col\": 400,\n",
    "            \"ohe_min_freq_per_col\": 1,\n",
    "            \"outlier_lower_q\": 0.01,\n",
    "            \"outlier_upper_q\": 0.99,\n",
    "            \"ohe_high_card_threshold\": 800,\n",
    "            \"ohe_long_text_avglen\": 35,\n",
    "        }\n",
    "        sales_hp = {\n",
    "            \"cutoff\": 5.0,\n",
    "            \"feature_selection\": None,\n",
    "            \"max_features\": None,\n",
    "            \"scale_method\": \"standard\",\n",
    "            \"ohe_auto_exclude\": True,\n",
    "            \"tag_min_count\": 10,\n",
    "            \"tag_top_k\": 100,\n",
    "            \"ohe_top_k_per_col\": 200,\n",
    "            \"ohe_min_freq_per_col\": 5,\n",
    "            \"outlier_lower_q\": 0.01,\n",
    "            \"outlier_upper_q\": 0.99,\n",
    "            \"ohe_high_card_threshold\": 800,\n",
    "            \"ohe_long_text_avglen\": 20,\n",
    "        }\n",
    "    else:\n",
    "        steam_hp = {\n",
    "            \"cutoff\": 50.0,\n",
    "            \"feature_selection\": (feature_selection or \"none\").lower(),\n",
    "            \"max_features\": max_features,\n",
    "            \"scale_method\": scale_method,\n",
    "            \"ohe_auto_exclude\": False,\n",
    "            \"tag_min_count\": 5,\n",
    "            \"tag_top_k\": 200,\n",
    "            \"ohe_top_k_per_col\": 100,\n",
    "            \"ohe_min_freq_per_col\": 1,\n",
    "            \"outlier_lower_q\": 0.025,\n",
    "            \"outlier_upper_q\": 0.975,\n",
    "            \"ohe_high_card_threshold\": 500,\n",
    "            \"ohe_long_text_avglen\": 25,\n",
    "        }\n",
    "        olist_hp = steam_hp.copy()\n",
    "        olist_hp[\"cutoff\"] = 2.5\n",
    "        sales_hp = steam_hp.copy()\n",
    "        sales_hp[\"cutoff\"] = 5.0\n",
    "\n",
    "    # ---------- STEAM ----------\n",
    "    steam = steam_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        steam[\"target\"] = (steam[\"positive_ratio\"] >= float(steam_hp[\"cutoff\"])).astype(int)\n",
    "    else:\n",
    "        steam[\"target\"] = steam[\"positive_ratio\"]\n",
    "    steam.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    steam.drop(columns=[\"app_id\", \"user_id\", \"review_id\", \"positive_ratio\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    if {\"date\", \"date_release\"}.issubset(steam.columns):\n",
    "        steam[\"days_since_release\"] = (steam[\"date\"] - steam[\"date_release\"]).dt.days\n",
    "\n",
    "    for col in [\"is_recommended\", \"mac\", \"linux\", \"win\", \"steam_deck\"]:\n",
    "        if col in steam.columns:\n",
    "            steam[col] = steam[col].astype(int)\n",
    "\n",
    "    if \"hours\" in steam.columns:\n",
    "        steam[\"log_hours\"] = np.log1p(steam[\"hours\"])\n",
    "    if {\"hours\", \"user_reviews\"}.issubset(steam.columns):\n",
    "        steam[\"reviews_per_hour\"] = steam[\"user_reviews\"] / (steam[\"hours\"] + 0.000000001)\n",
    "\n",
    "    X = steam.drop(columns=[\"target\", \"rating\"], errors=\"ignore\")\n",
    "    y = steam[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"steam split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"steam datetime to numeric\")\n",
    "\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test, exclude_cols=[\"tags\"])\n",
    "\n",
    "    steam_kw = {\n",
    "        \"title\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"coop\", \"online\", \"free\", \"demo\", \"survival\"],\n",
    "        \"description\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"open world\", \"story\", \"puzzle\", \"horror\", \"early access\"],\n",
    "    }\n",
    "    steam_text_info = text_features_fit(X_train, steam_kw)\n",
    "    X_test = text_features_apply(X_test, steam_text_info)\n",
    "    if verbose:\n",
    "        timer.tick(\"steam text features\")\n",
    "        show_shape_mem(\"steam after text\", X_train, X_test)\n",
    "\n",
    "    if \"tags\" in X_train.columns:\n",
    "        from collections import Counter\n",
    "\n",
    "        def tag_col_name(t):\n",
    "            s = str(t).lower().strip().replace(\" \", \"_\")\n",
    "            return \"tag_\" + \"\".join(ch for ch in s if ch.isalnum() or ch == \"_\")[:60]\n",
    "\n",
    "        cnt = Counter()\n",
    "        for v in X_train[\"tags\"].fillna(\"\").values:\n",
    "            lst = v if isinstance(v, list) else []\n",
    "            for t in lst:\n",
    "                cnt[t] += 1\n",
    "\n",
    "        items = [(t, n) for t, n in cnt.items() if n >= steam_hp[\"tag_min_count\"]]\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        vocab = [t for t, _ in items[:steam_hp[\"tag_top_k\"]]]\n",
    "        tag_cols = [tag_col_name(t) for t in vocab]\n",
    "        if verbose:\n",
    "            print(f\"[info] steam tags unique={len(cnt)}, kept={len(vocab)} (min_count={steam_hp['tag_min_count']}, top_k={steam_hp['tag_top_k']})\")\n",
    "\n",
    "        def add_tag_cols_fast(df):\n",
    "            if \"tags\" in df.columns:\n",
    "                tag_lists = df[\"tags\"].apply(lambda v: v if isinstance(v, list) else [])\n",
    "            else:\n",
    "                tag_lists = pd.Series([[]] * len(df), index=df.index)\n",
    "            new_data = {}\n",
    "            for tag, col_name in zip(vocab, tag_cols):\n",
    "                new_data[col_name] = np.fromiter(\n",
    "                    (1 if tag in lst else 0 for lst in tag_lists),\n",
    "                    dtype=np.uint8,\n",
    "                    count=len(df)\n",
    "                )\n",
    "            new_df = pd.DataFrame(new_data, index=df.index)\n",
    "            return pd.concat([df.drop(columns=[\"tags\"], errors=\"ignore\"), new_df], axis=1)\n",
    "\n",
    "        X_train = add_tag_cols_fast(X_train)\n",
    "        X_test = add_tag_cols_fast(X_test)\n",
    "        if verbose:\n",
    "            timer.tick(\"steam tags multi-hot\")\n",
    "            show_shape_mem(\"steam after tags\", X_train, X_test)\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=steam_hp[\"ohe_top_k_per_col\"],\n",
    "        min_freq_per_col=steam_hp[\"ohe_min_freq_per_col\"],\n",
    "        auto_exclude=steam_hp[\"ohe_auto_exclude\"],\n",
    "        high_card_threshold=steam_hp[\"ohe_high_card_threshold\"],\n",
    "        long_text_avglen=steam_hp[\"ohe_long_text_avglen\"],\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"steam OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "    if verbose:\n",
    "        print(\"steam non-numeric after OHE:\", X_train.select_dtypes(exclude=[\"number\"]).columns.tolist())\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"steam impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    steam_squares = []\n",
    "    if \"log_hours\" in X_train.columns:\n",
    "        steam_squares.append(\"log_hours\")\n",
    "    elif \"hours\" in X_train.columns:\n",
    "        steam_squares.append(\"hours\")\n",
    "    if \"discount\" in X_train.columns:\n",
    "        steam_squares.append(\"discount\")\n",
    "    if \"days_since_release\" in X_train.columns:\n",
    "        steam_squares.append(\"days_since_release\")\n",
    "\n",
    "    steam_pairs = []\n",
    "    if (\"price_final\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_final\", \"discount\"))\n",
    "    if (\"price_original\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_original\", \"discount\"))\n",
    "    if (\"days_since_release\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"days_since_release\", \"discount\"))\n",
    "    if (\"user_reviews\" in X_train.columns) and (\"reviews\" in X_train.columns):\n",
    "        steam_pairs.append((\"user_reviews\", \"reviews\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, steam_squares, steam_pairs)\n",
    "    timer.tick(\"steam poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=steam_hp[\"outlier_lower_q\"],\n",
    "        upper_q=steam_hp[\"outlier_upper_q\"],\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test = outlier_clip_inplace(X_test, bounds)\n",
    "    timer.tick(\"steam outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, steam_hp[\"scale_method\"])\n",
    "    timer.tick(\"steam scale\")\n",
    "\n",
    "    keep_cols = select_features(\n",
    "        steam_hp[\"feature_selection\"],\n",
    "        steam_hp[\"max_features\"],\n",
    "        task_type,\n",
    "        random_state,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"steam select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"steam\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- OLIST ----------\n",
    "    olist = olist_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        olist[\"target\"] = (olist[\"review_score_mean_product\"] >= float(olist_hp[\"cutoff\"])).astype(int)\n",
    "    else:\n",
    "        olist[\"target\"] = olist[\"review_score_mean_product\"]\n",
    "    olist.dropna(subset=[\"target\"], inplace=True)\n",
    "    olist.drop(columns=[\"order_id\", \"customer_id\", \"customer_unique_id\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    olist[\"delivery_delay\"] = (olist[\"order_estimated_delivery_date\"] - olist[\"order_purchase_timestamp\"]).dt.days\n",
    "    denom = olist[\"payment_installments_max\"].replace(0, 1)\n",
    "    olist[\"avg_installment\"] = olist[\"payment_value_total\"] / denom\n",
    "\n",
    "    if {\"product_length_cm\", \"product_width_cm\", \"product_height_cm\"}.issubset(olist.columns):\n",
    "        olist[\"product_volume_cm3\"] = (\n",
    "            olist[\"product_length_cm\"] * olist[\"product_width_cm\"] * olist[\"product_height_cm\"]\n",
    "        )\n",
    "\n",
    "    X = olist.drop(columns=[\"review_score_mean_product\", \"target\"], errors=\"ignore\")\n",
    "    y = olist[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"olist split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"olist datetime to numeric\")\n",
    "\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test, exclude_cols=[\"tags\"])\n",
    "\n",
    "    olist_kw = {\n",
    "        \"order_status\": [\"delivered\", \"shipped\", \"canceled\", \"invoiced\", \"processing\"],\n",
    "        \"product_category_name\": [\"moveis\", \"auto\", \"pet\", \"perfumaria\", \"utilidades\", \"brinquedos\"]\n",
    "    }\n",
    "    olist_text_info = text_features_fit(X_train, olist_kw)\n",
    "    X_test = text_features_apply(X_test, olist_text_info)\n",
    "    timer.tick(\"olist text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=olist_hp[\"ohe_top_k_per_col\"],\n",
    "        min_freq_per_col=olist_hp[\"ohe_min_freq_per_col\"],\n",
    "        auto_exclude=olist_hp[\"ohe_auto_exclude\"],\n",
    "        high_card_threshold=olist_hp[\"ohe_high_card_threshold\"],\n",
    "        long_text_avglen=olist_hp[\"ohe_long_text_avglen\"],\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"olist OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"olist impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    olist_squares = []\n",
    "    if \"delivery_delay\" in X_train.columns:\n",
    "        olist_squares.append(\"delivery_delay\")\n",
    "    if \"price\" in X_train.columns:\n",
    "        olist_squares.append(\"price\")\n",
    "    if \"freight_value\" in X_train.columns:\n",
    "        olist_squares.append(\"freight_value\")\n",
    "\n",
    "    olist_pairs = []\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_weight_g\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_weight_g\"))\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_volume_cm3\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_volume_cm3\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"price\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"freight_value\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"freight_value\"))\n",
    "    if (\"payment_installments_max\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"payment_installments_max\", \"price\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, olist_squares, olist_pairs)\n",
    "    timer.tick(\"olist poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=olist_hp[\"outlier_lower_q\"],\n",
    "        upper_q=olist_hp[\"outlier_upper_q\"],\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test = outlier_clip_inplace(X_test, bounds)\n",
    "    timer.tick(\"olist outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, olist_hp[\"scale_method\"])\n",
    "    timer.tick(\"olist scale\")\n",
    "\n",
    "    keep_cols = select_features(\n",
    "        olist_hp[\"feature_selection\"],\n",
    "        olist_hp[\"max_features\"],\n",
    "        task_type,\n",
    "        random_state,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"olist select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"olist\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- SALES ----------\n",
    "    sales = sales_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        sales[\"target\"] = (sales[\"Critic_Score\"] >= float(sales_hp[\"cutoff\"])).astype(int)\n",
    "    else:\n",
    "        sales[\"target\"] = sales[\"Critic_Score\"]\n",
    "    sales.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    for c in [\"ESRB_Rating\", \"Genre\", \"Platform\", \"Publisher\", \"Developer\"]:\n",
    "        if c in sales.columns:\n",
    "            sales[c] = sales[c].fillna(\"Unknown\")\n",
    "\n",
    "    X = sales.drop(columns=[\"target\", \"Critic_Score\"], errors=\"ignore\")\n",
    "    y = sales[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"sales split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"sales datetime to numeric\")\n",
    "\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test, exclude_cols=[\"tags\"])\n",
    "\n",
    "    sales_kw = {\n",
    "        \"Name\": [\"mario\", \"pokemon\", \"zelda\", \"call of duty\", \"fifa\", \"minecraft\", \"final fantasy\"],\n",
    "        \"Genre\": [\"action\", \"sports\", \"shooter\", \"racing\", \"role\", \"adventure\", \"platform\", \"puzzle\"],\n",
    "        \"Publisher\": [\"nintendo\", \"electronic arts\", \"ea\", \"activision\", \"ubisoft\", \"sony\", \"sega\"],\n",
    "        \"ESRB_Rating\": [\"e\", \"t\", \"m\"]\n",
    "    }\n",
    "    sales_text_info = text_features_fit(X_train, sales_kw)\n",
    "    X_test = text_features_apply(X_test, sales_text_info)\n",
    "    timer.tick(\"sales text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=sales_hp[\"ohe_top_k_per_col\"],\n",
    "        min_freq_per_col=sales_hp[\"ohe_min_freq_per_col\"],\n",
    "        auto_exclude=sales_hp[\"ohe_auto_exclude\"],\n",
    "        high_card_threshold=sales_hp[\"ohe_high_card_threshold\"],\n",
    "        long_text_avglen=sales_hp[\"ohe_long_text_avglen\"],\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"sales OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"sales impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    sales_squares = []\n",
    "    if \"Year\" in X_train.columns:\n",
    "        sales_squares.append(\"Year\")\n",
    "    if \"User_Score\" in X_train.columns:\n",
    "        sales_squares.append(\"User_Score\")\n",
    "\n",
    "    sales_pairs = []\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"PAL_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"PAL_Sales\"))\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"JP_Sales\"))\n",
    "    if (\"PAL_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"PAL_Sales\", \"JP_Sales\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, sales_squares, sales_pairs)\n",
    "    timer.tick(\"sales poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=sales_hp[\"outlier_lower_q\"],\n",
    "        upper_q=sales_hp[\"outlier_upper_q\"],\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test = outlier_clip_inplace(X_test, bounds)\n",
    "    timer.tick(\"sales outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, sales_hp[\"scale_method\"])\n",
    "    timer.tick(\"sales scale\")\n",
    "\n",
    "    keep_cols = select_features(\n",
    "        sales_hp[\"feature_selection\"],\n",
    "        sales_hp[\"max_features\"],\n",
    "        task_type,\n",
    "        random_state,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"sales select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"sales\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    if verbose:\n",
    "        for name, parts in outputs.items():\n",
    "            Xtr, Xte, ytr, yte = parts\n",
    "            print(f\"[{name}] X_train: {Xtr.shape} | X_test: {Xte.shape} | y_train: {ytr.shape} | y_test: {yte.shape}\")\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Download Paths\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "\n",
    "# Load All\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# Download Shapes\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e10d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Task: classification ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3267d884ed4dbb811a2d3813f018e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting (classification):   0%|          | 0/72 [00:00<?, ?fit/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Dataset: steam | X_train: (40000, 228) | X_test: (10000, 228)\n",
      "  - k=all features (cols=228)\n",
      "     > RandomForest           | n_iter=10 | cv=3 ...\n",
      "       done in 301.25 s | best_cv=0.973267 | test_f1=0.980230\n",
      "     > ExtraTrees             | n_iter=10 | cv=3 ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 476\u001b[39m\n\u001b[32m    455\u001b[39m focus = {\n\u001b[32m    456\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    457\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mExtraTrees\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGradientBoosting\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    465\u001b[39m     }\n\u001b[32m    466\u001b[39m }\n\u001b[32m    468\u001b[39m skip = {\n\u001b[32m    469\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLogisticRegression\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLinearSVC\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    470\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAdaBoostClassifier\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAdaBoostRegressor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mKNeighborsClassifier\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKNeighborsRegressor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m results = \u001b[43mtune_and_report\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteam_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43molist_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43molist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43msales_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43msales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter_best\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.60\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfs_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtree\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstandard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_prep\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheavy_models_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfocus_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfocus\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 378\u001b[39m, in \u001b[36mtune_and_report\u001b[39m\u001b[34m(steam_df, olist_df, sales_df, n_iter, n_iter_best, cv, test_size, random_state, k_grid, fs_method, scale_method, verbose_prep, suppress_warnings, heavy_models_sample, skip_models, focus_models)\u001b[39m\n\u001b[32m    375\u001b[39m     label += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m | sample=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_use)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    376\u001b[39m pbar.write(label + \u001b[33m\"\u001b[39m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m search, took = \u001b[43mrs_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter_here\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m    381\u001b[39m cv_score = \u001b[38;5;28mfloat\u001b[39m(search.best_score_) \u001b[38;5;28;01mif\u001b[39;00m search.best_score_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mtune_and_report.<locals>.rs_fit\u001b[39m\u001b[34m(est, space, scoring, X, y, n_iter_override)\u001b[39m\n\u001b[32m     80\u001b[39m search = RandomizedSearchCV(\n\u001b[32m     81\u001b[39m     estimator=est,\n\u001b[32m     82\u001b[39m     param_distributions=space,\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     error_score=np.nan\n\u001b[32m     91\u001b[39m )\n\u001b[32m     92\u001b[39m t0 = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m search, time.perf_counter() - t0\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    HistGradientBoostingClassifier, HistGradientBoostingRegressor,\n",
    "    BaggingClassifier, BaggingRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "# Note: assumes select_features(...) and prepare_data(...) exist.\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Tuning function (with tqdm)\n",
    "# =========================\n",
    "def tune_and_report(\n",
    "    steam_df,\n",
    "    olist_df,\n",
    "    sales_df,\n",
    "    n_iter=12,\n",
    "    n_iter_best=40,\n",
    "    cv=3,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    k_grid=(None, 0.15, 0.30, 0.60),\n",
    "    fs_method=\"tree\",\n",
    "    scale_method=\"standard\",\n",
    "    verbose_prep=False,\n",
    "    suppress_warnings=True,\n",
    "    heavy_models_sample=25000,\n",
    "    skip_models=None,\n",
    "    focus_models=None\n",
    "):\n",
    "    if suppress_warnings:\n",
    "        os.environ[\"PYTHONWARNINGS\"] = os.environ.get(\"PYTHONWARNINGS\", \"ignore\")\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        warnings.filterwarnings(\"ignore\", message=\"The max_iter was reached which means the coef_ did not converge\")\n",
    "        warnings.filterwarnings(\"ignore\", message=\"'n_jobs' > 1 does not have any effect\")\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    def resolve_k(total_cols, k):\n",
    "        if k is None:\n",
    "            return total_cols\n",
    "        if isinstance(k, float) and 0 < k < 1:\n",
    "            return max(1, int(round(k * total_cols)))\n",
    "        if isinstance(k, int):\n",
    "            return max(1, min(total_cols, k))\n",
    "        return total_cols\n",
    "\n",
    "    def choose_cols(X, y, task_type, k):\n",
    "        k_use = resolve_k(X.shape[1], k)\n",
    "        if k_use >= X.shape[1]:\n",
    "            return list(X.columns)\n",
    "        cols = select_features(\n",
    "            method=fs_method,\n",
    "            max_features=int(k_use),\n",
    "            task_type=task_type,\n",
    "            random_state=random_state,\n",
    "            X_train=X,\n",
    "            y_train=y,\n",
    "            verbose=False\n",
    "        )\n",
    "        return cols if cols else list(X.columns)\n",
    "\n",
    "    def rs_fit(est, space, scoring, X, y, n_iter_override=None):\n",
    "        trials = n_iter_override if n_iter_override is not None else n_iter\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=est,\n",
    "            param_distributions=space,\n",
    "            n_iter=trials,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            refit=True,\n",
    "            verbose=0,\n",
    "            error_score=np.nan\n",
    "        )\n",
    "        t0 = time.perf_counter()\n",
    "        search.fit(X, y)\n",
    "        return search, time.perf_counter() - t0\n",
    "\n",
    "    def rf_clf_space(_X):\n",
    "        return {\n",
    "            \"n_estimators\": [600, 1000, 1500, 2000],\n",
    "            \"max_depth\": [None, 12, 24, 40, 80],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "            \"max_features\": [\"sqrt\", \"log2\", 0.6, 0.8, None],\n",
    "            \"bootstrap\": [True, False]\n",
    "        }\n",
    "\n",
    "    def rf_reg_space(_X):\n",
    "        return {\n",
    "            \"n_estimators\": [600, 1000, 1500, 2000],\n",
    "            \"max_depth\": [None, 10, 20, 40, 80],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "            \"max_features\": [\"sqrt\", \"log2\", 0.6, 0.8, None],\n",
    "            \"bootstrap\": [True, False]\n",
    "        }\n",
    "\n",
    "    def et_clf_space(_X):\n",
    "        return {\n",
    "            \"n_estimators\": [600, 1000, 1500, 2000],\n",
    "            \"max_depth\": [None, 12, 24, 40],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "            \"max_features\": [\"sqrt\", \"log2\", 0.6, 0.8, None]\n",
    "        }\n",
    "\n",
    "    def et_reg_space(_X):\n",
    "        return {\n",
    "            \"n_estimators\": [600, 1000, 1500, 2000],\n",
    "            \"max_depth\": [None, 10, 20, 40],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "            \"max_features\": [\"sqrt\", \"log2\", 0.6, 0.8, None]\n",
    "        }\n",
    "\n",
    "    def gbc_space(_X):\n",
    "        return {\n",
    "            \"n_estimators\": [400, 800, 1200],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "            \"max_depth\": [2, 3, 4, 6],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"max_features\": [\"sqrt\", 0.8, None]\n",
    "        }\n",
    "\n",
    "    def hgb_clf_space(_X):\n",
    "        return {\n",
    "            \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "            \"max_depth\": [None, 3, 6, 9],\n",
    "            \"max_leaf_nodes\": [31, 63, 127, 255],\n",
    "            \"min_samples_leaf\": [5, 10, 20, 40],\n",
    "            \"l2_regularization\": [0.0, 0.001, 0.01],\n",
    "            \"max_bins\": [255]\n",
    "        }\n",
    "\n",
    "    def hgb_reg_space(_X):\n",
    "        return {\n",
    "            \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "            \"max_depth\": [None, 3, 6, 9],\n",
    "            \"max_leaf_nodes\": [31, 63, 127, 255],\n",
    "            \"min_samples_leaf\": [5, 10, 20, 40],\n",
    "            \"l2_regularization\": [0.0, 0.001, 0.01],\n",
    "            \"max_bins\": [255]\n",
    "        }\n",
    "\n",
    "    def bag_clf_space(_X):\n",
    "        return {\n",
    "            \"n_estimators\": [200, 400, 800],\n",
    "            \"max_samples\": [0.5, 0.8, 1.0],\n",
    "            \"max_features\": [0.5, 0.8, 1.0],\n",
    "            \"bootstrap\": [True, False],\n",
    "            \"bootstrap_features\": [False, True]\n",
    "        }\n",
    "\n",
    "    def bag_reg_space(_X):\n",
    "        return {\n",
    "            \"n_estimators\": [200, 400, 800],\n",
    "            \"max_samples\": [0.5, 0.8, 1.0],\n",
    "            \"max_features\": [0.5, 0.8, 1.0],\n",
    "            \"bootstrap\": [True, False],\n",
    "            \"bootstrap_features\": [False, True]\n",
    "        }\n",
    "\n",
    "    clf_models = {\n",
    "        \"RandomForest\": {\n",
    "            \"make\": lambda X: RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "            \"space\": rf_clf_space\n",
    "        },\n",
    "        \"ExtraTrees\": {\n",
    "            \"make\": lambda X: ExtraTreesClassifier(random_state=random_state, n_jobs=-1),\n",
    "            \"space\": et_clf_space\n",
    "        },\n",
    "        \"GradientBoosting\": {\n",
    "            \"make\": lambda X: GradientBoostingClassifier(random_state=random_state),\n",
    "            \"space\": gbc_space\n",
    "        },\n",
    "        \"HistGradientBoosting\": {\n",
    "            \"make\": lambda X: HistGradientBoostingClassifier(random_state=random_state),\n",
    "            \"space\": hgb_clf_space\n",
    "        },\n",
    "        \"BaggingClassifier\": {\n",
    "            \"make\": lambda X: BaggingClassifier(random_state=random_state, n_jobs=-1),\n",
    "            \"space\": bag_clf_space\n",
    "        },\n",
    "        \"DecisionTreeClassifier\": {\n",
    "            \"make\": lambda X: DecisionTreeClassifier(random_state=random_state),\n",
    "            \"space\": lambda X: {\n",
    "                \"max_depth\": [None, 10, 20, 40],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "                \"criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    reg_models = {\n",
    "        \"RandomForest\": {\n",
    "            \"make\": lambda X: RandomForestRegressor(random_state=random_state, n_jobs=-1),\n",
    "            \"space\": rf_reg_space\n",
    "        },\n",
    "        \"ExtraTrees\": {\n",
    "            \"make\": lambda X: ExtraTreesRegressor(random_state=random_state, n_jobs=-1),\n",
    "            \"space\": et_reg_space\n",
    "        },\n",
    "        \"HistGradientBoosting\": {\n",
    "            \"make\": lambda X: HistGradientBoostingRegressor(random_state=random_state),\n",
    "            \"space\": hgb_reg_space\n",
    "        },\n",
    "        \"BaggingRegressor\": {\n",
    "            \"make\": lambda X: BaggingRegressor(random_state=random_state, n_jobs=-1),\n",
    "            \"space\": bag_reg_space\n",
    "        },\n",
    "        \"DecisionTreeRegressor\": {\n",
    "            \"make\": lambda X: DecisionTreeRegressor(random_state=random_state),\n",
    "            \"space\": lambda X: {\n",
    "                \"max_depth\": [None, 10, 20, 40],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4],\n",
    "                \"criterion\": [\"squared_error\", \"absolute_error\", \"friedman_mse\"]\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        from xgboost import XGBClassifier, XGBRegressor\n",
    "        clf_models[\"XGBoost\"] = {\n",
    "            \"make\": lambda X: XGBClassifier(\n",
    "                n_jobs=-1, random_state=random_state, tree_method=\"hist\", eval_metric=\"logloss\"\n",
    "            ),\n",
    "            \"space\": lambda X: {\n",
    "                \"n_estimators\": [400, 800, 1200],\n",
    "                \"max_depth\": [3, 6, 9],\n",
    "                \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "                \"subsample\": [0.6, 0.8, 1.0],\n",
    "                \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "                \"reg_alpha\": [0.0, 0.001, 0.01],\n",
    "                \"reg_lambda\": [1.0, 3.0, 10.0]\n",
    "            }\n",
    "        }\n",
    "        reg_models[\"XGBoost\"] = {\n",
    "            \"make\": lambda X: XGBRegressor(\n",
    "                n_jobs=-1, random_state=random_state, tree_method=\"hist\"\n",
    "            ),\n",
    "            \"space\": lambda X: {\n",
    "                \"n_estimators\": [600, 1200, 1800],\n",
    "                \"max_depth\": [3, 6, 9],\n",
    "                \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "                \"subsample\": [0.6, 0.8, 1.0],\n",
    "                \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "                \"reg_alpha\": [0.0, 0.001, 0.01],\n",
    "                \"reg_lambda\": [1.0, 3.0, 10.0]\n",
    "            }\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        clf_models[\"LightGBM\"] = {\n",
    "            \"make\": lambda X: lgb.LGBMClassifier(random_state=random_state, n_estimators=400, n_jobs=-1),\n",
    "            \"space\": lambda X: {\n",
    "                \"n_estimators\": [400, 800, 1200],\n",
    "                \"num_leaves\": [31, 63, 127, 255],\n",
    "                \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "                \"subsample\": [0.6, 0.8, 1.0],\n",
    "                \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "                \"reg_alpha\": [0.0, 0.001, 0.01],\n",
    "                \"reg_lambda\": [0.0, 1.0, 3.0]\n",
    "            }\n",
    "        }\n",
    "        reg_models[\"LightGBM\"] = {\n",
    "            \"make\": lambda X: lgb.LGBMRegressor(random_state=random_state, n_estimators=600, n_jobs=-1),\n",
    "            \"space\": lambda X: {\n",
    "                \"n_estimators\": [600, 1200, 1800],\n",
    "                \"num_leaves\": [31, 63, 127, 255],\n",
    "                \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "                \"subsample\": [0.6, 0.8, 1.0],\n",
    "                \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "                \"reg_alpha\": [0.0, 0.001, 0.01],\n",
    "                \"reg_lambda\": [0.0, 1.0, 3.0]\n",
    "            }\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if skip_models is None:\n",
    "        skip_models = set()\n",
    "\n",
    "    datasets = {\"steam\": steam_df, \"olist\": olist_df, \"sales\": sales_df}\n",
    "    t0_global = time.perf_counter()\n",
    "    results = {}\n",
    "\n",
    "    for task in [\"classification\", \"regression\"]:\n",
    "        print(f\"\\n=== Task: {task} ===\")\n",
    "        prepared = prepare_data(\n",
    "            steam_df=datasets[\"steam\"],\n",
    "            olist_df=datasets[\"olist\"],\n",
    "            sales_df=datasets[\"sales\"],\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            task_type=task,\n",
    "            scale_method=scale_method,\n",
    "            use_best_from_sweep=True,\n",
    "            verbose=verbose_prep\n",
    "        )\n",
    "        scoring = \"f1_macro\" if task == \"classification\" else \"neg_mean_absolute_error\"\n",
    "\n",
    "        registry = clf_models if task == \"classification\" else reg_models\n",
    "        if focus_models and isinstance(focus_models, dict) and task in focus_models:\n",
    "            keep = set(focus_models[task])\n",
    "            registry = {k: v for k, v in registry.items() if k in keep}\n",
    "\n",
    "        model_names = [m for m in registry.keys() if m not in skip_models]\n",
    "        total_fits_task = len(prepared) * len(k_grid) * len(model_names)\n",
    "\n",
    "        with tqdm(total=total_fits_task, desc=f\"Fitting ({task})\", unit=\"fit\") as pbar:\n",
    "            for ds_name, parts in prepared.items():\n",
    "                X_train, X_test, y_train, y_test = parts\n",
    "                pbar.write(f\">> Dataset: {ds_name} | X_train: {tuple(X_train.shape)} | X_test: {tuple(X_test.shape)}\")\n",
    "\n",
    "                best_overall = {\"cv_score\": None, \"test_score\": None, \"model_name\": None, \"k\": None, \"best_params\": None, \"best_estimator\": None}\n",
    "                per_model_best = {}\n",
    "\n",
    "                for k in k_grid:\n",
    "                    cols = choose_cols(X_train, y_train, task, k)\n",
    "                    Xtr = X_train[cols]\n",
    "                    Xte = X_test[cols]\n",
    "                    k_tag_val = resolve_k(X_train.shape[1], k)\n",
    "                    k_tag = \"all\" if k_tag_val >= X_train.shape[1] else k_tag_val\n",
    "                    pbar.write(f\"  - k={k_tag} features (cols={len(cols)})\")\n",
    "\n",
    "                    heavy_names = {\"SVC\", \"SVR\", \"KNeighborsClassifier\", \"KNeighborsRegressor\", \"MLPClassifier\", \"MLPRegressor\"}\n",
    "\n",
    "                    for model_name in model_names:\n",
    "                        cfg = registry[model_name]\n",
    "\n",
    "                        X_use, y_use = Xtr, y_train\n",
    "                        use_small = False\n",
    "                        if model_name in heavy_names and heavy_models_sample is not None and len(Xtr) > heavy_models_sample:\n",
    "                            rng = np.random.RandomState(random_state)\n",
    "                            idx = rng.choice(len(Xtr), size=heavy_models_sample, replace=False)\n",
    "                            X_use = Xtr.iloc[idx]\n",
    "                            y_use = y_train.iloc[idx]\n",
    "                            use_small = True\n",
    "\n",
    "                        est = cfg[\"make\"](X_use)\n",
    "                        space = cfg[\"space\"](X_use) if callable(cfg[\"space\"]) else cfg[\"space\"]\n",
    "\n",
    "                        best_families = {\n",
    "                            \"RandomForest\", \"ExtraTrees\", \"GradientBoosting\", \"HistGradientBoosting\",\n",
    "                            \"BaggingClassifier\", \"BaggingRegressor\", \"DecisionTreeClassifier\", \"DecisionTreeRegressor\",\n",
    "                            \"XGBoost\", \"LightGBM\"\n",
    "                        }\n",
    "                        n_iter_here = n_iter_best if model_name in best_families else n_iter\n",
    "\n",
    "                        label = f\"     > {model_name:<22s} | n_iter={n_iter_here} | cv={cv}\"\n",
    "                        if use_small:\n",
    "                            label += f\" | sample={len(X_use)}\"\n",
    "                        pbar.write(label + \" ...\")\n",
    "\n",
    "                        search, took = rs_fit(est, space, scoring, X_use, y_use, n_iter_override=n_iter_here)\n",
    "                        pbar.update(1)\n",
    "\n",
    "                        cv_score = float(search.best_score_) if search.best_score_ is not None else float(\"-inf\")\n",
    "\n",
    "                        try:\n",
    "                            y_pred = search.best_estimator_.predict(Xte)\n",
    "                            if task == \"classification\":\n",
    "                                test_score = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "                            else:\n",
    "                                test_score = float(mean_absolute_error(y_test, y_pred))\n",
    "                        except Exception:\n",
    "                            test_score = float(\"nan\")\n",
    "\n",
    "                        if task == \"classification\":\n",
    "                            pbar.write(f\"       done in {took:.2f} s | best_cv={cv_score:.6f} | test_f1={test_score:.6f}\")\n",
    "                        else:\n",
    "                            pbar.write(f\"       done in {took:.2f} s | best_cv={cv_score:.6f} | test_mae={test_score:.6f}\")\n",
    "\n",
    "                        prev = per_model_best.get(model_name)\n",
    "                        if prev is None or cv_score > prev[\"cv_score\"]:\n",
    "                            per_model_best[model_name] = {\n",
    "                                \"cv_score\": cv_score,\n",
    "                                \"test_score\": test_score,\n",
    "                                \"k\": k,\n",
    "                                \"best_params\": search.best_params_,\n",
    "                                \"best_estimator\": search.best_estimator_\n",
    "                            }\n",
    "\n",
    "                        if (best_overall[\"cv_score\"] is None) or (cv_score > best_overall[\"cv_score\"]):\n",
    "                            best_overall = {\n",
    "                                \"cv_score\": cv_score,\n",
    "                                \"test_score\": test_score,\n",
    "                                \"model_name\": model_name,\n",
    "                                \"k\": k,\n",
    "                                \"best_params\": search.best_params_,\n",
    "                                \"best_estimator\": search.best_estimator_\n",
    "                            }\n",
    "\n",
    "                if task == \"classification\":\n",
    "                    pbar.write(\n",
    "                        f\"\\n   >>> Winner: {best_overall['model_name']} | k={('all' if resolve_k(X_train.shape[1], best_overall['k']) >= X_train.shape[1] else resolve_k(X_train.shape[1], best_overall['k']))} \"\n",
    "                        f\"| F1_macro_cv={best_overall['cv_score']:.6f} | F1_macro_test={best_overall['test_score']:.6f}\"\n",
    "                    )\n",
    "                else:\n",
    "                    mae_cv = -best_overall[\"cv_score\"] if best_overall[\"cv_score\"] is not None else float(\"nan\")\n",
    "                    mae_test = best_overall[\"test_score\"]\n",
    "                    pbar.write(\n",
    "                        f\"\\n   >>> Winner: {best_overall['model_name']} | k={('all' if resolve_k(X_train.shape[1], best_overall['k']) >= X_train.shape[1] else resolve_k(X_train.shape[1], best_overall['k']))} \"\n",
    "                        f\"| MAE_cv={mae_cv:.6f} | MAE_test={mae_test:.6f}\"\n",
    "                    )\n",
    "\n",
    "                pbar.write(\"   --- Per-model best (CV + Test) ---\")\n",
    "                if task == \"classification\":\n",
    "                    for mname, rec in per_model_best.items():\n",
    "                        pbar.write(\n",
    "                            f\"   {mname:22s} | k={('all' if resolve_k(X_train.shape[1], rec['k']) >= X_train.shape[1] else resolve_k(X_train.shape[1], rec['k'])):>4} \"\n",
    "                            f\"| F1_macro_cv={rec['cv_score']:.6f} | F1_macro_test={rec['test_score']:.6f}\"\n",
    "                        )\n",
    "                else:\n",
    "                    for mname, rec in per_model_best.items():\n",
    "                        mae_cv = -rec[\"cv_score\"]\n",
    "                        mae_test = rec[\"test_score\"]\n",
    "                        pbar.write(\n",
    "                            f\"   {mname:22s} | k={('all' if resolve_k(X_train.shape[1], rec['k']) >= X_train.shape[1] else resolve_k(X_train.shape[1], rec['k'])):>4} \"\n",
    "                            f\"| MAE_cv={mae_cv:.6f} | MAE_test={mae_test:.6f}\"\n",
    "                        )\n",
    "\n",
    "                results.setdefault(task, {})[ds_name] = {\"winner\": best_overall, \"per_model_best\": per_model_best}\n",
    "\n",
    "    print(f\"\\nAll searches complete in {time.perf_counter() - t0_global:.2f} s.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run it\n",
    "# =========================\n",
    "focus = {\n",
    "    \"classification\": {\n",
    "        \"RandomForest\", \"ExtraTrees\", \"GradientBoosting\",\n",
    "        \"HistGradientBoosting\", \"BaggingClassifier\", \"DecisionTreeClassifier\",\n",
    "        \"XGBoost\", \"LightGBM\"\n",
    "    },\n",
    "    \"regression\": {\n",
    "        \"RandomForest\", \"ExtraTrees\", \"HistGradientBoosting\",\n",
    "        \"BaggingRegressor\", \"DecisionTreeRegressor\",\n",
    "        \"XGBoost\", \"LightGBM\"\n",
    "    }\n",
    "}\n",
    "\n",
    "skip = {\n",
    "    \"LogisticRegression\", \"LinearSVC\",\n",
    "    \"AdaBoostClassifier\", \"AdaBoostRegressor\",\n",
    "    \"MLPClassifier\", \"MLPRegressor\",\n",
    "    \"SVC\", \"SVR\",\n",
    "    \"KNeighborsClassifier\", \"KNeighborsRegressor\"\n",
    "}\n",
    "\n",
    "results = tune_and_report(\n",
    "    steam_df=steam,\n",
    "    olist_df=olist,\n",
    "    sales_df=sales,\n",
    "    n_iter=10,\n",
    "    n_iter_best=10,\n",
    "    cv=3,\n",
    "    test_size=0.2,\n",
    "    random_state=random_state,\n",
    "    k_grid=(None, 0.15, 0.30, 0.60),\n",
    "    fs_method=\"tree\",\n",
    "    scale_method=\"standard\",\n",
    "    verbose_prep=False,\n",
    "    suppress_warnings=True,\n",
    "    heavy_models_sample=25000,\n",
    "    skip_models=skip,\n",
    "    focus_models=focus\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
