{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e204c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTS AND GLOBALS ####\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import (\n",
    "    f_regression,      # for regression\n",
    "    f_classif,         # for classification\n",
    "    SelectKBest,\n",
    "    mutual_info_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Global Settings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random_state = RANDOM_STATE\n",
    "N_ROWS = 50_000\n",
    "\n",
    "# No scientific notation in pandas display\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "# Reproducibility for NumPy-based randomness\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892baa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Small Helpers (updated)\n",
    "# =============================\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "    return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bd5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99b1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Model builder + tuner (with oversampling option)\n",
    "# =============================\n",
    "def build_and_tune_models(X_train, y_train, task_type, num_folds, num_iterations, oversample=False, oversample_method=\"random\"):\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV\n",
    "    from sklearn.pipeline import Pipeline as SKPipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    import warnings\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "    # optional oversampling tools\n",
    "    if oversample and str(task_type).strip().lower() == \"classification\":\n",
    "        try:\n",
    "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "        except Exception as e:\n",
    "            print(\"imblearn not available. Oversampling disabled.\")\n",
    "            oversample = False\n",
    "            ImbPipeline = None\n",
    "            RandomOverSampler = None\n",
    "            SMOTE = None\n",
    "\n",
    "    # print formats\n",
    "    np.set_printoptions(suppress=True)\n",
    "    pd.options.display.float_format = lambda x: f\"{x:.6f}\"\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    # drop constant columns once\n",
    "    non_constant = X_train.columns[X_train.nunique(dropna=False) > 1]\n",
    "    if len(non_constant) < X_train.shape[1]:\n",
    "        dropped = X_train.shape[1] - len(non_constant)\n",
    "        print(f\"Removed {dropped} constant feature(s).\")\n",
    "        X_train = X_train[non_constant]\n",
    "\n",
    "    task = str(task_type).strip().lower()\n",
    "    if task == \"classification\":\n",
    "        scoring = \"f1_macro\"\n",
    "        selector_score_func = f_classif\n",
    "        # cap folds by minority class count\n",
    "        min_class = int(y_train.value_counts().min())\n",
    "        eff_folds = max(2, min(int(num_folds), min_class))\n",
    "        baseline_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        # choose sampler\n",
    "        sampler_obj = None\n",
    "        if oversample:\n",
    "            if oversample_method == \"smote\":\n",
    "                # keep neighbors safe for tiny minority classes\n",
    "                k_neighbors_for_smote = max(1, min(5, min_class - 1))\n",
    "                if k_neighbors_for_smote < 1:\n",
    "                    print(\"SMOTE not possible (minority class too small). Falling back to RandomOverSampler.\")\n",
    "                    sampler_obj = RandomOverSampler(random_state=42)\n",
    "                else:\n",
    "                    sampler_obj = SMOTE(random_state=42, k_neighbors=k_neighbors_for_smote)\n",
    "            else:\n",
    "                sampler_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "        # turn off class_weight when oversampling\n",
    "        cw = None if oversample else \"balanced\"\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingClassifier(random_state=42),\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=42, class_weight=cw, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeClassifier(random_state=42, class_weight=cw),\n",
    "            \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=cw),\n",
    "            \"LinearSVM\": LinearSVC(max_iter=5000, class_weight=cw),\n",
    "            \"NaiveBayes\": GaussianNB(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"Dummy\": DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "        }\n",
    "        metric_name = \"F1_macro\"\n",
    "        higher_is_better = True\n",
    "    elif task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "        selector_score_func = f_regression\n",
    "        eff_folds = max(2, int(num_folds))\n",
    "        baseline_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingRegressor(random_state=42),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(max_iter=5000),\n",
    "            \"Lasso\": Lasso(max_iter=5000),\n",
    "            \"ElasticNet\": ElasticNet(max_iter=5000),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"Dummy\": DummyRegressor(strategy=\"mean\"),\n",
    "        }\n",
    "        metric_name = \"CV_MAE\"\n",
    "        higher_is_better = False\n",
    "        sampler_obj = None  # not used\n",
    "    else:\n",
    "        raise ValueError('task_type must be \"classification\" or \"regression\"')\n",
    "\n",
    "    total_features = X_train.shape[1]\n",
    "    feature_fractions = [0.10, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "    # scale for linear models, KNN, and Naive Bayes\n",
    "    needs_scaling = {\"LogisticRegression\", \"LinearSVM\", \"KNN\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"NaiveBayes\"}\n",
    "    skip_selection = {\"Dummy\"}\n",
    "    tree_like = {\"RandomForest\", \"DecisionTree\", \"GBT\"}\n",
    "\n",
    "    def k_from_fraction(frac, total_cols):\n",
    "        if frac >= 1.0:\n",
    "            return \"all\"\n",
    "        k = int(max(1, math.ceil(frac * total_cols)))\n",
    "        return min(k, total_cols)\n",
    "\n",
    "    # build a pipeline with optional sampler inside CV\n",
    "    def make_pipeline_for_k(model_name, model_obj, k_value):\n",
    "        select = \"passthrough\" if (model_name in skip_selection or model_name in tree_like) else SelectKBest(score_func=selector_score_func, k=k_value)\n",
    "        scale = StandardScaler() if model_name in needs_scaling else \"passthrough\"\n",
    "        if task == \"classification\" and oversample and sampler_obj is not None:\n",
    "            # order: select -> scale -> sampler -> model\n",
    "            return ImbPipeline([(\"select\", select), (\"scale\", scale), (\"sampler\", sampler_obj), (\"model\", model_obj)])\n",
    "        else:\n",
    "            return SKPipeline([(\"select\", select), (\"scale\", scale), (\"model\", model_obj)])\n",
    "\n",
    "    # dynamic cap for KNN neighbors\n",
    "    per_fold_train = int(len(X_train) * (eff_folds - 1) / eff_folds)\n",
    "    max_knn_k = max(3, min(101, per_fold_train - 1))\n",
    "    knn_ks = list(range(3, max_knn_k + 1, 2))\n",
    "\n",
    "    def logspace_list(low_exp, high_exp, num):\n",
    "        return list(np.logspace(low_exp, high_exp, num))\n",
    "\n",
    "    def linspace_list(low_val, high_val, num):\n",
    "        return list(np.linspace(low_val, high_val, num))\n",
    "\n",
    "    param_spaces_classification = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LogisticRegression\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__solver\": [\"saga\"],\n",
    "        },\n",
    "        \"LinearSVM\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        },\n",
    "        \"NaiveBayes\": {\n",
    "            \"model__var_smoothing\": list(10 ** np.linspace(-11, -7, 9))\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"most_frequent\", \"stratified\", \"uniform\"]},\n",
    "    }\n",
    "\n",
    "    param_spaces_regression = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LinearRegression\": {},\n",
    "        \"Ridge\": {\"model__alpha\": logspace_list(-3, 3, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"Lasso\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"ElasticNet\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__l1_ratio\": linspace_list(0.1, 0.9, 9), \"model__fit_intercept\": [True, False]},\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"mean\", \"median\"]},\n",
    "    }\n",
    "\n",
    "    # streaming baseline search\n",
    "    rows = []\n",
    "    total_steps = len(model_space) * len(feature_fractions)\n",
    "    step = 0\n",
    "    print(\"Streaming results (each line is one model × feature count):\")\n",
    "    for model_name, model_obj in model_space.items():\n",
    "        for frac in feature_fractions:\n",
    "            step += 1\n",
    "            k_val = k_from_fraction(frac, total_features)\n",
    "            k_print = total_features if k_val == \"all\" else int(k_val)\n",
    "            pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=baseline_cv, scoring=scoring, n_jobs=1)\n",
    "            mean_score = float(np.mean(scores))\n",
    "            std_score = float(np.std(scores))\n",
    "            if task == \"regression\":\n",
    "                display_mean = -mean_score\n",
    "                display_std = float(np.std(-scores))\n",
    "            else:\n",
    "                display_mean = mean_score\n",
    "                display_std = std_score\n",
    "            rows.append({\"Model\": model_name, \"K_features\": k_print, \"MeanScore\": display_mean, \"StdDev\": display_std, \"Metric\": metric_name})\n",
    "            print(f\"[{step}/{total_steps}] {model_name} | k={k_print} | {metric_name}={display_mean:.6f} ± {display_std:.6f}\", flush=True)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    if task == \"classification\":\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    else:\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Baseline results (CV) ===\")\n",
    "    print(results_df[[\"Model\", \"K_features\", \"MeanScore\", \"StdDev\", \"Metric\"]])\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    best_model_name = str(best_row[\"Model\"])\n",
    "    best_k = int(best_row[\"K_features\"])\n",
    "    best_model_obj = model_space[best_model_name]\n",
    "    k_val_for_search = \"all\" if best_k >= total_features else best_k\n",
    "    best_pipeline = make_pipeline_for_k(best_model_name, best_model_obj, k_val_for_search)\n",
    "\n",
    "    search_space = (param_spaces_classification if task == \"classification\" else param_spaces_regression).get(best_model_name, {})\n",
    "    if len(search_space) == 0:\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        print(\"\\nBest model had no tunable params. Returning fitted pipeline.\")\n",
    "        return best_pipeline\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=best_pipeline,\n",
    "        param_distributions=search_space,\n",
    "        n_iter=int(max(1, num_iterations)),\n",
    "        scoring=scoring,\n",
    "        cv=search_cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    if task == \"regression\":\n",
    "        tuned_score_display = -float(search.best_score_)\n",
    "        tuned_metric_name = \"CV MAE\"\n",
    "    else:\n",
    "        tuned_score_display = float(search.best_score_)\n",
    "        tuned_metric_name = \"F1 macro\"\n",
    "\n",
    "    print(\"\\n=== Best model after randomized search ===\")\n",
    "    print(f\"Model name: {best_model_name}\")\n",
    "    print(f\"Number of features: {best_k}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best CV score ({tuned_metric_name}): {tuned_score_display:.6f}\")\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Holdout evaluation helper\n",
    "# =============================\n",
    "def evaluate_on_holdout(model, X_test, y_test, task_type):\n",
    "    from sklearn.metrics import f1_score, mean_absolute_error, classification_report, confusion_matrix\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n=== Holdout (time split) ===\")\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(f\"F1 macro: {f1:.6f}\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        return f1\n",
    "    else:\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4631b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Helpers\n",
    "# =============================\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KeepTrainColumns(BaseEstimator, TransformerMixin):\n",
    "    # remembers training columns and reindexes any input to match\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.keep_columns_ = list(X.columns)\n",
    "        else:\n",
    "            self.keep_columns_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.keep_columns_ is None:\n",
    "            return X\n",
    "        if hasattr(X, \"reindex\"):\n",
    "            return X.reindex(columns=self.keep_columns_, fill_value=0)\n",
    "        return X\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model builder + tuner (with oversampling option)\n",
    "# =============================\n",
    "def build_and_tune_models(\n",
    "    X_train, y_train,\n",
    "    task_type,\n",
    "    num_folds,\n",
    "    num_iterations,\n",
    "    oversample=False,\n",
    "    oversample_method=\"random\"\n",
    "):\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV\n",
    "    from sklearn.pipeline import Pipeline as SKPipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, f_classif, VarianceThreshold\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "    # optional oversampling tools\n",
    "    ImbPipeline = None\n",
    "    RandomOverSampler = None\n",
    "    SMOTE = None\n",
    "    if oversample and str(task_type).strip().lower() == \"classification\":\n",
    "        try:\n",
    "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "        except Exception:\n",
    "            print(\"imblearn not available. Oversampling disabled.\")\n",
    "            oversample = False\n",
    "\n",
    "    # neat prints\n",
    "    np.set_printoptions(suppress=True)\n",
    "    pd.options.display.float_format = lambda x: f\"{x:.6f}\"\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    # drop constant columns once on train\n",
    "    non_constant_columns = X_train.columns[X_train.nunique(dropna=False) > 1]\n",
    "    if len(non_constant_columns) < X_train.shape[1]:\n",
    "        dropped = X_train.shape[1] - len(non_constant_columns)\n",
    "        print(f\"Removed {dropped} constant feature(s).\")\n",
    "        X_train = X_train[non_constant_columns]\n",
    "\n",
    "    # task settings\n",
    "    task = str(task_type).strip().lower()\n",
    "    if task == \"classification\":\n",
    "        scoring = \"f1_macro\"\n",
    "        selector_score_func = f_classif\n",
    "        min_class = int(y_train.value_counts().min())\n",
    "        eff_folds = max(2, min(int(num_folds), min_class))\n",
    "        baseline_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        sampler_obj = None\n",
    "        if oversample:\n",
    "            if oversample_method == \"smote\":\n",
    "                k_neighbors_for_smote = max(1, min(5, min_class - 1))\n",
    "                if k_neighbors_for_smote < 1:\n",
    "                    print(\"SMOTE not possible (minority class too small). Using RandomOverSampler.\")\n",
    "                    sampler_obj = RandomOverSampler(random_state=42)\n",
    "                else:\n",
    "                    sampler_obj = SMOTE(random_state=42, k_neighbors=k_neighbors_for_smote)\n",
    "            else:\n",
    "                sampler_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "        class_weight_choice = None if oversample else \"balanced\"\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingClassifier(random_state=42),\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=42, class_weight=class_weight_choice, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeClassifier(random_state=42, class_weight=class_weight_choice),\n",
    "            \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"LinearSVM\": LinearSVC(max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"NaiveBayes\": GaussianNB(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"Dummy\": DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "        }\n",
    "        metric_name = \"F1_macro\"\n",
    "        higher_is_better = True\n",
    "    elif task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "        selector_score_func = f_regression\n",
    "        eff_folds = max(2, int(num_folds))\n",
    "        baseline_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingRegressor(random_state=42),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(max_iter=5000),\n",
    "            \"Lasso\": Lasso(max_iter=5000),\n",
    "            \"ElasticNet\": ElasticNet(max_iter=5000),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"Dummy\": DummyRegressor(strategy=\"mean\"),\n",
    "        }\n",
    "        metric_name = \"CV_MAE\"\n",
    "        higher_is_better = False\n",
    "        sampler_obj = None\n",
    "    else:\n",
    "        raise ValueError('task_type must be \"classification\" or \"regression\"')\n",
    "\n",
    "    total_features = X_train.shape[1]\n",
    "    feature_fractions = [0.10, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "    # which models need scaling and selection\n",
    "    needs_scaling = {\"LogisticRegression\", \"LinearSVM\", \"KNN\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"NaiveBayes\"}\n",
    "    skip_selection = {\"Dummy\"}\n",
    "    tree_like = {\"RandomForest\", \"DecisionTree\", \"GBT\"}\n",
    "\n",
    "    def k_from_fraction(frac, total_cols):\n",
    "        if frac >= 1.0:\n",
    "            return \"all\"\n",
    "        k = int(max(1, math.ceil(frac * total_cols)))\n",
    "        return min(k, total_cols)\n",
    "\n",
    "    # dynamic KNN neighbors cap\n",
    "    per_fold_train = int(len(X_train) * (eff_folds - 1) / eff_folds)\n",
    "    max_knn_k = max(3, min(101, per_fold_train - 1))\n",
    "    knn_ks = list(range(3, max_knn_k + 1, 2))\n",
    "\n",
    "    def logspace_list(low_exp, high_exp, num):\n",
    "        return list(np.logspace(low_exp, high_exp, num))\n",
    "\n",
    "    def linspace_list(low_val, high_val, num):\n",
    "        return list(np.linspace(low_val, high_val, num))\n",
    "\n",
    "    param_spaces_classification = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LogisticRegression\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__solver\": [\"saga\"],\n",
    "        },\n",
    "        \"LinearSVM\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        },\n",
    "        \"NaiveBayes\": {\n",
    "            \"model__var_smoothing\": list(10 ** np.linspace(-11, -7, 9))\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"most_frequent\", \"stratified\", \"uniform\"]},\n",
    "    }\n",
    "\n",
    "    param_spaces_regression = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LinearRegression\": {},\n",
    "        \"Ridge\": {\"model__alpha\": logspace_list(-3, 3, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"Lasso\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"ElasticNet\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__l1_ratio\": linspace_list(0.1, 0.9, 9), \"model__fit_intercept\": [True, False]},\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"mean\", \"median\"]},\n",
    "    }\n",
    "\n",
    "    # build a pipeline for a given k\n",
    "    def make_pipeline_for_k(model_name, model_obj, k_value):\n",
    "        # order: align -> impute -> variance -> select -> scale -> sampler -> model\n",
    "        align_step = (\"align\", KeepTrainColumns())\n",
    "        impute_step = (\"impute\", SimpleImputer(strategy=\"median\"))\n",
    "        variance_step = (\"variance\", VarianceThreshold(threshold=0.0))\n",
    "\n",
    "        if model_name in skip_selection or model_name in tree_like:\n",
    "            select_step = (\"select\", \"passthrough\")\n",
    "        else:\n",
    "            select_step = (\"select\", SelectKBest(score_func=selector_score_func, k=k_value))\n",
    "\n",
    "        scale_step = (\"scale\", StandardScaler() if model_name in needs_scaling else \"passthrough\")\n",
    "\n",
    "        steps = [align_step, impute_step, variance_step, select_step, scale_step]\n",
    "\n",
    "        if task == \"classification\" and oversample and sampler_obj is not None:\n",
    "            steps.append((\"sampler\", sampler_obj))\n",
    "\n",
    "        steps.append((\"model\", model_obj))\n",
    "\n",
    "        if oversample and ImbPipeline is not None and task == \"classification\":\n",
    "            return ImbPipeline(steps)\n",
    "        else:\n",
    "            return SKPipeline(steps)\n",
    "\n",
    "    # baseline sweep across models × k\n",
    "    rows = []\n",
    "    total_steps = len(model_space) * len(feature_fractions)\n",
    "    step = 0\n",
    "    print(\"Streaming results (each line is one model × feature count):\")\n",
    "    for model_name, model_obj in model_space.items():\n",
    "        for frac in feature_fractions:\n",
    "            step += 1\n",
    "            k_val = k_from_fraction(frac, total_features)\n",
    "            k_print = total_features if k_val == \"all\" else int(k_val)\n",
    "            pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=baseline_cv, scoring=scoring, n_jobs=1)\n",
    "            mean_score = float(np.mean(scores))\n",
    "            std_score = float(np.std(scores))\n",
    "            if task == \"regression\":\n",
    "                display_mean = -mean_score\n",
    "                display_std = float(np.std(-scores))\n",
    "            else:\n",
    "                display_mean = mean_score\n",
    "                display_std = std_score\n",
    "            rows.append({\"Model\": model_name, \"K_features\": k_print, \"MeanScore\": display_mean, \"StdDev\": display_std, \"Metric\": metric_name})\n",
    "            print(f\"[{step}/{total_steps}] {model_name} | k={k_print} | {metric_name}={display_mean:.6f} ± {display_std:.6f}\", flush=True)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    if task == \"classification\":\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    else:\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Baseline results (CV) ===\")\n",
    "    print(results_df[[\"Model\", \"K_features\", \"MeanScore\", \"StdDev\", \"Metric\"]])\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    best_model_name = str(best_row[\"Model\"])\n",
    "    best_k = int(best_row[\"K_features\"])\n",
    "    best_model_obj = model_space[best_model_name]\n",
    "    k_val_for_search = \"all\" if best_k >= total_features else best_k\n",
    "    best_pipeline = make_pipeline_for_k(best_model_name, best_model_obj, k_val_for_search)\n",
    "\n",
    "    search_space = (param_spaces_classification if task == \"classification\" else param_spaces_regression).get(best_model_name, {})\n",
    "    if len(search_space) == 0:\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        try:\n",
    "            best_pipeline.input_columns_ = list(X_train.columns)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"\\nBest model had no tunable params. Returning fitted pipeline.\")\n",
    "        return best_pipeline\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=best_pipeline,\n",
    "        param_distributions=search_space,\n",
    "        n_iter=int(max(1, num_iterations)),\n",
    "        scoring=scoring,\n",
    "        cv=search_cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    if task == \"regression\":\n",
    "        tuned_score_display = -float(search.best_score_)\n",
    "        tuned_metric_name = \"CV MAE\"\n",
    "    else:\n",
    "        tuned_score_display = float(search.best_score_)\n",
    "        tuned_metric_name = \"F1 macro\"\n",
    "\n",
    "    print(\"\\n=== Best model after randomized search ===\")\n",
    "    print(f\"Model name: {best_model_name}\")\n",
    "    print(f\"Number of features: {best_k}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best CV score ({tuned_metric_name}): {tuned_score_display:.6f}\")\n",
    "\n",
    "    try:\n",
    "        search.best_estimator_.input_columns_ = list(X_train.columns)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Holdout evaluation helper\n",
    "# =============================\n",
    "def evaluate_on_holdout(model, X_test, y_test, task_type):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import f1_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "    # align columns to what the model saw at fit\n",
    "    try:\n",
    "        if hasattr(model, \"input_columns_\") and hasattr(X_test, \"reindex\"):\n",
    "            X_test = X_test.reindex(columns=model.input_columns_, fill_value=0)\n",
    "        elif hasattr(model, \"named_steps\") and \"align\" in getattr(model, \"named_steps\", {}):\n",
    "            keep_cols = getattr(model.named_steps[\"align\"], \"keep_columns_\", None)\n",
    "            if keep_cols is not None and hasattr(X_test, \"reindex\"):\n",
    "                X_test = X_test.reindex(columns=list(keep_cols), fill_value=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not align columns: {e}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n=== Holdout (time split) ===\")\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(f\"F1 macro: {f1:.6f}\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        return f1\n",
    "    else:\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        return mae\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Optional: simple threshold tuner for classification\n",
    "# =============================\n",
    "def choose_threshold(model, X, y, metric=\"f1_macro\"):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    # get scores\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X)\n",
    "        s_min, s_max = float(s.min()), float(s.max())\n",
    "        scores = (s - s_min) / (s_max - s_min + 1e-9)\n",
    "    else:\n",
    "        # fallback: use predictions as scores\n",
    "        scores = model.predict(X).astype(float)\n",
    "\n",
    "    best_t = 0.5\n",
    "    best_val = -1.0\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_hat = (scores >= t).astype(int)\n",
    "        val = f1_score(y, y_hat, average=\"macro\") if metric == \"f1_macro\" else f1_score(y, y_hat)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_t = t\n",
    "\n",
    "    y_hat = (scores >= best_t).astype(int)\n",
    "    print(f\"best threshold: {best_t:.2f}, {metric}: {best_val:.6f}\")\n",
    "    print(\"confusion matrix at best threshold:\")\n",
    "    print(confusion_matrix(y, y_hat))\n",
    "    return best_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a4c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all(\n",
    "    steam, olist, sales,\n",
    "    top_k_tags=200, max_total_features=400,\n",
    "    test_size=0.2, random_state=42,\n",
    "    balance_method=\"none\",\n",
    "    min_class_ratio=0.5,\n",
    "    feature_select_method=\"none\",\n",
    "    feature_select_k=100,\n",
    "    task_type=\"classification\",\n",
    "    scale_method=\"none\",\n",
    "    poly_degree=2,\n",
    "    poly_interaction_only=False,\n",
    "    poly_include_bias=False,\n",
    "    poly_feature_limit=25\n",
    "):\n",
    "    def is_sparse_dtype(dtype):\n",
    "        return isinstance(dtype, pd.SparseDtype)\n",
    "\n",
    "    is_classification = str(task_type).lower().startswith(\"c\")\n",
    "\n",
    "    # forward-selected feature lists you provided\n",
    "    best_forward_features_raw = {\n",
    "        \"steam\": [\n",
    "            \"poly_price_final_log1p^2\", \"mac\", \"poly_price_original_log1p\",\n",
    "            \"tag_'early access'\", \"tag_'great soundtrack'\", \"poly_days_since_release\",\n",
    "            \"poly_hours_log1p^2\", \"tag_'2d'\", \"tag_'massively multiplayer'\",\n",
    "            \"tag_'free to play'\", \"tag_'cute'\", \"tag_'action rpg'\",\n",
    "            \"poly_price_original_log1p^2\", \"tag_'first-person'\", \"tag_'fast-paced'\",\n",
    "            \"poly_title_len price_final_log1p\", \"poly_days_since_release review_year\",\n",
    "            \"poly_products_log1p reviews_log1p\", \"tag_'mmorpg'\", \"tag_'puzzle'\",\n",
    "            \"tag_'pvp'\", \"poly_desc_len hours_log1p\", \"poly_hours_log1p\",\n",
    "            \"tag_'management'\", \"tag_'memes'\", \"tag_'relaxing'\", \"tag_'visual novel'\",\n",
    "            \"tag_'sexual content'\", \"tag_'difficult'\", \"tag_'emotional'\"\n",
    "        ],\n",
    "        \"olist\": [\n",
    "            \"poly_to_customer_h\", \"order_status_delivered\", \"order_status_shipped\",\n",
    "            \"poly_to_carrier_h\", \"seller_state_SP\", \"order_item_id\",\n",
    "            \"product_category_bed_bath_table\", \"product_category_office_furniture\",\n",
    "            \"product_category_furniture_decor\", \"poly_customer_zip_code_prefix\",\n",
    "            \"product_category_watches_gifts\", \"product_category_computers_accessories\",\n",
    "            \"product_category_telephony\", \"product_category_books_general_interest\",\n",
    "            \"product_category_home_confort\", \"poly_to_customer_h est_delivery_h\",\n",
    "            \"poly_to_customer_h price\", \"order_status_processing\", \"seller_state_RS\",\n",
    "            \"seller_state_PE\", \"poly_payment_value_per_payment approval_delay_h\",\n",
    "            \"product_category_audio\", \"poly_product_description_lenght product_height_cm\",\n",
    "            \"product_name_lenght\", \"product_category_stationery\",\n",
    "            \"product_category_luggage_accessories\", \"product_category_electronics\",\n",
    "            \"seller_state_MA\", \"product_category_Unknown\", \"product_category_baby\"\n",
    "        ],\n",
    "        \"sales\": [\n",
    "            \"poly_Developer_freq\", \"Genre_Action\", \"ESRB_Rating_M\", \"poly_Year^2\",\n",
    "            \"Decade_1970\", \"Platform_Family_PC\", \"poly_Developer_freq^2\",\n",
    "            \"Platform_Family_Nintendo\", \"poly_Publisher_freq\",\n",
    "            \"poly_Year Publisher_freq\", \"is_remaster\", \"Genre_Party\", \"Genre_Misc\",\n",
    "            \"Genre_Simulation\", \"is_portable\", \"Genre_Role-Playing\", \"Decade_1980\",\n",
    "            \"Genre_Strategy\", \"Genre_Fighting\", \"Genre_Puzzle\", \"poly_Year\",\n",
    "            \"Decade_<NA>\", \"poly_Year Developer_freq\", \"poly_Publisher_freq^2\",\n",
    "            \"Platform_Family_PlayStation\", \"Genre_Board Game\", \"Genre_Racing\",\n",
    "            \"Genre_Adventure\", \"Genre_Shooter\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # normalize names like \"tag_'great soundtrack'\" -> \"tag_great soundtrack\"\n",
    "    def _normalize_forward_name(name):\n",
    "        s = str(name).strip()\n",
    "        if s.startswith(\"tag_\"):\n",
    "            rest = s[4:].strip()\n",
    "            if (rest.startswith(\"'\") and rest.endswith(\"'\")) or (rest.startswith('\"') and rest.endswith('\"')):\n",
    "                rest = rest[1:-1]\n",
    "            return \"tag_\" + rest.lower()\n",
    "        return s\n",
    "\n",
    "    best_forward_features = {\n",
    "        k: [_normalize_forward_name(n) for n in v]\n",
    "        for k, v in best_forward_features_raw.items()\n",
    "    }\n",
    "\n",
    "    # make poly interaction names order-agnostic\n",
    "    def canonical_poly_name(name):\n",
    "        s = str(name)\n",
    "        if not s.startswith(\"poly_\"):\n",
    "            return s\n",
    "        body = s[5:]\n",
    "        if \"^\" in body and \" \" not in body:\n",
    "            return s  # keep squares like \"poly_x^2\"\n",
    "        tokens = body.split()\n",
    "        tokens = sorted(tokens)\n",
    "        return \"poly_\" + \" \".join(tokens)\n",
    "\n",
    "    def build_poly_canonical_map(cols):\n",
    "        col_map = {}\n",
    "        for c in cols:\n",
    "            if c.startswith(\"poly_\"):\n",
    "                col_map[canonical_poly_name(c)] = c\n",
    "            else:\n",
    "                col_map[c] = c\n",
    "        return col_map\n",
    "\n",
    "    # small debug print for missing columns (uses canonical poly names)\n",
    "    def debug_forward_missing(X, dataset_name, best_forward_features):\n",
    "        expect = best_forward_features.get(dataset_name, [])\n",
    "        col_map = build_poly_canonical_map(X.columns)\n",
    "        missing = [f for f in expect if canonical_poly_name(f) not in col_map]\n",
    "        if missing:\n",
    "            print(f\"[debug] {dataset_name} missing:\", missing)\n",
    "        return missing\n",
    "\n",
    "    def drop_known_leaks_from_features(X_in, y_in=None):\n",
    "        leak_names = {\"y_is_4_plus\", \"delivered_late\", \"target\", \"label\",\n",
    "                      \"target_olist\", \"target_sales\", \"target_steam\"}\n",
    "        to_drop = [c for c in X_in.columns if (c in leak_names) or c.lower().startswith(\"target\") or c.lower().startswith(\"label\")]\n",
    "        X_in = X_in.drop(columns=to_drop, errors=\"ignore\")\n",
    "        if is_classification and (y_in is not None):\n",
    "            y_series = pd.Series(y_in).reset_index(drop=True)\n",
    "            for c in list(X_in.columns):\n",
    "                xc = pd.Series(X_in[c]).reset_index(drop=True)\n",
    "                try:\n",
    "                    ux = set(pd.unique(xc.dropna()))\n",
    "                    uy = set(pd.unique(y_series.dropna()))\n",
    "                    if ux <= {0, 1} and uy <= {0, 1}:\n",
    "                        if (xc.astype(\"int8\") == y_series.astype(\"int8\")).all():\n",
    "                            X_in = X_in.drop(columns=[c])\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return X_in\n",
    "\n",
    "    def resample_binary(Xb, yb, method=\"oversample\", random_state=42):\n",
    "        counts = yb.value_counts(dropna=False)\n",
    "        if counts.shape[0] != 2:\n",
    "            return Xb, yb\n",
    "        majority_class = counts.idxmax()\n",
    "        minority_class = counts.idxmin()\n",
    "        majority_idx = yb[yb == majority_class].index.values\n",
    "        minority_idx = yb[yb == minority_class].index.values\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        if method == \"undersample\":\n",
    "            keep_majority = rng.choice(majority_idx, size=len(minority_idx), replace=False)\n",
    "            new_index = np.concatenate([minority_idx, keep_majority])\n",
    "        else:\n",
    "            need = int(len(majority_idx) - len(minority_idx))\n",
    "            need = max(0, need)\n",
    "            add_minority = rng.choice(minority_idx, size=need, replace=True)\n",
    "            new_index = np.concatenate([majority_idx, minority_idx, add_minority])\n",
    "        rng.shuffle(new_index)\n",
    "        Xb2 = Xb.loc[new_index].reset_index(drop=True)\n",
    "        yb2 = yb.loc[new_index].reset_index(drop=True)\n",
    "        return Xb2, yb2\n",
    "\n",
    "    def split_and_balance(df_in, target_col, balance_method, min_class_ratio, random_state):\n",
    "        X_in = df_in.drop(columns=[target_col]).copy()\n",
    "        y_in = df_in[target_col].copy()\n",
    "        X_in = drop_known_leaks_from_features(X_in, y_in)\n",
    "\n",
    "        dense_num_cols = [c for c in X_in.select_dtypes(include=[np.number]).columns\n",
    "                          if not is_sparse_dtype(X_in[c].dtype)]\n",
    "        for c in dense_num_cols:\n",
    "            X_in[c] = pd.to_numeric(X_in[c], errors=\"coerce\").fillna(pd.to_numeric(X_in[c], errors=\"coerce\").median())\n",
    "        obj_cols = X_in.select_dtypes(include=[\"object\"]).columns\n",
    "        for c in obj_cols:\n",
    "            X_in[c] = X_in[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        if is_classification:\n",
    "            if pd.Series(y_in).nunique() != 2:\n",
    "                raise ValueError(f\"{target_col}: for classification, target must have 2 classes.\")\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_in, y_in.astype(int), test_size=test_size, random_state=random_state, stratify=y_in\n",
    "            )\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_in, y_in, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "\n",
    "        if is_classification:\n",
    "            vc = y_tr.value_counts()\n",
    "            ratio = float(vc.min()) / float(vc.max()) if vc.max() > 0 else 1.0\n",
    "            if balance_method != \"none\" and ratio < min_class_ratio:\n",
    "                method = (\n",
    "                    \"oversample\" if balance_method == \"auto\" and len(X_tr) <= 200000\n",
    "                    else (\"undersample\" if balance_method == \"auto\" else balance_method)\n",
    "                )\n",
    "                X_tr, y_tr = resample_binary(X_tr, y_tr, method=method, random_state=random_state)\n",
    "            print(f\"{target_col} train class counts:\", y_tr.value_counts().to_dict())\n",
    "            print(f\"{target_col} test class counts:\", y_te.value_counts().to_dict())\n",
    "        else:\n",
    "            def sstats(s):\n",
    "                return {\"n\": int(s.shape[0]),\n",
    "                        \"mean\": float(np.nanmean(s)),\n",
    "                        \"std\": float(np.nanstd(s)),\n",
    "                        \"min\": float(np.nanmin(s)),\n",
    "                        \"max\": float(np.nanmax(s))}\n",
    "            print(f\"{target_col} train stats:\", sstats(y_tr))\n",
    "            print(f\"{target_col} test stats:\", sstats(y_te))\n",
    "        return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "    def apply_feature_selection(X_tr, y_tr, X_te, method, k, random_state, dataset_name=None):\n",
    "        if method == \"none\":\n",
    "            return X_tr, X_te\n",
    "\n",
    "        if method in {\"forward_best\", \"forward\", \"best_forward\"}:\n",
    "            feature_list = best_forward_features.get(dataset_name, [])\n",
    "            if not feature_list:\n",
    "                print(f\"forward_best: no saved list for {dataset_name}, using all features.\")\n",
    "                return X_tr, X_te\n",
    "\n",
    "            col_map = build_poly_canonical_map(X_tr.columns)\n",
    "            kept_cols = []\n",
    "            for want in feature_list:\n",
    "                key = canonical_poly_name(want)\n",
    "                if key in col_map:\n",
    "                    kept_cols.append(col_map[key])\n",
    "\n",
    "            missing = [f for f in feature_list if canonical_poly_name(f) not in col_map]\n",
    "            if missing:\n",
    "                print(f\"forward_best: {dataset_name} missing {len(missing)} of {len(feature_list)} saved features.\")\n",
    "\n",
    "            if not kept_cols:\n",
    "                print(f\"forward_best: none of the saved features found for {dataset_name}, using all features.\")\n",
    "                return X_tr, X_te\n",
    "\n",
    "            if isinstance(k, int) and k > 0:\n",
    "                kept_cols = kept_cols[: min(k, len(kept_cols))]\n",
    "\n",
    "            return X_tr[kept_cols].copy(), X_te[kept_cols].copy()\n",
    "\n",
    "        n_features = X_tr.shape[1]\n",
    "        if n_features <= 1 or k >= n_features:\n",
    "            return X_tr, X_te\n",
    "        k = int(max(1, min(k, n_features)))\n",
    "        try:\n",
    "            if method == \"mi\":\n",
    "                if is_classification:\n",
    "                    sel = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "                else:\n",
    "                    sel = SelectKBest(score_func=f_regression, k=k)\n",
    "                sel.fit(X_tr, y_tr)\n",
    "                kept_cols = X_tr.columns[sel.get_support()].tolist()\n",
    "            elif method == \"rf\":\n",
    "                if is_classification:\n",
    "                    rf = RandomForestClassifier(\n",
    "                        n_estimators=300, random_state=random_state, n_jobs=-1,\n",
    "                        class_weight=\"balanced_subsample\"\n",
    "                    )\n",
    "                else:\n",
    "                    rf = RandomForestRegressor(\n",
    "                        n_estimators=300, random_state=random_state, n_jobs=-1\n",
    "                    )\n",
    "                rf.fit(X_tr, y_tr)\n",
    "                order = np.argsort(rf.feature_importances_)[::-1][:k]\n",
    "                kept_cols = X_tr.columns[order].tolist()\n",
    "            else:\n",
    "                print(\"feature selection: unknown method, skipping\")\n",
    "                return X_tr, X_te\n",
    "            return X_tr[kept_cols].copy(), X_te[kept_cols].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"feature selection error ({method}): {e}. using all features.\")\n",
    "            return X_tr, X_te\n",
    "\n",
    "    # pick continuous columns, but always keep forced bases (even if low variance)\n",
    "    def pick_continuous_columns(X, max_cols, must_have=None):\n",
    "        sample = X.iloc[: min(10000, len(X))].copy()\n",
    "        num_cols = sample.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        def ok(col):\n",
    "            return (not str(X[col].dtype).startswith(\"uint8\")) and (sample[col].nunique(dropna=False) > 2)\n",
    "\n",
    "        must_have = [c for c in (must_have or []) if c in X.columns and not str(X[c].dtype).startswith(\"uint8\")]\n",
    "        rest = [c for c in num_cols if ok(c) and c not in must_have]\n",
    "        if rest:\n",
    "            var_series = sample[rest].var().sort_values(ascending=False)\n",
    "            rest_sorted = var_series.index.tolist()\n",
    "        else:\n",
    "            rest_sorted = []\n",
    "        cols = (must_have + [c for c in rest_sorted if c not in must_have])[: max_cols]\n",
    "        return cols\n",
    "\n",
    "    def apply_scaling_and_poly(X_tr, X_te):\n",
    "        # bases needed by your forward polys on Steam\n",
    "        steam_poly_must_have = [\n",
    "            \"price_final_log1p\", \"price_original_log1p\",\n",
    "            \"days_since_release\", \"review_year\",\n",
    "            \"hours_log1p\", \"products_log1p\", \"reviews_log1p\",\n",
    "            \"title_len\", \"desc_len\"\n",
    "        ]\n",
    "        cont_cols = pick_continuous_columns(X_tr, poly_feature_limit, must_have=steam_poly_must_have)\n",
    "        X_tr = X_tr.copy()\n",
    "        X_te = X_te.copy()\n",
    "\n",
    "        do_scale = (scale_method in {\"standard\", \"minmax\"}) and bool(cont_cols)\n",
    "        do_poly = (poly_degree is not None) and (int(poly_degree) >= 2 or bool(poly_interaction_only)) and bool(cont_cols)\n",
    "\n",
    "        if not cont_cols:\n",
    "            return X_tr, X_te\n",
    "\n",
    "        tr_cont = X_tr[cont_cols].astype(\"float32\").values\n",
    "        te_cont = X_te[cont_cols].astype(\"float32\").values\n",
    "\n",
    "        if do_scale:\n",
    "            if scale_method == \"standard\":\n",
    "                scaler = StandardScaler()\n",
    "            elif scale_method == \"minmax\":\n",
    "                scaler = MinMaxScaler()\n",
    "            else:\n",
    "                scaler = None\n",
    "            if scaler is not None:\n",
    "                tr_cont = scaler.fit_transform(tr_cont).astype(\"float32\")\n",
    "                te_cont = scaler.transform(te_cont).astype(\"float32\")\n",
    "                print(f\"scaled columns: {len(cont_cols)} with {scale_method}\")\n",
    "\n",
    "        if do_poly:\n",
    "            poly = PolynomialFeatures(\n",
    "                degree=int(poly_degree),\n",
    "                include_bias=bool(poly_include_bias),\n",
    "                interaction_only=bool(poly_interaction_only)\n",
    "            )\n",
    "            tr_poly = poly.fit_transform(tr_cont).astype(\"float32\")\n",
    "            te_poly = poly.transform(te_cont).astype(\"float32\")\n",
    "            poly_names = [f\"poly_{n}\" for n in poly.get_feature_names_out(cont_cols)]\n",
    "            tr_poly_df = pd.DataFrame(tr_poly, columns=poly_names, index=X_tr.index)\n",
    "            te_poly_df = pd.DataFrame(te_poly, columns=poly_names, index=X_te.index)\n",
    "            X_tr = X_tr.drop(columns=cont_cols).join(tr_poly_df)\n",
    "            X_te = X_te.drop(columns=cont_cols).join(te_poly_df)\n",
    "            print(f\"polynomial features added: {len(poly_names)} (from {len(cont_cols)} columns)\")\n",
    "            return X_tr, X_te\n",
    "\n",
    "        if do_scale:\n",
    "            tr_scaled_df = pd.DataFrame(tr_cont, columns=cont_cols, index=X_tr.index).astype(\"float32\")\n",
    "            te_scaled_df = pd.DataFrame(te_cont, columns=cont_cols, index=X_te.index).astype(\"float32\")\n",
    "            X_tr = X_tr.drop(columns=cont_cols).join(tr_scaled_df)\n",
    "            X_te = X_te.drop(columns=cont_cols).join(te_scaled_df)\n",
    "\n",
    "        return X_tr, X_te\n",
    "\n",
    "    def prepare_steam_df(steam_in):\n",
    "        print(\"prep: steam\")\n",
    "        df = steam_in.copy()\n",
    "        for col in [\"title\", \"description\", \"tags\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(\"string\")\n",
    "\n",
    "        def safe_to_datetime(series):\n",
    "            return pd.to_datetime(series.astype(\"string\"), errors=\"coerce\")\n",
    "        for c in [\"date\", \"date_release\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = safe_to_datetime(df[c])\n",
    "            else:\n",
    "                df[c] = pd.NaT\n",
    "\n",
    "        df[\"days_since_release\"] = (df[\"date\"] - df[\"date_release\"]).dt.days\n",
    "        df[\"days_since_release\"] = df[\"days_since_release\"].clip(lower=0).fillna(0).astype(\"int32\")\n",
    "        df[\"review_year\"] = df[\"date\"].dt.year.astype(\"float64\")\n",
    "        df[\"review_month\"] = df[\"date\"].dt.month.astype(\"float64\")\n",
    "        df[\"review_dow\"] = df[\"date\"].dt.dayofweek.astype(\"float64\")\n",
    "        df[\"review_year\"] = df[\"review_year\"].fillna(-1).astype(\"int16\")\n",
    "        df[\"review_month\"] = df[\"review_month\"].fillna(-1).astype(\"int8\")\n",
    "        df[\"review_dow\"] = df[\"review_dow\"].fillna(-1).astype(\"int8\")\n",
    "\n",
    "        df[\"title_len\"] = df[\"title\"].astype(\"string\").str.len().fillna(0).astype(\"int32\")\n",
    "        df[\"desc_len\"]  = df[\"description\"].astype(\"string\").str.len().fillna(0).astype(\"int32\")\n",
    "\n",
    "        for col in [\"hours\",\"products\",\"reviews\",\"price_final\",\"price_original\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df[col + \"_log1p\"] = np.log1p(df[col])\n",
    "\n",
    "        df[\"is_free\"] = (df[\"price_final\"] == 0).astype(\"int8\")\n",
    "        df[\"discount_ratio\"] = np.where(\n",
    "            pd.to_numeric(df[\"price_original\"], errors=\"coerce\") > 0,\n",
    "            1.0 - (pd.to_numeric(df[\"price_final\"], errors=\"coerce\") /\n",
    "                   pd.to_numeric(df[\"price_original\"], errors=\"coerce\")),\n",
    "            0.0\n",
    "        )\n",
    "        df[\"discount_ratio\"] = pd.Series(df[\"discount_ratio\"]).clip(0, 1).fillna(0.0)\n",
    "\n",
    "        for b in [\"win\",\"mac\",\"linux\",\"steam_deck\"]:\n",
    "            if b in df.columns:\n",
    "                df[b] = (df[b] == True).astype(\"int8\")\n",
    "            else:\n",
    "                df[b] = 0\n",
    "\n",
    "        pos = pd.to_numeric(df.get(\"positive_ratio\"), errors=\"coerce\")\n",
    "        if is_classification:\n",
    "            y = (pos >= 80).astype(\"Int64\")\n",
    "        else:\n",
    "            y = pos.round().astype(\"Int64\")\n",
    "\n",
    "        keep_dense = [\n",
    "            \"win\",\"mac\",\"linux\",\"steam_deck\",\n",
    "            \"days_since_release\",\"review_year\",\"review_month\",\"review_dow\",\n",
    "            \"title_len\",\"desc_len\",\n",
    "            \"hours_log1p\",\"products_log1p\",\"reviews_log1p\",\n",
    "            \"price_final_log1p\",\"price_original_log1p\",\n",
    "            \"discount_ratio\",\"is_free\"\n",
    "        ]\n",
    "        X = df[keep_dense].copy()\n",
    "\n",
    "        base_cols_count = X.shape[1]\n",
    "        allowed_tag_cols = max(0, min(top_k_tags, max_total_features - base_cols_count))\n",
    "        if allowed_tag_cols > 0 and \"tags\" in df.columns:\n",
    "            print(\"prep: steam build sparse tag matrix\")\n",
    "            tags_clean = (\n",
    "                df[\"tags\"].astype(\"string\").fillna(\"\").str.lower()\n",
    "                  .str.replace(r\"[\\[\\]\\\"]\", \"\", regex=True)\n",
    "                  .str.replace(\";\", \",\").str.replace(\"/\", \",\")\n",
    "            )\n",
    "            vec = CountVectorizer(\n",
    "                tokenizer=lambda s: [t.strip() for t in s.split(\",\") if t.strip()],\n",
    "                lowercase=False, binary=True, max_features=allowed_tag_cols\n",
    "            )\n",
    "            tag_sparse = vec.fit_transform(tags_clean.values)\n",
    "            tag_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "                tag_sparse,\n",
    "                columns=[f\"tag_{t}\" for t in vec.get_feature_names_out()],\n",
    "                index=df.index\n",
    "            ).astype(pd.SparseDtype(\"uint8\", 0))\n",
    "            X = pd.concat([X, tag_df], axis=1)\n",
    "\n",
    "            # force-create chosen tags even if not in top-K\n",
    "            force_tags = [\n",
    "                \"early access\",\"great soundtrack\",\"2d\",\"massively multiplayer\",\"free to play\",\"cute\",\n",
    "                \"action rpg\",\"first-person\",\"fast-paced\",\"mmorpg\",\"puzzle\",\"pvp\",\"management\",\"memes\",\n",
    "                \"relaxing\",\"visual novel\",\"sexual content\",\"difficult\",\"emotional\"\n",
    "            ]\n",
    "            def has_tag(series, t):\n",
    "                pattern = rf\"(?:^|,)\\s*{re.escape(t)}\\s*(?:,|$)\"\n",
    "                return series.str.contains(pattern, regex=True)\n",
    "\n",
    "            for t in force_tags:\n",
    "                col = f\"tag_{t}\"\n",
    "                if col not in X.columns:\n",
    "                    X[col] = has_tag(tags_clean, t).astype(\"uint8\")\n",
    "\n",
    "            # alias variants -> OR into canonical columns\n",
    "            tag_aliases = {\n",
    "                \"first-person\": [\"first person\"],\n",
    "                \"action rpg\": [\"action-rpg\"],\n",
    "                \"mmorpg\": [\"mmo rpg\", \"mmo-rpg\"]\n",
    "            }\n",
    "            for t, alts in tag_aliases.items():\n",
    "                col = f\"tag_{t}\"\n",
    "                if col not in X.columns:\n",
    "                    X[col] = has_tag(tags_clean, t).astype(\"uint8\")\n",
    "                for a in alts:\n",
    "                    alias_hits = has_tag(tags_clean, a).astype(\"uint8\")\n",
    "                    if alias_hits.any():\n",
    "                        X[col] = (X[col].astype(\"uint8\") | alias_hits).astype(\"uint8\")\n",
    "\n",
    "        if any(is_sparse_dtype(X[c].dtype) for c in X.columns):\n",
    "            for c in X.columns:\n",
    "                if is_sparse_dtype(X[c].dtype):\n",
    "                    X[c] = X[c].sparse.to_dense().astype(\"uint8\")\n",
    "\n",
    "        dense_num_cols = [c for c in X.select_dtypes(include=[np.number]).columns\n",
    "                          if not is_sparse_dtype(X[c].dtype)]\n",
    "        for c in dense_num_cols:\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(pd.to_numeric(X[c], errors=\"coerce\").median())\n",
    "\n",
    "        raw = df[[\"date\",\"app_id\"]].copy()\n",
    "        out_df = X.copy()\n",
    "        out_df[\"target_steam\"] = y\n",
    "        out_df = out_df[out_df[\"target_steam\"].notna()].copy()\n",
    "        if is_classification:\n",
    "            out_df[\"target_steam\"] = out_df[\"target_steam\"].astype(\"int8\")\n",
    "        else:\n",
    "            out_df[\"target_steam\"] = out_df[\"target_steam\"].astype(\"int16\")\n",
    "        return out_df, raw\n",
    "\n",
    "    def prepare_olist_df(olist_in):\n",
    "        print(\"prep: olist\")\n",
    "        o = olist_in.copy()\n",
    "\n",
    "        def safe_to_datetime(series):\n",
    "            return pd.to_datetime(series.astype(\"string\"), errors=\"coerce\")\n",
    "        date_cols = [\n",
    "            \"order_purchase_timestamp\",\"order_approved_at\",\"order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\",\"order_estimated_delivery_date\",\"shipping_limit_date\",\n",
    "        ]\n",
    "        for c in date_cols:\n",
    "            if c in o.columns:\n",
    "                o[c] = safe_to_datetime(o[c])\n",
    "            else:\n",
    "                o[c] = pd.NaT\n",
    "\n",
    "        o[\"purchase_dayofweek\"] = o[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "        o[\"purchase_month\"] = o[\"order_purchase_timestamp\"].dt.month\n",
    "        o[\"purchase_hour\"] = o[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "        def to_hours(td):\n",
    "            return td.dt.total_seconds() / 3600.0\n",
    "\n",
    "        for c in [\"product_length_cm\",\"product_width_cm\",\"product_height_cm\",\"product_weight_g\",\n",
    "                  \"payment_installments_max\",\"payment_value_total\",\"payment_count\",\"freight_value\",\"order_item_id\"]:\n",
    "            if c in o.columns:\n",
    "                o[c] = pd.to_numeric(o[c], errors=\"coerce\")\n",
    "\n",
    "        o[\"approval_delay_h\"] = to_hours(o[\"order_approved_at\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_carrier_h\"] = to_hours(o[\"order_delivered_carrier_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_customer_h\"] = to_hours(o[\"order_delivered_customer_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"est_delivery_h\"] = to_hours(o[\"order_estimated_delivery_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"limit_from_purchase_h\"] = to_hours(o[\"shipping_limit_date\"] - o[\"order_purchase_timestamp\"])\n",
    "\n",
    "        o[\"delivered_late\"] = (o[\"order_delivered_customer_date\"] > o[\"order_estimated_delivery_date\"]).astype(\"Int64\")\n",
    "\n",
    "        for c in [\"product_length_cm\",\"product_width_cm\",\"product_height_cm\",\"product_weight_g\"]:\n",
    "            if c not in o.columns:\n",
    "                o[c] = np.nan\n",
    "        o[\"product_volume_cm3\"] = o[\"product_length_cm\"] * o[\"product_width_cm\"] * o[\"product_height_cm\"]\n",
    "        o[\"density_g_per_cm3\"] = np.where(\n",
    "            (o[\"product_volume_cm3\"] > 0) & o[\"product_weight_g\"].notna(),\n",
    "            o[\"product_weight_g\"] / o[\"product_volume_cm3\"],\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        for c in [\"payment_installments_max\",\"payment_value_total\",\"payment_count\"]:\n",
    "            if c not in o.columns:\n",
    "                o[c] = np.nan\n",
    "        o[\"avg_installment_value\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_installments_max\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_installments_max\"],\n",
    "            np.nan,\n",
    "        )\n",
    "        o[\"payment_value_per_payment\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_count\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_count\"],\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        if \"freight_value\" not in o.columns:\n",
    "            o[\"freight_value\"] = np.nan\n",
    "        o[\"freight_per_kg\"] = np.where(\n",
    "            pd.to_numeric(o[\"product_weight_g\"], errors=\"coerce\") > 0,\n",
    "            o[\"freight_value\"] / (o[\"product_weight_g\"] / 1000.0),\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        if \"order_item_id\" not in o.columns:\n",
    "            o[\"order_item_id\"] = 1\n",
    "        o[\"is_multi_item_order\"] = (pd.to_numeric(o[\"order_item_id\"], errors=\"coerce\") > 1).astype(\"Int64\")\n",
    "\n",
    "        if \"product_category_name\" in o.columns:\n",
    "            o[\"product_category_name\"] = o[\"product_category_name\"].astype(\"string\")\n",
    "        if \"product_category_name_english\" in o.columns:\n",
    "            o[\"product_category_name_english\"] = o[\"product_category_name_english\"].astype(\"string\")\n",
    "        if \"product_category_name\" in o.columns and \"product_category_name_english\" in o.columns:\n",
    "            s1 = o[\"product_category_name_english\"]\n",
    "            s2 = o[\"product_category_name\"]\n",
    "            o[\"product_category\"] = s1.mask(s1.isna() | (s1 == \"\"), s2)\n",
    "            o = o.drop(columns=[\"product_category_name\",\"product_category_name_english\"])\n",
    "        elif \"product_category_name_english\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name_english\"]\n",
    "            o = o.drop(columns=[\"product_category_name_english\"])\n",
    "        elif \"product_category_name\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name\"]\n",
    "            o = o.drop(columns=[\"product_category_name\"])\n",
    "        else:\n",
    "            o[\"product_category\"] = \"Unknown\"\n",
    "\n",
    "        for col in [\"customer_city\",\"seller_city\"]:\n",
    "            if col in o.columns:\n",
    "                s_ = o[col].astype(\"string\").fillna(\"Unknown\")\n",
    "                freq = s_.map(s_.value_counts(normalize=True))\n",
    "                o[col + \"_freq\"] = freq.astype(float)\n",
    "                o = o.drop(columns=[col])\n",
    "\n",
    "        cat_cols = []\n",
    "        for c in [\"order_status\",\"customer_state\",\"seller_state\",\"product_category\"]:\n",
    "            if c in o.columns:\n",
    "                cat_cols.append(c)\n",
    "                o[c] = o[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        if cat_cols:\n",
    "            o = pd.get_dummies(o, columns=cat_cols, dtype=np.uint8)\n",
    "\n",
    "        if \"review_score_mean_product\" in olist_in.columns:\n",
    "            base = pd.to_numeric(olist_in[\"review_score_mean_product\"], errors=\"coerce\")\n",
    "            if is_classification:\n",
    "                y = (base >= 4.0).astype(\"Int64\")\n",
    "            else:\n",
    "                y = base.astype(\"Float64\")\n",
    "        else:\n",
    "            print(\"olist: 'review_score_mean_product' missing; falling back to delivered_late.\")\n",
    "            y = o[\"delivered_late\"].copy()\n",
    "\n",
    "        o[\"target_olist\"] = y\n",
    "        o = o[o[\"target_olist\"].notna()].copy()\n",
    "        if is_classification:\n",
    "            o[\"target_olist\"] = o[\"target_olist\"].astype(\"int8\")\n",
    "        else:\n",
    "            o[\"target_olist\"] = o[\"target_olist\"].astype(\"float32\")\n",
    "\n",
    "        for leak_col in [\"review_score_mean_product\",\"review_count_product\",\"review_score_mean\",\"delivered_late\"]:\n",
    "            if leak_col in o.columns:\n",
    "                o = o.drop(columns=[leak_col])\n",
    "\n",
    "        drop_ids = [\"order_id\",\"customer_id\",\"customer_unique_id\",\"product_id\",\"seller_id\"]\n",
    "        o = o.drop(columns=[c for c in drop_ids if c in o.columns], errors=\"ignore\")\n",
    "        o = o.drop(columns=[c for c in date_cols if c in o.columns], errors=\"ignore\")\n",
    "\n",
    "        num_cols = o.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        num_cols_no_target = [c for c in num_cols if c != \"target_olist\"]\n",
    "        for c in num_cols_no_target:\n",
    "            o[c] = pd.to_numeric(o[c], errors=\"coerce\").fillna(pd.to_numeric(o[c], errors=\"coerce\").median())\n",
    "        obj_cols = o.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        for c in obj_cols:\n",
    "            o[c] = o[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        return o.copy()\n",
    "    \n",
    "    def prepare_sales_df(sales_in):\n",
    "        print(\"prep: sales\")\n",
    "        s = sales_in.copy()\n",
    "\n",
    "        # target\n",
    "        if \"Critic_Score\" in s.columns:\n",
    "            cs = pd.to_numeric(s[\"Critic_Score\"], errors=\"coerce\")\n",
    "            y = (cs > 8.0).astype(\"Int64\") if is_classification else cs.astype(\"Float64\")\n",
    "        else:\n",
    "            y = pd.Series(np.nan, index=s.index, dtype=\"Float64\")\n",
    "\n",
    "        s[\"target_sales\"] = y\n",
    "        s = s[s[\"target_sales\"].notna()].copy()\n",
    "        s[\"target_sales\"] = s[\"target_sales\"].astype(\"int8\" if is_classification else \"float32\")\n",
    "\n",
    "        # remove obvious leaks and junk\n",
    "        leak_cols = [\"NA_Sales\",\"PAL_Sales\",\"JP_Sales\",\"Other_Sales\",\"Total_Shipped\",\"Rank\",\"Global_Sales\"]\n",
    "        junk_cols = [\"VGChartz_Score\",\"Vgchartzscore\",\"url\",\"img_url\",\"status\",\"Last_Update\",\"basename\",\"User_Score\"]\n",
    "        s = s.drop(columns=[c for c in leak_cols + junk_cols if c in s.columns], errors=\"ignore\")\n",
    "        if \"Critic_Score\" in s.columns:\n",
    "            s = s.drop(columns=[\"Critic_Score\"], errors=\"ignore\")\n",
    "\n",
    "        # simple remaster flag, then drop Name\n",
    "        if \"Name\" in s.columns:\n",
    "            terms = [\"remaster\",\"remastered\",\"hd\",\"definitive\",\"collection\",\"trilogy\",\"anniversary\"]\n",
    "            s[\"is_remaster\"] = s[\"Name\"].astype(\"string\").str.lower().str.contains(\"|\".join(terms), na=False).astype(\"Int64\")\n",
    "            s = s.drop(columns=[\"Name\"])\n",
    "        else:\n",
    "            s[\"is_remaster\"] = pd.Series(0, index=s.index, dtype=\"Int64\")\n",
    "\n",
    "        # platform family + handheld flag\n",
    "        def platform_family(p):\n",
    "            p = \"\" if pd.isna(p) else str(p).upper()\n",
    "            if p.startswith(\"PS\") or p in {\"PSP\",\"PSV\"}: return \"PlayStation\"\n",
    "            if p.startswith(\"X\") or p in {\"XB\",\"XBLA\"}: return \"Xbox\"\n",
    "            if p in {\"SWITCH\",\"WII\",\"WIIU\",\"GC\",\"N64\",\"SNES\",\"NES\",\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\"}: return \"Nintendo\"\n",
    "            if p == \"PC\": return \"PC\"\n",
    "            if p in {\"DC\",\"DREAMCAST\",\"SAT\",\"GEN\",\"MD\",\"MEGADRIVE\",\"GG\"}: return \"Sega\"\n",
    "            if \"ATARI\" in p: return \"Atari\"\n",
    "            return \"Other\"\n",
    "\n",
    "        if \"Platform\" in s.columns:\n",
    "            s[\"Platform_Family\"] = s[\"Platform\"].apply(platform_family)\n",
    "            handhelds = {\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\",\"PSP\",\"PSV\"}\n",
    "            s[\"is_portable\"] = s[\"Platform\"].astype(\"string\").str.upper().isin(handhelds).astype(\"Int64\")\n",
    "            s = s.drop(columns=[\"Platform\"])\n",
    "        else:\n",
    "            s[\"Platform_Family\"] = \"Other\"\n",
    "            s[\"is_portable\"] = pd.Series(0, index=s.index, dtype=\"Int64\")\n",
    "\n",
    "        # year / decade\n",
    "        if \"Year\" in s.columns:\n",
    "            s[\"Year\"] = pd.to_numeric(s[\"Year\"], errors=\"coerce\")\n",
    "            s.loc[~s[\"Year\"].between(1970, 2025, inclusive=\"both\"), \"Year\"] = np.nan\n",
    "            s[\"Decade\"] = (s[\"Year\"] // 10 * 10).astype(\"Int64\").astype(str)\n",
    "        else:\n",
    "            s[\"Year\"] = np.nan\n",
    "            s[\"Decade\"] = \"<NA>\"\n",
    "\n",
    "        # frequency encodes then drop raw strings\n",
    "        for col in [\"Publisher\",\"Developer\"]:\n",
    "            if col in s.columns:\n",
    "                series = s[col].astype(\"string\").fillna(\"Unknown\")\n",
    "                freq = series.map(series.value_counts(normalize=True))\n",
    "                s[col + \"_freq\"] = freq.astype(float)\n",
    "                s = s.drop(columns=[col])\n",
    "\n",
    "        # one-hot encode once, after building the list\n",
    "        cat_cols = [c for c in [\"Genre\",\"ESRB_Rating\",\"Platform_Family\",\"Decade\"] if c in s.columns]\n",
    "        for c in cat_cols:\n",
    "            s[c] = s[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        if len(cat_cols) > 0:\n",
    "            s = pd.get_dummies(s, columns=cat_cols, dtype=np.uint8)\n",
    "\n",
    "        # numeric cleanup\n",
    "        num_cols = s.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        num_cols_no_target = [c for c in num_cols if c != \"target_sales\"]\n",
    "        for c in num_cols_no_target:\n",
    "            s[c] = pd.to_numeric(s[c], errors=\"coerce\").fillna(pd.to_numeric(s[c], errors=\"coerce\").median())\n",
    "\n",
    "        # object cleanup\n",
    "        obj_cols = s.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        for c in obj_cols:\n",
    "            s[c] = s[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        return s.copy()\n",
    "\n",
    "\n",
    "    steam_df, steam_raw = prepare_steam_df(steam)\n",
    "    olist_df = prepare_olist_df(olist)\n",
    "    sales_df = prepare_sales_df(sales)\n",
    "\n",
    "    raw_for_split = steam_raw.loc[steam_df.index].copy()\n",
    "    raw_for_split[\"date\"] = pd.to_datetime(raw_for_split[\"date\"].astype(\"string\"), errors=\"coerce\")\n",
    "    cutoff_q = 1.0 - float(test_size)\n",
    "    cutoff_date = raw_for_split[\"date\"].sort_values().quantile(cutoff_q)\n",
    "\n",
    "    grp_first = raw_for_split.groupby(\"app_id\")[\"date\"].min()\n",
    "    app_train = set(grp_first[grp_first <= cutoff_date].index.tolist())\n",
    "    app_test = set(grp_first[grp_first > cutoff_date].index.tolist())\n",
    "\n",
    "    is_train_mask = raw_for_split[\"app_id\"].isin(app_train)\n",
    "    is_test_mask = raw_for_split[\"app_id\"].isin(app_test)\n",
    "    is_train_mask = is_train_mask | (~(is_train_mask | is_test_mask))\n",
    "\n",
    "    Xs = steam_df.drop(columns=[\"target_steam\"])\n",
    "    ys = steam_df[\"target_steam\"]\n",
    "\n",
    "    X_train_steam = Xs[is_train_mask].reset_index(drop=True)\n",
    "    X_test_steam = Xs[is_test_mask].reset_index(drop=True)\n",
    "    y_train_steam = ys[is_train_mask].reset_index(drop=True)\n",
    "    y_test_steam = ys[is_test_mask].reset_index(drop=True)\n",
    "\n",
    "    print(\"steam cutoff_date:\", pd.to_datetime(cutoff_date))\n",
    "    print(\"steam apps in test:\", len(app_test), \"of\", len(grp_first))\n",
    "    print(\"steam train rows:\", int(X_train_steam.shape[0]), \"| test rows:\", int(X_test_steam.shape[0]))\n",
    "    if is_classification:\n",
    "        print(\"steam y_train counts:\", y_train_steam.value_counts().to_dict())\n",
    "        print(\"steam y_test counts:\", y_test_steam.value_counts().to_dict())\n",
    "    else:\n",
    "        def sstats(s):\n",
    "            return {\"n\": int(s.shape[0]),\n",
    "                    \"mean\": float(np.nanmean(s)),\n",
    "                    \"std\": float(np.nanstd(s)),\n",
    "                    \"min\": float(np.nanmin(s)),\n",
    "                    \"max\": float(np.nanmax(s))}\n",
    "        print(\"steam y_train stats:\", sstats(y_train_steam))\n",
    "        print(\"steam y_test stats:\", sstats(y_test_steam))\n",
    "\n",
    "    if is_classification:\n",
    "        if y_train_steam.nunique() == 2:\n",
    "            vc = y_train_steam.value_counts()\n",
    "            ratio = float(vc.min()) / float(vc.max()) if vc.max() > 0 else 1.0\n",
    "            if balance_method != \"none\" and ratio < min_class_ratio:\n",
    "                method = \"oversample\" if (balance_method == \"auto\" and len(X_train_steam) <= 200000) else \\\n",
    "                         (\"undersample\" if balance_method == \"auto\" else balance_method)\n",
    "                X_train_steam, y_train_steam = resample_binary(X_train_steam, y_train_steam, method=method, random_state=random_state)\n",
    "                print(\"steam balanced train counts:\", y_train_steam.value_counts().to_dict())\n",
    "\n",
    "    steam_split = (X_train_steam, X_test_steam, y_train_steam, y_test_steam)\n",
    "\n",
    "    olist_split = split_and_balance(olist_df, \"target_olist\", balance_method, min_class_ratio, random_state)\n",
    "    sales_split = split_and_balance(sales_df, \"target_sales\", balance_method, min_class_ratio, random_state)\n",
    "\n",
    "    def scale_poly_wrapper(split):\n",
    "        Xtr, Xte, ytr, yte = split\n",
    "        Xtr2, Xte2 = apply_scaling_and_poly(Xtr, Xte)\n",
    "        return (Xtr2, Xte2, ytr, yte)\n",
    "\n",
    "    steam_split = scale_poly_wrapper(steam_split)\n",
    "    olist_split = scale_poly_wrapper(olist_split)\n",
    "    sales_split = scale_poly_wrapper(sales_split)\n",
    "\n",
    "    if feature_select_method != \"none\":\n",
    "        Xtr, Xte, ytr, yte = steam_split\n",
    "        Xtr_s, Xte_s = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"steam\"\n",
    "        )\n",
    "        steam_split = (Xtr_s, Xte_s, ytr, yte)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = olist_split\n",
    "        Xtr_o, Xte_o = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"olist\"\n",
    "        )\n",
    "        olist_split = (Xtr_o, Xte_o, ytr, yte)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = sales_split\n",
    "        Xtr_v, Xte_v = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"sales\"\n",
    "        )\n",
    "        sales_split = (Xtr_v, Xte_v, ytr, yte)\n",
    "\n",
    "    # show exactly which saved features are missing (after selection)\n",
    "    _ = debug_forward_missing(steam_split[0], \"steam\", best_forward_features)\n",
    "    _ = debug_forward_missing(olist_split[0], \"olist\", best_forward_features)\n",
    "    _ = debug_forward_missing(sales_split[0], \"sales\", best_forward_features)\n",
    "\n",
    "    print(\"\\nsteam selected features (train, test):\", steam_split[0].shape[1], steam_split[1].shape[1])\n",
    "    print(\"olist selected features (train, test):\", olist_split[0].shape[1], olist_split[1].shape[1])\n",
    "    print(\"sales selected features (train, test):\", sales_split[0].shape[1], sales_split[1].shape[1])\n",
    "\n",
    "    return {\n",
    "        \"steam\": steam_split,\n",
    "        \"olist\": olist_split,\n",
    "        \"sales\": sales_split,\n",
    "        \"feature_names\": {\n",
    "            \"steam\": steam_split[0].columns.tolist(),\n",
    "            \"olist\": olist_split[0].columns.tolist(),\n",
    "            \"sales\": sales_split[0].columns.tolist(),\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b18a3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: start downloads\n",
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.314 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.208 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.179 sec\n",
      "main: downloads finished\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 50000 of 41154794 rows in 6.089 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(50000, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: picked 50000 of 99441 rows in 0.004 sec\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (57112, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: class counts too small for requested size, falling back to simple sample\n",
      "simple_random_sample: picked 50000 of 55792 rows in 0.003 sec\n",
      "vg2019: done shape=(50000, 16)\n",
      "main: load all done in 18.158 sec (00:00:18)\n",
      "download: shapes summary\n",
      "download: steam shape = (50000, 24)\n",
      "download: olist shape = (57112, 38)\n",
      "download: sales shape = (50000, 16)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Download Paths\n",
    "# =============================\n",
    "print(\"main: start downloads\")\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "print(\"main: downloads finished\")\n",
    "\n",
    "# =============================\n",
    "# Load All\n",
    "# =============================\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# =============================\n",
    "# Download Shapes\n",
    "# =============================\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3177e9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e30164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep: steam\n",
      "prep: steam build sparse tag matrix\n",
      "prep: olist\n",
      "prep: sales\n",
      "steam cutoff_date: 2022-02-19 00:00:00\n",
      "steam apps in test: 951 of 6829\n",
      "steam train rows: 47314 | test rows: 2686\n",
      "steam y_train counts: {1: 38721, 0: 8593}\n",
      "steam y_test counts: {1: 1767, 0: 919}\n",
      "target_olist train class counts: {1: 30503, 0: 15186}\n",
      "target_olist test class counts: {1: 7626, 0: 3797}\n",
      "target_sales train class counts: {0: 38521, 1: 1479}\n",
      "target_sales test class counts: {0: 9630, 1: 370}\n",
      "polynomial features added: 90 (from 12 columns)\n",
      "polynomial features added: 350 (from 25 columns)\n",
      "polynomial features added: 9 (from 3 columns)\n",
      "[debug] olist missing: ['product_name_lenght']\n",
      "\n",
      "steam selected features (train, test): 314 314\n",
      "olist selected features (train, test): 491 491\n",
      "sales selected features (train, test): 53 53\n",
      "\n",
      "=== STEAM Dataset ===\n",
      "Removed 19 constant feature(s).\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=30 | F1_macro=0.729782 ± 0.003112\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Classification call\n",
    "splits = prepare_all(steam, olist, sales, task_type=\"classification\", test_size=0.2)\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "print(\"\\n=== STEAM Dataset ===\")\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True,\n",
    "    oversample_method=\"smote\"\n",
    ")\n",
    "\n",
    "score_steam = evaluate_on_holdout(best_steam_model, X_test_steam, y_test_steam, task_type=\"classification\")\n",
    "\n",
    "print(\"\\n=== OLIST Dataset ===\")\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True,\n",
    "    oversample_method=\"smote\"\n",
    ")\n",
    "\n",
    "score_olist = evaluate_on_holdout(best_olist_model, X_test_olist, y_test_olist, task_type=\"classification\")\n",
    "\n",
    "print(\"\\n=== SALES Dataset ===\")\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True,\n",
    "    oversample_method=\"smote\"\n",
    ") \n",
    "\n",
    "score_sales = evaluate_on_holdout(best_sales_model, X_test_sales, y_test_sales, task_type=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec77e4c",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc10f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1959098138.py, line 12)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbest_steam_model = build_and_tune_models(x\u001b[39m\n                                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regression call\n",
    "splits = prepare_all(\n",
    "    steam, olist, sales,\n",
    "    task_type=\"regression\",\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
