{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Forward Selection RF Sweeps (Classification + Regression)\n",
    "# =============================\n",
    "\n",
    "# standard libs\n",
    "import os, time, math, json, warnings\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# progress + display\n",
    "from IPython.display import display\n",
    "\n",
    "# data libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# kaggle\n",
    "import kagglehub\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, KFold, StratifiedShuffleSplit, cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_classif, f_regression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# clean prints\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.options.display.float_format = \"{:.6f}\".format\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "RANDOM_STATE = 42\n",
    "N_ROWS = 1_000_000  # cap when loading\n",
    "DATASET_KEYS = [\"steam\", \"olist\", \"sales\"]  # sales == VG2019\n",
    "CV_OUTER = 5       # for reported scores\n",
    "CV_INNER = 3       # inside forward selection\n",
    "K_VALUES = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "SAMPLE_SIZES = [10_000, 20_000, 30_000, 40_000, 50_000]\n",
    "FORWARD_PREFILTER_TOP = 120  # shrink search pool before forward\n",
    "EARLY_STOP_ROUNDS = 8        # stop if no gain for many steps\n",
    "MIN_DELTA = 1e-6             # tiny improvement threshold\n",
    "\n",
    "# =============================\n",
    "# Small helpers\n",
    "# =============================\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    if folder_path is None or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return [f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path is not None else None\n",
    "    if full_path is not None and os.path.exists(full_path):\n",
    "        return pd.read_csv(full_path, **kwargs)\n",
    "    return None\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    print(f\"download: start {dataset_name}\")\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        print(f\"download: ok -> {path} in {round(time.perf_counter() - t0, 2)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"download: error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    for col in df.columns:\n",
    "        low = col.lower()\n",
    "        if (\"date\" in low) or (\"time\" in low):\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col].astype(\"string\"), errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def optimize_dtypes(df, convert_categoricals=True, category_threshold=0.2):\n",
    "    if df is None:\n",
    "        return None\n",
    "    start_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    for c in df.select_dtypes(include=[np.floating]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    for c in df.select_dtypes(include=[np.integer]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    if convert_categoricals:\n",
    "        obj_cols = [c for c in df.select_dtypes(include=[\"object\"]).columns if (\"date\" not in c.lower() and \"time\" not in c.lower())]\n",
    "        n_rows = max(len(df), 1)\n",
    "        for c in obj_cols:\n",
    "            try:\n",
    "                if df[c].apply(lambda x: isinstance(x, (list, dict, set))).any():\n",
    "                    continue\n",
    "                uniq_ratio = df[c].nunique(dropna=False) / n_rows\n",
    "                if uniq_ratio <= category_threshold:\n",
    "                    df[c] = df[c].astype(\"category\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    end_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"optimize_dtypes: {start_mb:.2f} MB -> {end_mb:.2f} MB\")\n",
    "    return df\n",
    "\n",
    "# =============================\n",
    "# Loaders (patched to avoid test_size=0)\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip (no path)\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recs  = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    meta = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            meta = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(\"steam: metadata read error:\", str(e))\n",
    "\n",
    "    print(f\"steam: shapes games={None if games is None else games.shape}, users={None if users is None else users.shape}, recs={None if recs is None else recs.shape}, meta={None if meta is None else meta.shape}\")\n",
    "\n",
    "    if recs is None:\n",
    "        print(\"steam: skip (recommendations.csv missing)\")\n",
    "        return None\n",
    "\n",
    "    # patched: guard against test_size=0\n",
    "    if \"is_recommended\" in recs.columns:\n",
    "        total = len(recs)\n",
    "        n_take = min(n_rows, total)\n",
    "        if n_take >= total:\n",
    "            recs = recs.copy()\n",
    "        else:\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1, test_size=total - n_take, random_state=seed)\n",
    "            keep_idx, _ = next(splitter.split(recs, recs[\"is_recommended\"]))\n",
    "            recs = recs.iloc[keep_idx].copy()\n",
    "    else:\n",
    "        recs = recs.sample(n=min(n_rows, len(recs)), random_state=seed).copy()\n",
    "\n",
    "    if meta is not None and games is not None and \"app_id\" in meta.columns and \"app_id\" in games.columns:\n",
    "        games = games.merge(meta, on=\"app_id\", how=\"left\")\n",
    "    df = recs\n",
    "    if games is not None and \"app_id\" in df.columns and \"app_id\" in games.columns:\n",
    "        df = df.merge(games, on=\"app_id\", how=\"left\")\n",
    "    if users is not None and \"user_id\" in df.columns and \"user_id\" in users.columns:\n",
    "        df = df.merge(users, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    df = coerce_datetime_columns(df)\n",
    "    print(\"steam: done shape\", None if df is None else df.shape)\n",
    "    return df\n",
    "\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip (no path)\")\n",
    "        return None\n",
    "\n",
    "    oc = try_read_csv(base_path, \"olist_customers_dataset.csv\")\n",
    "    og = try_read_csv(base_path, \"olist_geolocation_dataset.csv\")\n",
    "    oi = try_read_csv(base_path, \"olist_order_items_dataset.csv\")\n",
    "    op = try_read_csv(base_path, \"olist_order_payments_dataset.csv\")\n",
    "    orv = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\")\n",
    "    oo = try_read_csv(base_path, \"olist_orders_dataset.csv\")\n",
    "    opr = try_read_csv(base_path, \"olist_products_dataset.csv\")\n",
    "    osell = try_read_csv(base_path, \"olist_sellers_dataset.csv\")\n",
    "    octr = try_read_csv(base_path, \"product_category_name_translation.csv\")\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if oc is None else oc.shape}, \"\n",
    "        f\"geolocation={None if og is None else og.shape}, \"\n",
    "        f\"items={None if oi is None else oi.shape}, \"\n",
    "        f\"payments={None if op is None else op.shape}, \"\n",
    "        f\"reviews={None if orv is None else orv.shape}, \"\n",
    "        f\"orders={None if oo is None else oo.shape}, \"\n",
    "        f\"products={None if opr is None else opr.shape}, \"\n",
    "        f\"sellers={None if osell is None else osell.shape}, \"\n",
    "        f\"cat_trans={None if octr is None else octr.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [oo, oi, opr, osell, oc]):\n",
    "        print(\"olist: skip (core tables missing)\")\n",
    "        return None\n",
    "\n",
    "    orders_small = oo.sample(n=min(n_rows, len(oo)), random_state=seed)\n",
    "    items_small = oi[oi[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if octr is not None and \"product_category_name\" in opr.columns:\n",
    "        products_en = opr.merge(octr, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = opr\n",
    "\n",
    "    if orv is not None:\n",
    "        pr = items_small[[\"order_id\", \"product_id\"]].merge(orv[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        pr = pr.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = pr.groupby(\"product_id\", as_index=False).agg(\n",
    "            review_count_product=(\"review_score\", \"count\"),\n",
    "            review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(osell, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if og is not None:\n",
    "        geo_zip = og.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "            geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "            geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "            geo_points=(\"geolocation_city\", \"count\"),\n",
    "        )\n",
    "        customers_geo = oc.merge(\n",
    "            geo_zip,\n",
    "            left_on=\"customer_zip_code_prefix\",\n",
    "            right_on=\"geolocation_zip_code_prefix\",\n",
    "            how=\"left\",\n",
    "        ).drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "    else:\n",
    "        customers_geo = oc\n",
    "\n",
    "    if op is not None:\n",
    "        payments_agg = op.groupby(\"order_id\", as_index=False).agg(\n",
    "            payment_value_total=(\"payment_value\", \"sum\"),\n",
    "            payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "            payment_count=(\"payment_type\", \"count\"),\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "    if payments_agg is not None:\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "    if product_stats is not None:\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "    print(\"olist: shape after assemble\", olist_full.shape)\n",
    "    return olist_full\n",
    "\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip (no path)\")\n",
    "        return None\n",
    "    csvs = list_csvs(base_path)\n",
    "    target_csv = \"vgsales-12-4-2019.csv\" if \"vgsales-12-4-2019.csv\" in csvs else (csvs[0] if csvs else None)\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip (no csv)\")\n",
    "        return None\n",
    "    df = pd.read_csv(os.path.join(base_path, target_csv), low_memory=False)\n",
    "    print(\"vg2019: loaded\", target_csv, \"shape\", df.shape)\n",
    "\n",
    "    # patched: guard against test_size=0\n",
    "    total = len(df)\n",
    "    n_take = min(n_rows, total)\n",
    "    if \"Genre\" in df.columns:\n",
    "        if n_take >= total:\n",
    "            df = df.copy()\n",
    "        else:\n",
    "            splitter = StratifiedShuffleSplit(n_splits=1, test_size=total - n_take, random_state=seed)\n",
    "            keep_idx, _ = next(splitter.split(df, df[\"Genre\"]))\n",
    "            df = df.iloc[keep_idx].copy()\n",
    "    else:\n",
    "        df = df.sample(n=n_take, random_state=seed).copy()\n",
    "\n",
    "    print(\"vg2019: done shape\", df.shape)\n",
    "    return df\n",
    "\n",
    "# =============================\n",
    "# Download + load\n",
    "# =============================\n",
    "print(\"main: start downloads\")\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "print(\"main: downloads finished\")\n",
    "\n",
    "t0_load = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=RANDOM_STATE)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=RANDOM_STATE)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=RANDOM_STATE)\n",
    "print(f\"main: load all done in {round(time.perf_counter() - t0_load, 2)} sec ({format_hms(time.perf_counter() - t0_load)})\")\n",
    "\n",
    "if steam is not None:\n",
    "    print(\"optimize steam\")\n",
    "    steam = optimize_dtypes(steam, True, 0.2)\n",
    "if olist is not None:\n",
    "    print(\"optimize olist\")\n",
    "    olist = optimize_dtypes(olist, True, 0.2)\n",
    "if sales is not None:\n",
    "    print(\"optimize vg2019\")\n",
    "    sales = optimize_dtypes(sales, True, 0.2)\n",
    "\n",
    "# =============================\n",
    "# Feature prep for both tasks\n",
    "# =============================\n",
    "def prepare_all(steam, olist, sales):\n",
    "    print(\"prepare: start\")\n",
    "\n",
    "    # ----- Steam -----\n",
    "    def prepare_steam(df_in):\n",
    "        df = df_in.copy()\n",
    "\n",
    "        # strings\n",
    "        for c in [\"title\", \"description\", \"tags\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = df[c].astype(\"string\")\n",
    "\n",
    "        # dates\n",
    "        def sdt(s): return pd.to_datetime(s.astype(\"string\"), errors=\"coerce\")\n",
    "        for c in [\"date\", \"date_release\"]:\n",
    "            df[c] = sdt(df[c]) if c in df.columns else pd.NaT\n",
    "\n",
    "        # time parts\n",
    "        df[\"days_since_release\"] = (df[\"date\"] - df[\"date_release\"]).dt.days\n",
    "        df[\"days_since_release\"] = df[\"days_since_release\"].clip(lower=0).fillna(0).astype(\"int32\")\n",
    "        df[\"review_year\"] = df[\"date\"].dt.year.fillna(-1).astype(\"int16\")\n",
    "        df[\"review_month\"] = df[\"date\"].dt.month.fillna(-1).astype(\"int8\")\n",
    "        df[\"review_dow\"] = df[\"date\"].dt.dayofweek.fillna(-1).astype(\"int8\")\n",
    "\n",
    "        # text lengths\n",
    "        df[\"title_len\"] = df[\"title\"].str.len().fillna(0).astype(\"int32\")\n",
    "        df[\"desc_len\"]  = df[\"description\"].str.len().fillna(0).astype(\"int32\")\n",
    "\n",
    "        # numeric log1p\n",
    "        for c in [\"hours\", \"products\", \"reviews\", \"price_final\", \"price_original\"]:\n",
    "            df[c] = pd.to_numeric(df.get(c), errors=\"coerce\")\n",
    "            df[c + \"_log1p\"] = np.log1p(df[c])\n",
    "\n",
    "        # platforms\n",
    "        for b in [\"win\", \"mac\", \"linux\", \"steam_deck\"]:\n",
    "            df[b] = (df.get(b) == True).astype(\"int8\") if b in df.columns else 0\n",
    "\n",
    "        # price flags\n",
    "        df[\"is_free\"] = (df[\"price_final\"] == 0).astype(\"int8\")\n",
    "        df[\"discount_ratio\"] = np.where(\n",
    "            pd.to_numeric(df[\"price_original\"], errors=\"coerce\") > 0,\n",
    "            1.0 - (pd.to_numeric(df[\"price_final\"], errors=\"coerce\") / pd.to_numeric(df[\"price_original\"], errors=\"coerce\")),\n",
    "            0.0\n",
    "        )\n",
    "        df[\"discount_ratio\"] = pd.Series(df[\"discount_ratio\"]).clip(0, 1).fillna(0.0)\n",
    "\n",
    "        # base features\n",
    "        keep_dense = [\n",
    "            \"win\",\"mac\",\"linux\",\"steam_deck\",\n",
    "            \"days_since_release\",\"review_year\",\"review_month\",\"review_dow\",\n",
    "            \"title_len\",\"desc_len\",\n",
    "            \"hours_log1p\",\"products_log1p\",\"reviews_log1p\",\n",
    "            \"price_final_log1p\",\"price_original_log1p\",\n",
    "            \"discount_ratio\",\"is_free\"\n",
    "        ]\n",
    "        X = df[keep_dense].copy()\n",
    "\n",
    "        # top-K tags (binary)\n",
    "        if \"tags\" in df.columns:\n",
    "            print(\"steam: build tag one-hot\")\n",
    "            tags_clean = (\n",
    "                df[\"tags\"].astype(\"string\").fillna(\"\").str.lower()\n",
    "                  .str.replace(r\"[\\[\\]\\\"]\", \"\", regex=True)\n",
    "                  .str.replace(\";\", \",\").str.replace(\"/\", \",\")\n",
    "            )\n",
    "            from sklearn.feature_extraction.text import CountVectorizer\n",
    "            vec = CountVectorizer(\n",
    "                tokenizer=lambda s: [t.strip() for t in s.split(\",\") if t.strip()],\n",
    "                lowercase=False, binary=True, max_features=max(0, 200 - X.shape[1])\n",
    "            )\n",
    "            mat = vec.fit_transform(tags_clean.values)\n",
    "            tag_df = pd.DataFrame(mat.toarray(), columns=[f\"tag_{t}\" for t in vec.get_feature_names_out()], index=df.index).astype(\"uint8\")\n",
    "            X = pd.concat([X, tag_df], axis=1)\n",
    "\n",
    "        # numeric fix\n",
    "        for c in X.select_dtypes(include=[np.number]).columns:\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(pd.to_numeric(X[c], errors=\"coerce\").median())\n",
    "\n",
    "        # targets\n",
    "        pos = pd.to_numeric(df.get(\"positive_ratio\"), errors=\"coerce\")\n",
    "        y_cls = (pos >= 80).astype(\"Int64\")\n",
    "        y_reg = pos.round().astype(\"Int64\")\n",
    "\n",
    "        # split by app_id time (avoid leakage)\n",
    "        raw = df[[\"date\", \"app_id\"]].copy()\n",
    "        raw = raw.loc[X.index]\n",
    "        raw[\"date\"] = pd.to_datetime(raw[\"date\"].astype(\"string\"), errors=\"coerce\")\n",
    "        cutoff = raw[\"date\"].sort_values().quantile(0.8)\n",
    "        first = raw.groupby(\"app_id\")[\"date\"].min()\n",
    "        app_train = set(first[first <= cutoff].index.tolist())\n",
    "        app_test  = set(first[first >  cutoff].index.tolist())\n",
    "        is_train = raw[\"app_id\"].isin(app_train)\n",
    "        is_test  = raw[\"app_id\"].isin(app_test) | (~(raw[\"app_id\"].isin(app_train) | raw[\"app_id\"].isin(app_test)))\n",
    "\n",
    "        def time_split(y):\n",
    "            X_tr = X[is_train].reset_index(drop=True)\n",
    "            X_te = X[is_test].reset_index(drop=True)\n",
    "            y_tr = y[is_train].reset_index(drop=True)\n",
    "            y_te = y[is_test].reset_index(drop=True)\n",
    "            return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "        steam_cls_split = time_split(y_cls.dropna().reindex(X.index))\n",
    "        steam_reg_split = time_split(y_reg.dropna().reindex(X.index))\n",
    "        return steam_cls_split, steam_reg_split\n",
    "\n",
    "    # ----- Olist -----\n",
    "    def prepare_olist(df_in):\n",
    "        o = df_in.copy()\n",
    "\n",
    "        # dates -> parts, then drop\n",
    "        def sdt(s): return pd.to_datetime(s.astype(\"string\"), errors=\"coerce\")\n",
    "        date_cols = [\n",
    "            \"order_purchase_timestamp\",\"order_approved_at\",\"order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\",\"order_estimated_delivery_date\",\"shipping_limit_date\",\n",
    "        ]\n",
    "        for c in date_cols:\n",
    "            o[c] = sdt(o[c]) if c in o.columns else pd.NaT\n",
    "        o[\"purchase_dayofweek\"] = o[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "        o[\"purchase_month\"] = o[\"order_purchase_timestamp\"].dt.month\n",
    "        o[\"purchase_hour\"] = o[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "        def to_h(td): return td.dt.total_seconds() / 3600.0\n",
    "        for c in [\"product_length_cm\",\"product_width_cm\",\"product_height_cm\",\"product_weight_g\",\n",
    "                  \"payment_installments_max\",\"payment_value_total\",\"payment_count\",\"freight_value\",\"order_item_id\"]:\n",
    "            if c in o.columns: o[c] = pd.to_numeric(o[c], errors=\"coerce\")\n",
    "        o[\"approval_delay_h\"] = to_h(o[\"order_approved_at\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_carrier_h\"] = to_h(o[\"order_delivered_carrier_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_customer_h\"] = to_h(o[\"order_delivered_customer_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"est_delivery_h\"] = to_h(o[\"order_estimated_delivery_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"limit_from_purchase_h\"] = to_h(o[\"shipping_limit_date\"] - o[\"order_purchase_timestamp\"])\n",
    "\n",
    "        if \"freight_value\" not in o.columns:\n",
    "            o[\"freight_value\"] = np.nan\n",
    "        o[\"product_volume_cm3\"] = o[\"product_length_cm\"] * o[\"product_width_cm\"] * o[\"product_height_cm\"]\n",
    "        o[\"density_g_per_cm3\"] = np.where(\n",
    "            (o[\"product_volume_cm3\"] > 0) & o[\"product_weight_g\"].notna(),\n",
    "            o[\"product_weight_g\"] / o[\"product_volume_cm3\"], np.nan\n",
    "        )\n",
    "\n",
    "        for c in [\"payment_installments_max\",\"payment_value_total\",\"payment_count\"]:\n",
    "            if c not in o.columns: o[c] = np.nan\n",
    "        o[\"avg_installment_value\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_installments_max\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_installments_max\"], np.nan\n",
    "        )\n",
    "        o[\"payment_value_per_payment\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_count\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_count\"], np.nan\n",
    "        )\n",
    "\n",
    "        if \"order_item_id\" not in o.columns: o[\"order_item_id\"] = 1\n",
    "        o[\"is_multi_item_order\"] = (pd.to_numeric(o[\"order_item_id\"], errors=\"coerce\") > 1).astype(\"Int64\")\n",
    "\n",
    "        # category and states\n",
    "        if \"product_category_name_english\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name_english\"].astype(\"string\")\n",
    "        elif \"product_category_name\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name\"].astype(\"string\")\n",
    "        else:\n",
    "            o[\"product_category\"] = \"Unknown\"\n",
    "\n",
    "        cat_cols = []\n",
    "        for c in [\"order_status\",\"customer_state\",\"seller_state\",\"product_category\"]:\n",
    "            if c in o.columns:\n",
    "                o[c] = o[c].astype(\"string\").fillna(\"Unknown\")\n",
    "                cat_cols.append(c)\n",
    "        if cat_cols:\n",
    "            o = pd.get_dummies(o, columns=cat_cols, dtype=np.uint8)\n",
    "\n",
    "        # targets\n",
    "        y_base = pd.to_numeric(df_in.get(\"review_score_mean_product\"), errors=\"coerce\")\n",
    "        if y_base.notna().any():\n",
    "            y_cls = (y_base >= 4.0).astype(\"Int64\")\n",
    "            y_reg = y_base.astype(\"Float64\")\n",
    "        else:\n",
    "            late = (o.get(\"order_delivered_customer_date\") > o.get(\"order_estimated_delivery_date\")).astype(\"Int64\")\n",
    "            y_cls = late\n",
    "            y_reg = late.astype(\"Float64\")\n",
    "\n",
    "        # drop ids and raw dates\n",
    "        drop_ids = [\"order_id\",\"customer_id\",\"customer_unique_id\",\"product_id\",\"seller_id\"]\n",
    "        for c in drop_ids:\n",
    "            if c in o.columns: o = o.drop(columns=[c])\n",
    "        for c in date_cols:\n",
    "            if c in o.columns: o = o.drop(columns=[c])\n",
    "\n",
    "        # numeric clean\n",
    "        for c in o.select_dtypes(include=[np.number]).columns:\n",
    "            o[c] = pd.to_numeric(o[c], errors=\"coerce\").fillna(pd.to_numeric(o[c], errors=\"coerce\").median())\n",
    "\n",
    "        # split\n",
    "        def split(y, stratify=False):\n",
    "            X = o.copy()\n",
    "            if stratify:\n",
    "                X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "            else:\n",
    "                X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "            return X_tr.reset_index(drop=True), X_te.reset_index(drop=True), y_tr.reset_index(drop=True), y_te.reset_index(drop=True)\n",
    "\n",
    "        olist_cls_split = split(y_cls.dropna(), stratify=True)\n",
    "        olist_reg_split = split(y_reg.dropna(), stratify=False)\n",
    "        return olist_cls_split, olist_reg_split\n",
    "\n",
    "    # ----- VG2019 -----\n",
    "    def prepare_sales(df_in):\n",
    "        s = df_in.copy()\n",
    "\n",
    "        # targets\n",
    "        if \"Critic_Score\" in s.columns:\n",
    "            cs = pd.to_numeric(s[\"Critic_Score\"], errors=\"coerce\")\n",
    "            y_cls = (cs > 8.0).astype(\"Int64\")\n",
    "            y_reg = cs.astype(\"Float64\")\n",
    "        else:\n",
    "            y_cls = pd.Series(np.nan, index=s.index, dtype=\"Int64\")\n",
    "            y_reg = pd.Series(np.nan, index=s.index, dtype=\"Float64\")\n",
    "\n",
    "        # remove leaks and junk\n",
    "        leak = [\"NA_Sales\",\"PAL_Sales\",\"JP_Sales\",\"Other_Sales\",\"Total_Shipped\",\"Rank\",\"Global_Sales\",\"Critic_Score\"]\n",
    "        junk = [\"VGChartz_Score\",\"Vgchartzscore\",\"url\",\"img_url\",\"status\",\"Last_Update\",\"basename\",\"User_Score\",\"Name\"]\n",
    "        s = s.drop(columns=[c for c in leak + junk if c in s.columns], errors=\"ignore\")\n",
    "\n",
    "        # platform family + handheld\n",
    "        def platform_family(p):\n",
    "            p = \"\" if pd.isna(p) else str(p).upper()\n",
    "            if p.startswith(\"PS\") or p in {\"PSP\",\"PSV\"}: return \"PlayStation\"\n",
    "            if p.startswith(\"X\") or p in {\"XB\",\"XBLA\"}: return \"Xbox\"\n",
    "            if p in {\"SWITCH\",\"WII\",\"WIIU\",\"GC\",\"N64\",\"SNES\",\"NES\",\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\"}: return \"Nintendo\"\n",
    "            if p == \"PC\": return \"PC\"\n",
    "            if p in {\"DC\",\"DREAMCAST\",\"SAT\",\"GEN\",\"MD\",\"MEGADRIVE\",\"GG\"}: return \"Sega\"\n",
    "            if \"ATARI\" in p: return \"Atari\"\n",
    "            return \"Other\"\n",
    "        if \"Platform\" in s.columns:\n",
    "            s[\"Platform_Family\"] = s[\"Platform\"].apply(platform_family)\n",
    "            handhelds = {\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\",\"PSP\",\"PSV\"}\n",
    "            s[\"is_portable\"] = s[\"Platform\"].astype(\"string\").str.upper().isin(handhelds).astype(\"Int64\")\n",
    "            s = s.drop(columns=[\"Platform\"])\n",
    "        else:\n",
    "            s[\"Platform_Family\"] = \"Other\"\n",
    "            s[\"is_portable\"] = pd.Series(0, index=s.index, dtype=\"Int64\")\n",
    "\n",
    "        # year + decade\n",
    "        if \"Year\" in s.columns:\n",
    "            s[\"Year\"] = pd.to_numeric(s[\"Year\"], errors=\"coerce\")\n",
    "            s.loc[~s[\"Year\"].between(1970, 2025, inclusive=\"both\"), \"Year\"] = np.nan\n",
    "            s[\"Decade\"] = (s[\"Year\"] // 10 * 10).astype(\"Int64\").astype(str)\n",
    "        else:\n",
    "            s[\"Year\"] = np.nan\n",
    "            s[\"Decade\"] = \"<NA>\"\n",
    "\n",
    "        # frequency encodes\n",
    "        for col in [\"Publisher\",\"Developer\"]:\n",
    "            if col in s.columns:\n",
    "                ser = s[col].astype(\"string\").fillna(\"Unknown\")\n",
    "                freq = ser.map(ser.value_counts(normalize=True))\n",
    "                s[col + \"_freq\"] = freq.astype(float)\n",
    "                s = s.drop(columns=[col])\n",
    "\n",
    "        # one-hot\n",
    "        for c in [\"Genre\",\"ESRB_Rating\",\"Platform_Family\",\"Decade\"]:\n",
    "            if c in s.columns:\n",
    "                s[c] = s[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        s = pd.get_dummies(s, columns=[c for c in [\"Genre\",\"ESRB_Rating\",\"Platform_Family\",\"Decade\"] if c in s.columns], dtype=np.uint8)\n",
    "\n",
    "        # numeric clean\n",
    "        for c in s.select_dtypes(include=[np.number]).columns:\n",
    "            s[c] = pd.to_numeric(s[c], errors=\"coerce\").fillna(pd.to_numeric(s[c], errors=\"coerce\").median())\n",
    "\n",
    "        # split\n",
    "        def split(y, stratify=False):\n",
    "            X = s.copy()\n",
    "            if stratify:\n",
    "                X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "            else:\n",
    "                X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "            return X_tr.reset_index(drop=True), X_te.reset_index(drop=True), y_tr.reset_index(drop=True), y_te.reset_index(drop=True)\n",
    "\n",
    "        sales_cls_split = split(y_cls.dropna(), stratify=True)\n",
    "        sales_reg_split = split(y_reg.dropna(), stratify=False)\n",
    "        return sales_cls_split, sales_reg_split\n",
    "\n",
    "    steam_cls, steam_reg = prepare_steam(steam)\n",
    "    olist_cls, olist_reg = prepare_olist(olist)\n",
    "    sales_cls, sales_reg = prepare_sales(sales)\n",
    "\n",
    "    # simple scaling (no polys here to keep it simple)\n",
    "    def scale_split(split):\n",
    "        X_tr, X_te, y_tr, y_te = split\n",
    "        num_cols = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if len(num_cols) == 0:\n",
    "            return split\n",
    "        scaler = StandardScaler()\n",
    "        tr_scaled = scaler.fit_transform(X_tr[num_cols].astype(\"float32\"))\n",
    "        te_scaled = scaler.transform(X_te[num_cols].astype(\"float32\"))\n",
    "        X_tr2 = X_tr.copy(); X_te2 = X_te.copy()\n",
    "        X_tr2[num_cols] = tr_scaled\n",
    "        X_te2[num_cols] = te_scaled\n",
    "        return (X_tr2, X_te2, y_tr, y_te)\n",
    "\n",
    "    return {\n",
    "        \"classification\": {\n",
    "            \"steam\": scale_split(steam_cls),\n",
    "            \"olist\": scale_split(olist_cls),\n",
    "            \"sales\": scale_split(sales_cls),\n",
    "        },\n",
    "        \"regression\": {\n",
    "            \"steam\": scale_split(steam_reg),\n",
    "            \"olist\": scale_split(olist_reg),\n",
    "            \"sales\": scale_split(sales_reg),\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"prepare: building splits...\")\n",
    "PREP = prepare_all(steam, olist, sales)\n",
    "print(\"prepare: done\")\n",
    "\n",
    "# =============================\n",
    "# Build full X,y for each task\n",
    "# =============================\n",
    "def get_full_xy(task_type):\n",
    "    data = PREP[task_type]\n",
    "    out = {}\n",
    "    for key in DATASET_KEYS:\n",
    "        X_tr, X_te, y_tr, y_te = data[key]\n",
    "        X_all = pd.concat([X_tr, X_te], axis=0, ignore_index=True)\n",
    "        y_all = pd.concat([pd.Series(y_tr), pd.Series(y_te)], axis=0, ignore_index=True)\n",
    "        out[key] = (X_all, y_all)\n",
    "    return out\n",
    "\n",
    "def stratified_sample_xy(X, y, n, seed):\n",
    "    if n >= len(X):\n",
    "        return X.copy(), y.copy()\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=len(X)-n, random_state=seed)\n",
    "    keep_idx, _ = next(splitter.split(X, y))\n",
    "    return X.iloc[keep_idx].reset_index(drop=True), y.iloc[keep_idx].reset_index(drop=True)\n",
    "\n",
    "def simple_sample_xy(X, y, n, seed):\n",
    "    if n >= len(X):\n",
    "        return X.copy(), y.copy()\n",
    "    rs = np.random.RandomState(seed)\n",
    "    keep_idx = rs.choice(len(X), size=n, replace=False)\n",
    "    return X.iloc[keep_idx].reset_index(drop=True), y.iloc[keep_idx].reset_index(drop=True)\n",
    "\n",
    "# =============================\n",
    "# True forward selection\n",
    "# =============================\n",
    "def prefilter_features(X, y, top_p, task_type):\n",
    "    p = min(int(top_p), X.shape[1])\n",
    "    if p <= 0:\n",
    "        return X.columns[:0]\n",
    "    if task_type == \"classification\":\n",
    "        scores = mutual_info_classif(X, y, random_state=RANDOM_STATE)\n",
    "        order = np.argsort(scores)[::-1][:p]\n",
    "    else:\n",
    "        sc, _ = f_regression(X, y)\n",
    "        sc = np.nan_to_num(sc, nan=0.0, neginf=0.0, posinf=np.nanmax(sc) if np.isfinite(np.nanmax(sc)) else 0.0)\n",
    "        order = np.argsort(sc)[::-1][:p]\n",
    "    return X.columns[order]\n",
    "\n",
    "def forward_select_order(X, y, max_k, task_type, inner_folds=3, prefilter_top=120, early_stop_rounds=8, min_delta=1e-6):\n",
    "    t0 = time.perf_counter()\n",
    "    pool_cols = prefilter_features(X, y, prefilter_top, task_type)\n",
    "    Xp = X[pool_cols].reset_index(drop=True)\n",
    "\n",
    "    selected = []\n",
    "    remain = list(Xp.columns)\n",
    "    no_improve = 0\n",
    "\n",
    "    if task_type == \"classification\":\n",
    "        base_est = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        scoring = \"f1_macro\"\n",
    "        last_best = -1e9\n",
    "    else:\n",
    "        base_est = RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        scoring = \"neg_root_mean_squared_error\"\n",
    "        last_best = 1e9  # store RMSE (lower is better)\n",
    "\n",
    "    while remain and len(selected) < int(max_k):\n",
    "        best_feat = None\n",
    "        best_score = None\n",
    "        for feat in remain:\n",
    "            cols_now = selected + [feat]\n",
    "            if task_type == \"classification\":\n",
    "                cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "            else:\n",
    "                cv = KFold(n_splits=inner_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "            scores = cross_val_score(base_est, Xp[cols_now], y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "            mean_score = float(scores.mean())\n",
    "            if task_type == \"regression\":\n",
    "                # convert to positive RMSE\n",
    "                mean_score = -mean_score\n",
    "                if (best_score is None) or (mean_score < best_score):\n",
    "                    best_score = mean_score\n",
    "                    best_feat = feat\n",
    "            else:\n",
    "                if (best_score is None) or (mean_score > best_score):\n",
    "                    best_score = mean_score\n",
    "                    best_feat = feat\n",
    "\n",
    "        selected.append(best_feat)\n",
    "        remain.remove(best_feat)\n",
    "\n",
    "        if task_type == \"classification\":\n",
    "            gain = best_score - (last_best if last_best != -1e9 else 0.0)\n",
    "            last_best = best_score\n",
    "            if gain <= min_delta:\n",
    "                no_improve += 1\n",
    "            else:\n",
    "                no_improve = 0\n",
    "        else:\n",
    "            gain = (last_best - best_score) if last_best != 1e9 else 1e9\n",
    "            last_best = best_score\n",
    "            if gain <= min_delta:\n",
    "                no_improve += 1\n",
    "            else:\n",
    "                no_improve = 0\n",
    "\n",
    "        if no_improve >= int(early_stop_rounds):\n",
    "            print(f\"forward: early stop at k={len(selected)} (no gain for {early_stop_rounds} steps)\")\n",
    "            break\n",
    "\n",
    "    t_sel = time.perf_counter() - t0\n",
    "    return selected, t_sel, list(pool_cols)\n",
    "\n",
    "FORWARD_CACHE = {\"classification\": {}, \"regression\": {}}\n",
    "\n",
    "def get_forward_order_cached(task_type, dataset_key, Xn, yn, max_k):\n",
    "    key = (dataset_key, len(Xn))\n",
    "    if key in FORWARD_CACHE[task_type]:\n",
    "        return FORWARD_CACHE[task_type][key]\n",
    "    order, t_sel, pool = forward_select_order(\n",
    "        Xn, yn, max_k=max_k, task_type=task_type,\n",
    "        inner_folds=CV_INNER, prefilter_top=FORWARD_PREFILTER_TOP,\n",
    "        early_stop_rounds=EARLY_STOP_ROUNDS, min_delta=MIN_DELTA\n",
    "    )\n",
    "    FORWARD_CACHE[task_type][key] = (order, t_sel, pool)\n",
    "    return FORWARD_CACHE[task_type][key]\n",
    "\n",
    "# =============================\n",
    "# Run sweeps\n",
    "# =============================\n",
    "def run_sweep(task_type, sample_sizes, k_values, seed):\n",
    "    xy = get_full_xy(task_type)\n",
    "    rows = []\n",
    "    for dataset_key in DATASET_KEYS:\n",
    "        X_all, y_all = xy[dataset_key]\n",
    "        for n in sample_sizes:\n",
    "            if task_type == \"classification\":\n",
    "                Xn, yn = stratified_sample_xy(X_all, y_all, n, seed)\n",
    "            else:\n",
    "                Xn, yn = simple_sample_xy(X_all, y_all, n, seed)\n",
    "\n",
    "            print(f\"[{task_type}] forward order • {dataset_key} • n={len(Xn)} • prefilter={FORWARD_PREFILTER_TOP} • inner_cv={CV_INNER}\")\n",
    "            order, time_select, pre_cols = get_forward_order_cached(task_type, dataset_key, Xn, yn, max(K_VALUES))\n",
    "\n",
    "            for k in k_values:\n",
    "                k = int(min(k, len(order)))\n",
    "                if k == 0:\n",
    "                    continue\n",
    "                cols_use = order[:k]\n",
    "                t0 = time.perf_counter()\n",
    "\n",
    "                if task_type == \"classification\":\n",
    "                    cv = StratifiedKFold(n_splits=CV_OUTER, shuffle=True, random_state=seed)\n",
    "                    model = RandomForestClassifier(n_estimators=300, random_state=seed, n_jobs=-1)\n",
    "                    f1_list = []\n",
    "                    for tr_idx, te_idx in cv.split(Xn[cols_use], yn):\n",
    "                        Xtr, Xte = Xn[cols_use].iloc[tr_idx], Xn[cols_use].iloc[te_idx]\n",
    "                        ytr, yte = yn.iloc[tr_idx], yn.iloc[te_idx]\n",
    "                        model.fit(Xtr, ytr)\n",
    "                        y_pred = model.predict(Xte)\n",
    "                        f1_list.append(f1_score(yte, y_pred, average=\"macro\"))\n",
    "                    time_cv = time.perf_counter() - t0\n",
    "                    row = {\n",
    "                        \"dataset\": dataset_key,\n",
    "                        \"sample_size\": int(len(Xn)),\n",
    "                        \"k_features\": k,\n",
    "                        \"f1_macro\": float(np.mean(f1_list)),\n",
    "                        \"time_select_sec_total\": float(time_select),\n",
    "                        \"time_cv_sec\": float(time_cv),\n",
    "                        \"time_total_sec\": float(time_select + time_cv)\n",
    "                    }\n",
    "                else:\n",
    "                    cv = KFold(n_splits=CV_OUTER, shuffle=True, random_state=seed)\n",
    "                    model = RandomForestRegressor(n_estimators=300, random_state=seed, n_jobs=-1)\n",
    "                    mae_list, rmse_list = [], []\n",
    "                    for tr_idx, te_idx in cv.split(Xn[cols_use], yn):\n",
    "                        Xtr, Xte = Xn[cols_use].iloc[tr_idx], Xn[cols_use].iloc[te_idx]\n",
    "                        ytr, yte = yn.iloc[tr_idx], yn.iloc[te_idx]\n",
    "                        model.fit(Xtr, ytr)\n",
    "                        y_pred = model.predict(Xte)\n",
    "                        mae_list.append(mean_absolute_error(yte, y_pred))\n",
    "                        rmse_list.append(math.sqrt(mean_squared_error(yte, y_pred)))\n",
    "                    time_cv = time.perf_counter() - t0\n",
    "                    row = {\n",
    "                        \"dataset\": dataset_key,\n",
    "                        \"sample_size\": int(len(Xn)),\n",
    "                        \"k_features\": k,\n",
    "                        \"mae\": float(np.mean(mae_list)),\n",
    "                        \"rmse\": float(np.mean(rmse_list)),\n",
    "                        \"time_select_sec_total\": float(time_select),\n",
    "                        \"time_cv_sec\": float(time_cv),\n",
    "                        \"time_total_sec\": float(time_select + time_cv)\n",
    "                    }\n",
    "\n",
    "                rows.append(row)\n",
    "                nice = {k: v for k, v in row.items() if k not in [\"dataset\",\"sample_size\",\"k_features\"]}\n",
    "                print(f\"[{task_type}] {dataset_key} | n={row['sample_size']} | k={row['k_features']} -> {nice}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =============================\n",
    "# Plot helpers\n",
    "# =============================\n",
    "def plot_metric_vs_k(df, task_type):\n",
    "    metric = \"f1_macro\" if task_type == \"classification\" else \"rmse\"\n",
    "    name = \"F1 Macro\" if task_type == \"classification\" else \"RMSE\"\n",
    "    for dataset_key in DATASET_KEYS:\n",
    "        sub = df[df[\"dataset\"] == dataset_key].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        for n in sorted(sub[\"sample_size\"].unique()):\n",
    "            part = sub[sub[\"sample_size\"] == n]\n",
    "            means = part.groupby(\"k_features\")[metric].mean().reset_index()\n",
    "            plt.plot(means[\"k_features\"], means[metric], marker=\"o\", label=f\"n={n}\")\n",
    "        plt.title(f\"{dataset_key.upper()} • {name} vs. #Features (forward)\")\n",
    "        plt.xlabel(\"#Features (k)\")\n",
    "        plt.ylabel(name)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def plot_time_vs_samples(df, task_type):\n",
    "    for dataset_key in DATASET_KEYS:\n",
    "        sub = df[df[\"dataset\"] == dataset_key].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        for k in sorted(sub[\"k_features\"].unique()):\n",
    "            part = sub[sub[\"k_features\"] == k]\n",
    "            means = part.groupby(\"sample_size\")[\"time_total_sec\"].mean().reset_index()\n",
    "            plt.plot(means[\"sample_size\"], means[\"time_total_sec\"], marker=\"o\", label=f\"k={k}\")\n",
    "        plt.title(f\"{dataset_key.upper()} • Total Time vs. Sample Size\")\n",
    "        plt.xlabel(\"Sample size\")\n",
    "        plt.ylabel(\"Time (sec)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def pick_best_rows(df, task_type):\n",
    "    best_rows = []\n",
    "    for dataset_key in DATASET_KEYS:\n",
    "        sub = df[df[\"dataset\"] == dataset_key].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        idx = sub[\"f1_macro\"].idxmax() if task_type == \"classification\" else sub[\"rmse\"].idxmin()\n",
    "        best_rows.append(sub.loc[idx])\n",
    "    return pd.DataFrame(best_rows).reset_index(drop=True)\n",
    "\n",
    "def refit_and_plot_top_features(task_type, dataset_key, sample_size, k):\n",
    "    xy = get_full_xy(task_type)\n",
    "    X_all, y_all = xy[dataset_key]\n",
    "    if task_type == \"classification\":\n",
    "        Xn, yn = stratified_sample_xy(X_all, y_all, sample_size, RANDOM_STATE)\n",
    "        order, _, _ = get_forward_order_cached(\"classification\", dataset_key, Xn, yn, max(K_VALUES))\n",
    "        model = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    else:\n",
    "        Xn, yn = simple_sample_xy(X_all, y_all, sample_size, RANDOM_STATE)\n",
    "        order, _, _ = get_forward_order_cached(\"regression\", dataset_key, Xn, yn, max(K_VALUES))\n",
    "        model = RandomForestRegressor(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "    k = int(min(k, len(order)))\n",
    "    cols = order[:k]\n",
    "    model.fit(Xn[cols], yn)\n",
    "    fi = getattr(model, \"feature_importances_\", None)\n",
    "    if fi is None:\n",
    "        return None\n",
    "    order_idx = np.argsort(fi)[::-1]\n",
    "    top = min(20, len(order_idx))\n",
    "    top_idx = order_idx[:top]\n",
    "    top_names = [cols[i] for i in top_idx]\n",
    "    top_vals = fi[top_idx]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(range(top), top_vals[::-1])\n",
    "    plt.yticks(range(top), [top_names[i] for i in range(top - 1, -1, -1)])\n",
    "    plt.title(f\"{dataset_key.upper()} • Top Features ({task_type}, k={k}, n={sample_size})\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pd.DataFrame({\"feature\": top_names, \"importance\": top_vals})\n",
    "\n",
    "# =============================\n",
    "# Run experiments\n",
    "# =============================\n",
    "print(\"starting classification sweep (forward)...\")\n",
    "cls_df = run_sweep(\"classification\", SAMPLE_SIZES, K_VALUES, RANDOM_STATE)\n",
    "print(\"classification sweep: done.\\n\")\n",
    "\n",
    "print(\"starting regression sweep (forward)...\")\n",
    "reg_df = run_sweep(\"regression\", SAMPLE_SIZES, K_VALUES, RANDOM_STATE)\n",
    "print(\"regression sweep: done.\\n\")\n",
    "\n",
    "# =============================\n",
    "# Tables\n",
    "# =============================\n",
    "print(\"\\n=== Classification (RandomForest + true forward selection) ===\")\n",
    "display(cls_df.sort_values([\"dataset\",\"sample_size\",\"k_features\"]).reset_index(drop=True))\n",
    "\n",
    "print(\"\\n=== Regression (RandomForest + true forward selection) ===\")\n",
    "display(reg_df.sort_values([\"dataset\",\"sample_size\",\"k_features\"]).reset_index(drop=True))\n",
    "\n",
    "# =============================\n",
    "# Graphs\n",
    "# =============================\n",
    "plot_metric_vs_k(cls_df, \"classification\")\n",
    "plot_metric_vs_k(reg_df, \"regression\")\n",
    "\n",
    "plot_time_vs_samples(cls_df, \"classification\")\n",
    "plot_time_vs_samples(reg_df, \"regression\")\n",
    "\n",
    "# =============================\n",
    "# Best combos + feature bars\n",
    "# =============================\n",
    "best_cls = pick_best_rows(cls_df, \"classification\")\n",
    "best_reg = pick_best_rows(reg_df, \"regression\")\n",
    "\n",
    "print(\"\\n=== Best settings (Classification) ===\")\n",
    "display(best_cls)\n",
    "\n",
    "print(\"\\n=== Best settings (Regression) ===\")\n",
    "display(best_reg)\n",
    "\n",
    "cls_importance_tables = {}\n",
    "reg_importance_tables = {}\n",
    "\n",
    "for _, r in best_cls.iterrows():\n",
    "    ds, n, k = str(r[\"dataset\"]), int(r[\"sample_size\"]), int(r[\"k_features\"])\n",
    "    tab = refit_and_plot_top_features(\"classification\", ds, n, k)\n",
    "    cls_importance_tables[ds] = tab\n",
    "\n",
    "for _, r in best_reg.iterrows():\n",
    "    ds, n, k = str(r[\"dataset\"]), int(r[\"sample_size\"]), int(r[\"k_features\"])\n",
    "    tab = refit_and_plot_top_features(\"regression\", ds, n, k)\n",
    "    reg_importance_tables[ds] = tab\n",
    "\n",
    "# =============================\n",
    "# Summary\n",
    "# =============================\n",
    "def summarize(df, task_type):\n",
    "    lines = []\n",
    "    for _, r in df.iterrows():\n",
    "        ds = str(r[\"dataset\"]).upper()\n",
    "        n = int(r[\"sample_size\"])\n",
    "        k = int(r[\"k_features\"])\n",
    "        tsel = float(r[\"time_select_sec_total\"])\n",
    "        tcv = float(r[\"time_cv_sec\"])\n",
    "        if task_type == \"classification\":\n",
    "            f1m = float(r[\"f1_macro\"])\n",
    "            lines.append(f\"{ds}: best k={k}, n={n}, F1={f1m:.4f}, select={tsel:.1f}s, cv={tcv:.1f}s\")\n",
    "        else:\n",
    "            mae = float(r[\"mae\"])\n",
    "            rmse = float(r[\"rmse\"])\n",
    "            lines.append(f\"{ds}: best k={k}, n={n}, RMSE={rmse:.4f}, MAE={mae:.4f}, select={tsel:.1f}s, cv={tcv:.1f}s\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"\\n================ SUMMARY (true forward selection) ================\\n\")\n",
    "print(\"Classification (higher F1 is better):\")\n",
    "print(summarize(best_cls, \"classification\"))\n",
    "print(\"\\nRegression (lower RMSE and MAE are better):\")\n",
    "print(summarize(best_reg, \"regression\"))\n",
    "print(\"\\nImportance tables in:\")\n",
    "print(\" - cls_importance_tables['steam'|'olist'|'sales']\")\n",
    "print(\" - reg_importance_tables['steam'|'olist'|'sales']\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
