{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a297232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandlercampbell/.venvs/default/lib/python3.13/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:19: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports (run once)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# imports (only run if not already imported)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # keeps things compatible\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas.api.types import is_integer_dtype\n",
    "\n",
    "\n",
    "\n",
    "# build a strong, simple pipeline\n",
    "def build_classification_pipeline(k_features=200, random_state=42):\n",
    "    pipeline = ImbPipeline([\n",
    "        (\"select\", SelectKBest(mutual_info_classif, k=k_features)),\n",
    "        (\"smote\", SMOTE(random_state=random_state)),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=600,\n",
    "            max_features=\"sqrt\",\n",
    "            n_jobs=-1,\n",
    "            random_state=random_state\n",
    "        ))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# find the best probability threshold using out-of-fold predictions\n",
    "def tune_threshold_oof(pipeline, X, y, n_splits=5, random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    oof_prob = np.zeros(len(y), dtype=float)\n",
    "\n",
    "    for train_index, valid_index in skf.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "\n",
    "        fitted = pipeline.fit(X_train, y_train)\n",
    "        prob_valid = fitted.predict_proba(X_valid)[:, 1]\n",
    "        oof_prob[valid_index] = prob_valid\n",
    "\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    best_f1 = -1.0\n",
    "    best_t = 0.5\n",
    "\n",
    "    for t in thresholds:\n",
    "        preds = (oof_prob >= t).astype(int)\n",
    "        score = f1_score(y, preds, average=\"macro\")\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_t = t\n",
    "\n",
    "    final_model = pipeline.fit(X, y)\n",
    "\n",
    "    return final_model, best_t, float(best_f1)\n",
    "\n",
    "\n",
    "# predict on new data with your tuned threshold\n",
    "def predict_labels_with_threshold(model, X_new, threshold):\n",
    "    prob = model.predict_proba(X_new)[:, 1]\n",
    "    return (prob >= threshold).astype(int)\n",
    "\n",
    "\n",
    "def clip_outliers_dataframe(df, lower_q=0.01, upper_q=0.99, keep_int=True):\n",
    "    df_clipped = df.copy()\n",
    "\n",
    "    # numeric columns (handles pandas nullable Int64)\n",
    "    numeric_cols = df_clipped.select_dtypes(include=[\"number\"]).columns\n",
    "    if len(numeric_cols) == 0:\n",
    "        return df_clipped\n",
    "\n",
    "    lows = df_clipped[numeric_cols].quantile(lower_q)\n",
    "    highs = df_clipped[numeric_cols].quantile(upper_q)\n",
    "\n",
    "    clipped = df_clipped[numeric_cols].clip(lower=lows, upper=highs, axis=1)\n",
    "\n",
    "    # keep original integer dtypes if you want to avoid downcasting warnings\n",
    "    if keep_int:\n",
    "        for col in numeric_cols:\n",
    "            original_dtype = df_clipped[col].dtype\n",
    "            if is_integer_dtype(original_dtype):\n",
    "                clipped[col] = clipped[col].round().astype(original_dtype)\n",
    "\n",
    "    df_clipped[numeric_cols] = clipped\n",
    "\n",
    "    return df_clipped\n",
    "\n",
    "# build the regression pipeline\n",
    "def build_regression_pipeline(k_features=300, random_state=42):\n",
    "    base = HistGradientBoostingRegressor(\n",
    "        loss=\"absolute_error\",\n",
    "        max_iter=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    reg = TransformedTargetRegressor(\n",
    "        regressor=base,\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"select\", SelectKBest(mutual_info_regression, k=k_features)),\n",
    "        (\"model\", reg)\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "# cross-validated MAE (lower is better)\n",
    "def cv_mae(pipe, X, y, n_splits=5, random_state=42):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    maes = []\n",
    "\n",
    "    for train_index, valid_index in kf.split(X):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        fitted = pipe.fit(X_train, y_train)\n",
    "        preds = fitted.predict(X_valid)\n",
    "        maes.append(mean_absolute_error(y_valid, preds))\n",
    "\n",
    "    return float(np.mean(maes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e204c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORTS AND GLOBALS ####\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import (\n",
    "    f_regression,      # for regression\n",
    "    f_classif,         # for classification\n",
    "    SelectKBest,\n",
    "    mutual_info_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Global Settings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random_state = RANDOM_STATE\n",
    "N_ROWS = 10_000\n",
    "\n",
    "# No scientific notation in pandas display\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "# Reproducibility for NumPy-based randomness\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892baa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Small Helpers (updated)\n",
    "# =============================\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "    return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bd5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb965d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Helpers\n",
    "# =============================\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KeepTrainColumns(BaseEstimator, TransformerMixin):\n",
    "    # remembers training columns and reindexes any input to match\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.keep_columns_ = list(X.columns)\n",
    "        else:\n",
    "            self.keep_columns_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.keep_columns_ is None:\n",
    "            return X\n",
    "        if hasattr(X, \"reindex\"):\n",
    "            return X.reindex(columns=self.keep_columns_, fill_value=0)\n",
    "        return X\n",
    "\n",
    "\n",
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    # turn scores into 0/1 using a chosen threshold\n",
    "    import numpy as np\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        raw = model.decision_function(X)\n",
    "        raw_min, raw_max = float(raw.min()), float(raw.max())\n",
    "        scores = (raw - raw_min) / (raw_max - raw_min + 1e-9)\n",
    "    else:\n",
    "        scores = model.predict(X).astype(float)\n",
    "    return (scores >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model builder + tuner (with oversampling + threshold tuning)\n",
    "# =============================\n",
    "def build_and_tune_models(\n",
    "    X_train, y_train,\n",
    "    task_type,\n",
    "    num_folds,\n",
    "    num_iterations,\n",
    "    oversample=False,\n",
    "    oversample_method=\"random\"\n",
    "):\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV, cross_val_predict\n",
    "    from sklearn.pipeline import Pipeline as SKPipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, f_classif, VarianceThreshold\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "    # optional oversampling tools\n",
    "    ImbPipeline = None\n",
    "    RandomOverSampler = None\n",
    "    SMOTE = None\n",
    "    if oversample and str(task_type).strip().lower() == \"classification\":\n",
    "        try:\n",
    "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "        except Exception:\n",
    "            print(\"imblearn not available. Oversampling disabled.\")\n",
    "            oversample = False\n",
    "\n",
    "    # neat prints\n",
    "    np.set_printoptions(suppress=True)\n",
    "    pd.options.display.float_format = lambda x: f\"{x:.6f}\"\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    # drop constant columns once on train\n",
    "    non_constant_columns = X_train.columns[X_train.nunique(dropna=False) > 1]\n",
    "    if len(non_constant_columns) < X_train.shape[1]:\n",
    "        dropped = X_train.shape[1] - len(non_constant_columns)\n",
    "        print(f\"Removed {dropped} constant feature(s).\")\n",
    "        X_train = X_train[non_constant_columns]\n",
    "\n",
    "    # task settings\n",
    "    task = str(task_type).strip().lower()\n",
    "    if task == \"classification\":\n",
    "        scoring = \"f1_macro\"\n",
    "        selector_score_func = f_classif\n",
    "        min_class = int(y_train.value_counts().min())\n",
    "        eff_folds = max(2, min(int(num_folds), min_class))\n",
    "        baseline_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        sampler_obj = None\n",
    "        if oversample:\n",
    "            if oversample_method == \"smote\":\n",
    "                k_neighbors_for_smote = max(1, min(5, min_class - 1))\n",
    "                if k_neighbors_for_smote < 1:\n",
    "                    print(\"SMOTE not possible (minority class too small). Using RandomOverSampler.\")\n",
    "                    sampler_obj = RandomOverSampler(random_state=42)\n",
    "                else:\n",
    "                    sampler_obj = SMOTE(random_state=42, k_neighbors=k_neighbors_for_smote)\n",
    "            else:\n",
    "                sampler_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "        class_weight_choice = None if oversample else \"balanced\"\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingClassifier(random_state=42),\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=42, class_weight=class_weight_choice, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeClassifier(random_state=42, class_weight=class_weight_choice),\n",
    "            \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"LinearSVM\": LinearSVC(max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"NaiveBayes\": GaussianNB(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"Dummy\": DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "        }\n",
    "        metric_name = \"F1_macro\"\n",
    "        higher_is_better = True\n",
    "    elif task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "        selector_score_func = f_regression\n",
    "        eff_folds = max(2, int(num_folds))\n",
    "        baseline_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingRegressor(random_state=42),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(max_iter=5000),\n",
    "            \"Lasso\": Lasso(max_iter=5000),\n",
    "            \"ElasticNet\": ElasticNet(max_iter=5000),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"Dummy\": DummyRegressor(strategy=\"mean\"),\n",
    "        }\n",
    "        metric_name = \"CV_MAE\"\n",
    "        higher_is_better = False\n",
    "        sampler_obj = None\n",
    "    else:\n",
    "        raise ValueError('task_type must be \"classification\" or \"regression\"')\n",
    "\n",
    "    total_features = X_train.shape[1]\n",
    "    feature_fractions = [0.10, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "    # which models need scaling and selection\n",
    "    needs_scaling = {\"LogisticRegression\", \"LinearSVM\", \"KNN\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"NaiveBayes\"}\n",
    "    skip_selection = {\"Dummy\"}\n",
    "    tree_like = {\"RandomForest\", \"DecisionTree\", \"GBT\"}\n",
    "\n",
    "    def k_from_fraction(frac, total_cols):\n",
    "        if frac >= 1.0:\n",
    "            return \"all\"\n",
    "        k = int(max(1, math.ceil(frac * total_cols)))\n",
    "        return min(k, total_cols)\n",
    "\n",
    "    # dynamic KNN neighbors cap\n",
    "    per_fold_train = int(len(X_train) * (eff_folds - 1) / eff_folds)\n",
    "    max_knn_k = max(3, min(101, per_fold_train - 1))\n",
    "    knn_ks = list(range(3, max_knn_k + 1, 2))\n",
    "\n",
    "    def logspace_list(low_exp, high_exp, num):\n",
    "        return list(np.logspace(low_exp, high_exp, num))\n",
    "\n",
    "    def linspace_list(low_val, high_val, num):\n",
    "        return list(np.linspace(low_val, high_val, num))\n",
    "\n",
    "    param_spaces_classification = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LogisticRegression\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__solver\": [\"saga\"],\n",
    "        },\n",
    "        \"LinearSVM\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        },\n",
    "        \"NaiveBayes\": {\n",
    "            \"model__var_smoothing\": list(10 ** np.linspace(-11, -7, 9))\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"most_frequent\", \"stratified\", \"uniform\"]},\n",
    "    }\n",
    "\n",
    "    param_spaces_regression = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LinearRegression\": {},\n",
    "        \"Ridge\": {\"model__alpha\": logspace_list(-3, 3, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"Lasso\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"ElasticNet\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__l1_ratio\": linspace_list(0.1, 0.9, 9), \"model__fit_intercept\": [True, False]},\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"mean\", \"median\"]},\n",
    "    }\n",
    "\n",
    "    # build a pipeline for a given k\n",
    "    def make_pipeline_for_k(model_name, model_obj, k_value):\n",
    "        # order: align -> impute -> variance -> select -> scale -> sampler -> model\n",
    "        align_step = (\"align\", KeepTrainColumns())\n",
    "        impute_step = (\"impute\", SimpleImputer(strategy=\"median\"))\n",
    "        variance_step = (\"variance\", VarianceThreshold(threshold=0.0))\n",
    "\n",
    "        if model_name in skip_selection or model_name in tree_like:\n",
    "            select_step = (\"select\", \"passthrough\")\n",
    "        else:\n",
    "            select_step = (\"select\", SelectKBest(score_func=selector_score_func, k=k_value))\n",
    "\n",
    "        scale_step = (\"scale\", StandardScaler() if model_name in needs_scaling else \"passthrough\")\n",
    "\n",
    "        steps = [align_step, impute_step, variance_step, select_step, scale_step]\n",
    "\n",
    "        if task == \"classification\" and oversample and sampler_obj is not None:\n",
    "            steps.append((\"sampler\", sampler_obj))\n",
    "\n",
    "        steps.append((\"model\", model_obj))\n",
    "\n",
    "        if oversample and ImbPipeline is not None and task == \"classification\":\n",
    "            return ImbPipeline(steps)\n",
    "        else:\n",
    "            return SKPipeline(steps)\n",
    "\n",
    "    # baseline sweep across models × k\n",
    "    rows = []\n",
    "    total_steps = len(model_space) * len(feature_fractions)\n",
    "    step = 0\n",
    "    print(\"Streaming results (each line is one model × feature count):\")\n",
    "    for model_name, model_obj in model_space.items():\n",
    "        for frac in feature_fractions:\n",
    "            step += 1\n",
    "            k_val = k_from_fraction(frac, total_features)\n",
    "            k_print = total_features if k_val == \"all\" else int(k_val)\n",
    "            pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=baseline_cv, scoring=scoring, n_jobs=1)\n",
    "            mean_score = float(np.mean(scores))\n",
    "            std_score = float(np.std(scores))\n",
    "            if task == \"regression\":\n",
    "                display_mean = -mean_score\n",
    "                display_std = float(np.std(-scores))\n",
    "            else:\n",
    "                display_mean = mean_score\n",
    "                display_std = std_score\n",
    "            rows.append({\"Model\": model_name, \"K_features\": k_print, \"MeanScore\": display_mean, \"StdDev\": display_std, \"Metric\": metric_name})\n",
    "            print(f\"[{step}/{total_steps}] {model_name} | k={k_print} | {metric_name}={display_mean:.6f} ± {display_std:.6f}\", flush=True)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    if task == \"classification\":\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    else:\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Baseline results (CV) ===\")\n",
    "    print(results_df[[\"Model\", \"K_features\", \"MeanScore\", \"StdDev\", \"Metric\"]])\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    best_model_name = str(best_row[\"Model\"])\n",
    "    best_k = int(best_row[\"K_features\"])\n",
    "    best_model_obj = model_space[best_model_name]\n",
    "    k_val_for_search = \"all\" if best_k >= total_features else best_k\n",
    "    best_pipeline = make_pipeline_for_k(best_model_name, best_model_obj, k_val_for_search)\n",
    "\n",
    "    # pick search space\n",
    "    search_space = (param_spaces_classification if task == \"classification\" else param_spaces_regression).get(best_model_name, {})\n",
    "    if len(search_space) == 0:\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        try:\n",
    "            best_pipeline.input_columns_ = list(X_train.columns)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # threshold tuning only for classification\n",
    "        if task == \"classification\":\n",
    "            try:\n",
    "                _tune_threshold_inplace(best_pipeline, X_train, y_train, search_cv)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] threshold tuning failed: {e}\")\n",
    "        print(\"\\nBest model had no tunable params. Returning fitted pipeline.\")\n",
    "        return best_pipeline\n",
    "\n",
    "    # hyperparameter search\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=best_pipeline,\n",
    "        param_distributions=search_space,\n",
    "        n_iter=int(max(1, num_iterations)),\n",
    "        scoring=scoring,\n",
    "        cv=search_cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # print tuned CV result\n",
    "    if task == \"regression\":\n",
    "        tuned_score_display = -float(search.best_score_)\n",
    "        tuned_metric_name = \"CV MAE\"\n",
    "    else:\n",
    "        tuned_score_display = float(search.best_score_)\n",
    "        tuned_metric_name = \"F1 macro\"\n",
    "\n",
    "    print(\"\\n=== Best model after randomized search ===\")\n",
    "    print(f\"Model name: {best_model_name}\")\n",
    "    print(f\"Number of features: {best_k}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best CV score ({tuned_metric_name}): {tuned_score_display:.6f}\")\n",
    "\n",
    "    # remember training columns\n",
    "    try:\n",
    "        search.best_estimator_.input_columns_ = list(X_train.columns)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # threshold tuning only for classification\n",
    "    if task == \"classification\":\n",
    "        try:\n",
    "            _tune_threshold_inplace(search.best_estimator_, X_train, y_train, search_cv)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] threshold tuning failed: {e}\")\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "def _tune_threshold_inplace(fitted_estimator, X, y, cv):\n",
    "    \"\"\"\n",
    "    Finds a good decision threshold using out-of-fold scores on the training set.\n",
    "    Stores results on the estimator as .best_threshold_ and .best_threshold_cv_f1_.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    # try probabilities first\n",
    "    scores = None\n",
    "    try:\n",
    "        proba_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"predict_proba\", n_jobs=1)  # shape (n, 2)\n",
    "        scores = proba_oof[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback to decision function\n",
    "    if scores is None:\n",
    "        try:\n",
    "            decision_oof = cross_val_predict(fitted_estimator, X, y, cv=cv, method=\"decision_function\", n_jobs=1)\n",
    "            dec_min, dec_max = float(decision_oof.min()), float(decision_oof.max())\n",
    "            scores = (decision_oof - dec_min) / (dec_max - dec_min + 1e-9)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # if no scores available, keep default threshold\n",
    "    if scores is None:\n",
    "        fitted_estimator.best_threshold_ = 0.5\n",
    "        fitted_estimator.best_threshold_cv_f1_ = None\n",
    "        print(\"[info] model does not expose scores for thresholding. Using 0.5.\")\n",
    "        return\n",
    "\n",
    "    # sweep thresholds\n",
    "    best_threshold = 0.5\n",
    "    best_f1_macro = -1.0\n",
    "    thresholds_to_try = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    for t in thresholds_to_try:\n",
    "        y_hat = (scores >= t).astype(int)\n",
    "        f1_macro_val = float(f1_score(y, y_hat, average=\"macro\"))\n",
    "        if f1_macro_val > best_f1_macro:\n",
    "            best_f1_macro = f1_macro_val\n",
    "            best_threshold = float(t)\n",
    "\n",
    "    # show OOF result at best threshold\n",
    "    y_hat_final = (scores >= best_threshold).astype(int)\n",
    "    print(\"\\n=== Threshold tuning (OOF on train) ===\")\n",
    "    print(f\"Best threshold: {best_threshold:.2f} | F1_macro: {best_f1_macro:.6f}\")\n",
    "    print(\"Confusion matrix at best threshold:\")\n",
    "    print(confusion_matrix(y, y_hat_final))\n",
    "\n",
    "    # store on estimator\n",
    "    fitted_estimator.best_threshold_ = best_threshold\n",
    "    fitted_estimator.best_threshold_cv_f1_ = best_f1_macro\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Holdout evaluation helper (uses tuned threshold if available)\n",
    "# =============================\n",
    "def evaluate_on_holdout(model, X_test, y_test, task_type, threshold=None):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import f1_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "    # align columns to what the model saw at fit\n",
    "    try:\n",
    "        if hasattr(model, \"input_columns_\") and hasattr(X_test, \"reindex\"):\n",
    "            X_test = X_test.reindex(columns=model.input_columns_, fill_value=0)\n",
    "        elif hasattr(model, \"named_steps\") and \"align\" in getattr(model, \"named_steps\", {}):\n",
    "            keep_cols = getattr(model.named_steps[\"align\"], \"keep_columns_\", None)\n",
    "            if keep_cols is not None and hasattr(X_test, \"reindex\"):\n",
    "                X_test = X_test.reindex(columns=list(keep_cols), fill_value=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not align columns: {e}\")\n",
    "\n",
    "    # choose prediction path\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        final_threshold = threshold\n",
    "        if final_threshold is None and hasattr(model, \"best_threshold_\"):\n",
    "            final_threshold = float(model.best_threshold_)\n",
    "        if final_threshold is not None:\n",
    "            y_pred = predict_with_threshold(model, X_test, threshold=final_threshold)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n=== Holdout (time split) ===\")\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(f\"F1 macro: {f1:.6f}\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        return f1\n",
    "    else:\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4631b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Helpers\n",
    "# =============================\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KeepTrainColumns(BaseEstimator, TransformerMixin):\n",
    "    # remembers training columns and reindexes any input to match\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.keep_columns_ = list(X.columns)\n",
    "        else:\n",
    "            self.keep_columns_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.keep_columns_ is None:\n",
    "            return X\n",
    "        if hasattr(X, \"reindex\"):\n",
    "            return X.reindex(columns=self.keep_columns_, fill_value=0)\n",
    "        return X\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model builder + tuner (with oversampling option)\n",
    "# =============================\n",
    "def build_and_tune_models(\n",
    "    X_train, y_train,\n",
    "    task_type,\n",
    "    num_folds,\n",
    "    num_iterations,\n",
    "    oversample=False,\n",
    "    oversample_method=\"random\"\n",
    "):\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV\n",
    "    from sklearn.pipeline import Pipeline as SKPipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, f_classif, VarianceThreshold\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "    # optional oversampling tools\n",
    "    ImbPipeline = None\n",
    "    RandomOverSampler = None\n",
    "    SMOTE = None\n",
    "    if oversample and str(task_type).strip().lower() == \"classification\":\n",
    "        try:\n",
    "            from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "            from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "        except Exception:\n",
    "            print(\"imblearn not available. Oversampling disabled.\")\n",
    "            oversample = False\n",
    "\n",
    "    # neat prints\n",
    "    np.set_printoptions(suppress=True)\n",
    "    pd.options.display.float_format = lambda x: f\"{x:.6f}\"\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    # drop constant columns once on train\n",
    "    non_constant_columns = X_train.columns[X_train.nunique(dropna=False) > 1]\n",
    "    if len(non_constant_columns) < X_train.shape[1]:\n",
    "        dropped = X_train.shape[1] - len(non_constant_columns)\n",
    "        print(f\"Removed {dropped} constant feature(s).\")\n",
    "        X_train = X_train[non_constant_columns]\n",
    "\n",
    "    # task settings\n",
    "    task = str(task_type).strip().lower()\n",
    "    if task == \"classification\":\n",
    "        scoring = \"f1_macro\"\n",
    "        selector_score_func = f_classif\n",
    "        min_class = int(y_train.value_counts().min())\n",
    "        eff_folds = max(2, min(int(num_folds), min_class))\n",
    "        baseline_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = StratifiedKFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        sampler_obj = None\n",
    "        if oversample:\n",
    "            if oversample_method == \"smote\":\n",
    "                k_neighbors_for_smote = max(1, min(5, min_class - 1))\n",
    "                if k_neighbors_for_smote < 1:\n",
    "                    print(\"SMOTE not possible (minority class too small). Using RandomOverSampler.\")\n",
    "                    sampler_obj = RandomOverSampler(random_state=42)\n",
    "                else:\n",
    "                    sampler_obj = SMOTE(random_state=42, k_neighbors=k_neighbors_for_smote)\n",
    "            else:\n",
    "                sampler_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "        class_weight_choice = None if oversample else \"balanced\"\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingClassifier(random_state=42),\n",
    "            \"RandomForest\": RandomForestClassifier(random_state=42, class_weight=class_weight_choice, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeClassifier(random_state=42, class_weight=class_weight_choice),\n",
    "            \"LogisticRegression\": LogisticRegression(solver=\"saga\", max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"LinearSVM\": LinearSVC(max_iter=5000, class_weight=class_weight_choice),\n",
    "            \"NaiveBayes\": GaussianNB(),\n",
    "            \"KNN\": KNeighborsClassifier(),\n",
    "            \"Dummy\": DummyClassifier(strategy=\"most_frequent\", random_state=42),\n",
    "        }\n",
    "        metric_name = \"F1_macro\"\n",
    "        higher_is_better = True\n",
    "    elif task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "        selector_score_func = f_regression\n",
    "        eff_folds = max(2, int(num_folds))\n",
    "        baseline_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "        search_cv = KFold(n_splits=eff_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        model_space = {\n",
    "            \"GBT\": GradientBoostingRegressor(random_state=42),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Ridge\": Ridge(max_iter=5000),\n",
    "            \"Lasso\": Lasso(max_iter=5000),\n",
    "            \"ElasticNet\": ElasticNet(max_iter=5000),\n",
    "            \"KNN\": KNeighborsRegressor(),\n",
    "            \"Dummy\": DummyRegressor(strategy=\"mean\"),\n",
    "        }\n",
    "        metric_name = \"CV_MAE\"\n",
    "        higher_is_better = False\n",
    "        sampler_obj = None\n",
    "    else:\n",
    "        raise ValueError('task_type must be \"classification\" or \"regression\"')\n",
    "\n",
    "    total_features = X_train.shape[1]\n",
    "    feature_fractions = [0.10, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "    # which models need scaling and selection\n",
    "    needs_scaling = {\"LogisticRegression\", \"LinearSVM\", \"KNN\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"NaiveBayes\"}\n",
    "    skip_selection = {\"Dummy\"}\n",
    "    tree_like = {\"RandomForest\", \"DecisionTree\", \"GBT\"}\n",
    "\n",
    "    def k_from_fraction(frac, total_cols):\n",
    "        if frac >= 1.0:\n",
    "            return \"all\"\n",
    "        k = int(max(1, math.ceil(frac * total_cols)))\n",
    "        return min(k, total_cols)\n",
    "\n",
    "    # dynamic KNN neighbors cap\n",
    "    per_fold_train = int(len(X_train) * (eff_folds - 1) / eff_folds)\n",
    "    max_knn_k = max(3, min(101, per_fold_train - 1))\n",
    "    knn_ks = list(range(3, max_knn_k + 1, 2))\n",
    "\n",
    "    def logspace_list(low_exp, high_exp, num):\n",
    "        return list(np.logspace(low_exp, high_exp, num))\n",
    "\n",
    "    def linspace_list(low_val, high_val, num):\n",
    "        return list(np.linspace(low_val, high_val, num))\n",
    "\n",
    "    param_spaces_classification = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LogisticRegression\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__penalty\": [\"l1\", \"l2\"],\n",
    "            \"model__solver\": [\"saga\"],\n",
    "        },\n",
    "        \"LinearSVM\": {\n",
    "            \"model__C\": logspace_list(-3, 3, 20),\n",
    "            \"model__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        },\n",
    "        \"NaiveBayes\": {\n",
    "            \"model__var_smoothing\": list(10 ** np.linspace(-11, -7, 9))\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"most_frequent\", \"stratified\", \"uniform\"]},\n",
    "    }\n",
    "\n",
    "    param_spaces_regression = {\n",
    "        \"GBT\": {\n",
    "            \"model__n_estimators\": [100, 200, 300, 500],\n",
    "            \"model__learning_rate\": logspace_list(-3, 0, 12),\n",
    "            \"model__max_depth\": [2, 3, 4, 5],\n",
    "            \"model__subsample\": linspace_list(0.6, 1.0, 5),\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model__n_estimators\": [200, 400, 700],\n",
    "            \"model__max_depth\": [None, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "        },\n",
    "        \"DecisionTree\": {\n",
    "            \"model__max_depth\": [None, 10, 20, 40],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__splitter\": [\"best\", \"random\"],\n",
    "        },\n",
    "        \"LinearRegression\": {},\n",
    "        \"Ridge\": {\"model__alpha\": logspace_list(-3, 3, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"Lasso\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__fit_intercept\": [True, False]},\n",
    "        \"ElasticNet\": {\"model__alpha\": logspace_list(-4, 1, 20), \"model__l1_ratio\": linspace_list(0.1, 0.9, 9), \"model__fit_intercept\": [True, False]},\n",
    "        \"KNN\": {\n",
    "            \"model__n_neighbors\": knn_ks,\n",
    "            \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            \"model__p\": [1, 2],\n",
    "            \"model__leaf_size\": list(range(10, 61, 10)),\n",
    "        },\n",
    "        \"Dummy\": {\"model__strategy\": [\"mean\", \"median\"]},\n",
    "    }\n",
    "\n",
    "    # build a pipeline for a given k\n",
    "    def make_pipeline_for_k(model_name, model_obj, k_value):\n",
    "        # order: align -> impute -> variance -> select -> scale -> sampler -> model\n",
    "        align_step = (\"align\", KeepTrainColumns())\n",
    "        impute_step = (\"impute\", SimpleImputer(strategy=\"median\"))\n",
    "        variance_step = (\"variance\", VarianceThreshold(threshold=0.0))\n",
    "\n",
    "        if model_name in skip_selection or model_name in tree_like:\n",
    "            select_step = (\"select\", \"passthrough\")\n",
    "        else:\n",
    "            select_step = (\"select\", SelectKBest(score_func=selector_score_func, k=k_value))\n",
    "\n",
    "        scale_step = (\"scale\", StandardScaler() if model_name in needs_scaling else \"passthrough\")\n",
    "\n",
    "        steps = [align_step, impute_step, variance_step, select_step, scale_step]\n",
    "\n",
    "        if task == \"classification\" and oversample and sampler_obj is not None:\n",
    "            steps.append((\"sampler\", sampler_obj))\n",
    "\n",
    "        steps.append((\"model\", model_obj))\n",
    "\n",
    "        if oversample and ImbPipeline is not None and task == \"classification\":\n",
    "            return ImbPipeline(steps)\n",
    "        else:\n",
    "            return SKPipeline(steps)\n",
    "\n",
    "    # baseline sweep across models × k\n",
    "    rows = []\n",
    "    total_steps = len(model_space) * len(feature_fractions)\n",
    "    step = 0\n",
    "    print(\"Streaming results (each line is one model × feature count):\")\n",
    "    for model_name, model_obj in model_space.items():\n",
    "        for frac in feature_fractions:\n",
    "            step += 1\n",
    "            k_val = k_from_fraction(frac, total_features)\n",
    "            k_print = total_features if k_val == \"all\" else int(k_val)\n",
    "            pipeline = make_pipeline_for_k(model_name, model_obj, k_val)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=baseline_cv, scoring=scoring, n_jobs=1)\n",
    "            mean_score = float(np.mean(scores))\n",
    "            std_score = float(np.std(scores))\n",
    "            if task == \"regression\":\n",
    "                display_mean = -mean_score\n",
    "                display_std = float(np.std(-scores))\n",
    "            else:\n",
    "                display_mean = mean_score\n",
    "                display_std = std_score\n",
    "            rows.append({\"Model\": model_name, \"K_features\": k_print, \"MeanScore\": display_mean, \"StdDev\": display_std, \"Metric\": metric_name})\n",
    "            print(f\"[{step}/{total_steps}] {model_name} | k={k_print} | {metric_name}={display_mean:.6f} ± {display_std:.6f}\", flush=True)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    if task == \"classification\":\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    else:\n",
    "        results_df = results_df.sort_values(by=[\"MeanScore\", \"Model\"], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Baseline results (CV) ===\")\n",
    "    print(results_df[[\"Model\", \"K_features\", \"MeanScore\", \"StdDev\", \"Metric\"]])\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    best_model_name = str(best_row[\"Model\"])\n",
    "    best_k = int(best_row[\"K_features\"])\n",
    "    best_model_obj = model_space[best_model_name]\n",
    "    k_val_for_search = \"all\" if best_k >= total_features else best_k\n",
    "    best_pipeline = make_pipeline_for_k(best_model_name, best_model_obj, k_val_for_search)\n",
    "\n",
    "    search_space = (param_spaces_classification if task == \"classification\" else param_spaces_regression).get(best_model_name, {})\n",
    "    if len(search_space) == 0:\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        try:\n",
    "            best_pipeline.input_columns_ = list(X_train.columns)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"\\nBest model had no tunable params. Returning fitted pipeline.\")\n",
    "        return best_pipeline\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=best_pipeline,\n",
    "        param_distributions=search_space,\n",
    "        n_iter=int(max(1, num_iterations)),\n",
    "        scoring=scoring,\n",
    "        cv=search_cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    if task == \"regression\":\n",
    "        tuned_score_display = -float(search.best_score_)\n",
    "        tuned_metric_name = \"CV MAE\"\n",
    "    else:\n",
    "        tuned_score_display = float(search.best_score_)\n",
    "        tuned_metric_name = \"F1 macro\"\n",
    "\n",
    "    print(\"\\n=== Best model after randomized search ===\")\n",
    "    print(f\"Model name: {best_model_name}\")\n",
    "    print(f\"Number of features: {best_k}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Best CV score ({tuned_metric_name}): {tuned_score_display:.6f}\")\n",
    "\n",
    "    try:\n",
    "        search.best_estimator_.input_columns_ = list(X_train.columns)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return search.best_estimator_\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Holdout evaluation helper\n",
    "# =============================\n",
    "def evaluate_on_holdout(model, X_test, y_test, task_type):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import f1_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "    # align columns to what the model saw at fit\n",
    "    try:\n",
    "        if hasattr(model, \"input_columns_\") and hasattr(X_test, \"reindex\"):\n",
    "            X_test = X_test.reindex(columns=model.input_columns_, fill_value=0)\n",
    "        elif hasattr(model, \"named_steps\") and \"align\" in getattr(model, \"named_steps\", {}):\n",
    "            keep_cols = getattr(model.named_steps[\"align\"], \"keep_columns_\", None)\n",
    "            if keep_cols is not None and hasattr(X_test, \"reindex\"):\n",
    "                X_test = X_test.reindex(columns=list(keep_cols), fill_value=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] could not align columns: {e}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n=== Holdout (time split) ===\")\n",
    "    if str(task_type).strip().lower() == \"classification\":\n",
    "        f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(f\"F1 macro: {f1:.6f}\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        return f1\n",
    "    else:\n",
    "        mae = float(mean_absolute_error(y_test, y_pred))\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        return mae\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Optional: simple threshold tuner for classification\n",
    "# =============================\n",
    "def choose_threshold(model, X, y, metric=\"f1_macro\"):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "    # get scores\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X)\n",
    "        s_min, s_max = float(s.min()), float(s.max())\n",
    "        scores = (s - s_min) / (s_max - s_min + 1e-9)\n",
    "    else:\n",
    "        # fallback: use predictions as scores\n",
    "        scores = model.predict(X).astype(float)\n",
    "\n",
    "    best_t = 0.5\n",
    "    best_val = -1.0\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_hat = (scores >= t).astype(int)\n",
    "        val = f1_score(y, y_hat, average=\"macro\") if metric == \"f1_macro\" else f1_score(y, y_hat)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_t = t\n",
    "\n",
    "    y_hat = (scores >= best_t).astype(int)\n",
    "    print(f\"best threshold: {best_t:.2f}, {metric}: {best_val:.6f}\")\n",
    "    print(\"confusion matrix at best threshold:\")\n",
    "    print(confusion_matrix(y, y_hat))\n",
    "    return best_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a4c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all(\n",
    "    steam, olist, sales,\n",
    "    top_k_tags=200, max_total_features=400,\n",
    "    test_size=0.2, random_state=42,\n",
    "    balance_method=\"none\",\n",
    "    min_class_ratio=0.5,\n",
    "    feature_select_method=\"none\",\n",
    "    feature_select_k=100,\n",
    "    task_type=\"classification\",\n",
    "    scale_method=\"none\",\n",
    "    poly_degree=2,\n",
    "    poly_interaction_only=False,\n",
    "    poly_include_bias=False,\n",
    "    poly_feature_limit=25\n",
    "):\n",
    "    def is_sparse_dtype(dtype):\n",
    "        return isinstance(dtype, pd.SparseDtype)\n",
    "\n",
    "    is_classification = str(task_type).lower().startswith(\"c\")\n",
    "\n",
    "    # forward-selected feature lists you provided\n",
    "    best_forward_features_raw = {\n",
    "        \"steam\": [\n",
    "            \"poly_price_final_log1p^2\", \"mac\", \"poly_price_original_log1p\",\n",
    "            \"tag_'early access'\", \"tag_'great soundtrack'\", \"poly_days_since_release\",\n",
    "            \"poly_hours_log1p^2\", \"tag_'2d'\", \"tag_'massively multiplayer'\",\n",
    "            \"tag_'free to play'\", \"tag_'cute'\", \"tag_'action rpg'\",\n",
    "            \"poly_price_original_log1p^2\", \"tag_'first-person'\", \"tag_'fast-paced'\",\n",
    "            \"poly_title_len price_final_log1p\", \"poly_days_since_release review_year\",\n",
    "            \"poly_products_log1p reviews_log1p\", \"tag_'mmorpg'\", \"tag_'puzzle'\",\n",
    "            \"tag_'pvp'\", \"poly_desc_len hours_log1p\", \"poly_hours_log1p\",\n",
    "            \"tag_'management'\", \"tag_'memes'\", \"tag_'relaxing'\", \"tag_'visual novel'\",\n",
    "            \"tag_'sexual content'\", \"tag_'difficult'\", \"tag_'emotional'\"\n",
    "        ],\n",
    "        \"olist\": [\n",
    "            \"poly_to_customer_h\", \"order_status_delivered\", \"order_status_shipped\",\n",
    "            \"poly_to_carrier_h\", \"seller_state_SP\", \"order_item_id\",\n",
    "            \"product_category_bed_bath_table\", \"product_category_office_furniture\",\n",
    "            \"product_category_furniture_decor\", \"poly_customer_zip_code_prefix\",\n",
    "            \"product_category_watches_gifts\", \"product_category_computers_accessories\",\n",
    "            \"product_category_telephony\", \"product_category_books_general_interest\",\n",
    "            \"product_category_home_confort\", \"poly_to_customer_h est_delivery_h\",\n",
    "            \"poly_to_customer_h price\", \"order_status_processing\", \"seller_state_RS\",\n",
    "            \"seller_state_PE\", \"poly_payment_value_per_payment approval_delay_h\",\n",
    "            \"product_category_audio\", \"poly_product_description_lenght product_height_cm\",\n",
    "            \"product_name_lenght\", \"product_category_stationery\",\n",
    "            \"product_category_luggage_accessories\", \"product_category_electronics\",\n",
    "            \"seller_state_MA\", \"product_category_Unknown\", \"product_category_baby\"\n",
    "        ],\n",
    "        \"sales\": [\n",
    "            \"poly_Developer_freq\", \"Genre_Action\", \"ESRB_Rating_M\", \"poly_Year^2\",\n",
    "            \"Decade_1970\", \"Platform_Family_PC\", \"poly_Developer_freq^2\",\n",
    "            \"Platform_Family_Nintendo\", \"poly_Publisher_freq\",\n",
    "            \"poly_Year Publisher_freq\", \"is_remaster\", \"Genre_Party\", \"Genre_Misc\",\n",
    "            \"Genre_Simulation\", \"is_portable\", \"Genre_Role-Playing\", \"Decade_1980\",\n",
    "            \"Genre_Strategy\", \"Genre_Fighting\", \"Genre_Puzzle\", \"poly_Year\",\n",
    "            \"Decade_<NA>\", \"poly_Year Developer_freq\", \"poly_Publisher_freq^2\",\n",
    "            \"Platform_Family_PlayStation\", \"Genre_Board Game\", \"Genre_Racing\",\n",
    "            \"Genre_Adventure\", \"Genre_Shooter\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # normalize names like \"tag_'great soundtrack'\" -> \"tag_great soundtrack\"\n",
    "    def _normalize_forward_name(name):\n",
    "        s = str(name).strip()\n",
    "        if s.startswith(\"tag_\"):\n",
    "            rest = s[4:].strip()\n",
    "            if (rest.startswith(\"'\") and rest.endswith(\"'\")) or (rest.startswith('\"') and rest.endswith('\"')):\n",
    "                rest = rest[1:-1]\n",
    "            return \"tag_\" + rest.lower()\n",
    "        return s\n",
    "\n",
    "    best_forward_features = {\n",
    "        k: [_normalize_forward_name(n) for n in v]\n",
    "        for k, v in best_forward_features_raw.items()\n",
    "    }\n",
    "\n",
    "    # make poly interaction names order-agnostic\n",
    "    def canonical_poly_name(name):\n",
    "        s = str(name)\n",
    "        if not s.startswith(\"poly_\"):\n",
    "            return s\n",
    "        body = s[5:]\n",
    "        if \"^\" in body and \" \" not in body:\n",
    "            return s  # keep squares like \"poly_x^2\"\n",
    "        tokens = body.split()\n",
    "        tokens = sorted(tokens)\n",
    "        return \"poly_\" + \" \".join(tokens)\n",
    "\n",
    "    def build_poly_canonical_map(cols):\n",
    "        col_map = {}\n",
    "        for c in cols:\n",
    "            if c.startswith(\"poly_\"):\n",
    "                col_map[canonical_poly_name(c)] = c\n",
    "            else:\n",
    "                col_map[c] = c\n",
    "        return col_map\n",
    "\n",
    "    # small debug print for missing columns (uses canonical poly names)\n",
    "    def debug_forward_missing(X, dataset_name, best_forward_features):\n",
    "        expect = best_forward_features.get(dataset_name, [])\n",
    "        col_map = build_poly_canonical_map(X.columns)\n",
    "        missing = [f for f in expect if canonical_poly_name(f) not in col_map]\n",
    "        if missing:\n",
    "            print(f\"[debug] {dataset_name} missing:\", missing)\n",
    "        return missing\n",
    "\n",
    "    def drop_known_leaks_from_features(X_in, y_in=None):\n",
    "        leak_names = {\"y_is_4_plus\", \"delivered_late\", \"target\", \"label\",\n",
    "                      \"target_olist\", \"target_sales\", \"target_steam\"}\n",
    "        to_drop = [c for c in X_in.columns if (c in leak_names) or c.lower().startswith(\"target\") or c.lower().startswith(\"label\")]\n",
    "        X_in = X_in.drop(columns=to_drop, errors=\"ignore\")\n",
    "        if is_classification and (y_in is not None):\n",
    "            y_series = pd.Series(y_in).reset_index(drop=True)\n",
    "            for c in list(X_in.columns):\n",
    "                xc = pd.Series(X_in[c]).reset_index(drop=True)\n",
    "                try:\n",
    "                    ux = set(pd.unique(xc.dropna()))\n",
    "                    uy = set(pd.unique(y_series.dropna()))\n",
    "                    if ux <= {0, 1} and uy <= {0, 1}:\n",
    "                        if (xc.astype(\"int8\") == y_series.astype(\"int8\")).all():\n",
    "                            X_in = X_in.drop(columns=[c])\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return X_in\n",
    "\n",
    "    def resample_binary(Xb, yb, method=\"oversample\", random_state=42):\n",
    "        counts = yb.value_counts(dropna=False)\n",
    "        if counts.shape[0] != 2:\n",
    "            return Xb, yb\n",
    "        majority_class = counts.idxmax()\n",
    "        minority_class = counts.idxmin()\n",
    "        majority_idx = yb[yb == majority_class].index.values\n",
    "        minority_idx = yb[yb == minority_class].index.values\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        if method == \"undersample\":\n",
    "            keep_majority = rng.choice(majority_idx, size=len(minority_idx), replace=False)\n",
    "            new_index = np.concatenate([minority_idx, keep_majority])\n",
    "        else:\n",
    "            need = int(len(majority_idx) - len(minority_idx))\n",
    "            need = max(0, need)\n",
    "            add_minority = rng.choice(minority_idx, size=need, replace=True)\n",
    "            new_index = np.concatenate([majority_idx, minority_idx, add_minority])\n",
    "        rng.shuffle(new_index)\n",
    "        Xb2 = Xb.loc[new_index].reset_index(drop=True)\n",
    "        yb2 = yb.loc[new_index].reset_index(drop=True)\n",
    "        return Xb2, yb2\n",
    "\n",
    "    def split_and_balance(df_in, target_col, balance_method, min_class_ratio, random_state):\n",
    "        X_in = df_in.drop(columns=[target_col]).copy()\n",
    "        y_in = df_in[target_col].copy()\n",
    "        X_in = drop_known_leaks_from_features(X_in, y_in)\n",
    "\n",
    "        dense_num_cols = [c for c in X_in.select_dtypes(include=[np.number]).columns\n",
    "                          if not is_sparse_dtype(X_in[c].dtype)]\n",
    "        for c in dense_num_cols:\n",
    "            X_in[c] = pd.to_numeric(X_in[c], errors=\"coerce\").fillna(pd.to_numeric(X_in[c], errors=\"coerce\").median())\n",
    "        obj_cols = X_in.select_dtypes(include=[\"object\"]).columns\n",
    "        for c in obj_cols:\n",
    "            X_in[c] = X_in[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        if is_classification:\n",
    "            if pd.Series(y_in).nunique() != 2:\n",
    "                raise ValueError(f\"{target_col}: for classification, target must have 2 classes.\")\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_in, y_in.astype(int), test_size=test_size, random_state=random_state, stratify=y_in\n",
    "            )\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_in, y_in, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "\n",
    "        if is_classification:\n",
    "            vc = y_tr.value_counts()\n",
    "            ratio = float(vc.min()) / float(vc.max()) if vc.max() > 0 else 1.0\n",
    "            if balance_method != \"none\" and ratio < min_class_ratio:\n",
    "                method = (\n",
    "                    \"oversample\" if balance_method == \"auto\" and len(X_tr) <= 200000\n",
    "                    else (\"undersample\" if balance_method == \"auto\" else balance_method)\n",
    "                )\n",
    "                X_tr, y_tr = resample_binary(X_tr, y_tr, method=method, random_state=random_state)\n",
    "            print(f\"{target_col} train class counts:\", y_tr.value_counts().to_dict())\n",
    "            print(f\"{target_col} test class counts:\", y_te.value_counts().to_dict())\n",
    "        else:\n",
    "            def sstats(s):\n",
    "                return {\"n\": int(s.shape[0]),\n",
    "                        \"mean\": float(np.nanmean(s)),\n",
    "                        \"std\": float(np.nanstd(s)),\n",
    "                        \"min\": float(np.nanmin(s)),\n",
    "                        \"max\": float(np.nanmax(s))}\n",
    "            print(f\"{target_col} train stats:\", sstats(y_tr))\n",
    "            print(f\"{target_col} test stats:\", sstats(y_te))\n",
    "        return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "    def apply_feature_selection(X_tr, y_tr, X_te, method, k, random_state, dataset_name=None):\n",
    "        if method == \"none\":\n",
    "            return X_tr, X_te\n",
    "\n",
    "        if method in {\"forward_best\", \"forward\", \"best_forward\"}:\n",
    "            feature_list = best_forward_features.get(dataset_name, [])\n",
    "            if not feature_list:\n",
    "                print(f\"forward_best: no saved list for {dataset_name}, using all features.\")\n",
    "                return X_tr, X_te\n",
    "\n",
    "            col_map = build_poly_canonical_map(X_tr.columns)\n",
    "            kept_cols = []\n",
    "            for want in feature_list:\n",
    "                key = canonical_poly_name(want)\n",
    "                if key in col_map:\n",
    "                    kept_cols.append(col_map[key])\n",
    "\n",
    "            missing = [f for f in feature_list if canonical_poly_name(f) not in col_map]\n",
    "            if missing:\n",
    "                print(f\"forward_best: {dataset_name} missing {len(missing)} of {len(feature_list)} saved features.\")\n",
    "\n",
    "            if not kept_cols:\n",
    "                print(f\"forward_best: none of the saved features found for {dataset_name}, using all features.\")\n",
    "                return X_tr, X_te\n",
    "\n",
    "            if isinstance(k, int) and k > 0:\n",
    "                kept_cols = kept_cols[: min(k, len(kept_cols))]\n",
    "\n",
    "            return X_tr[kept_cols].copy(), X_te[kept_cols].copy()\n",
    "\n",
    "        n_features = X_tr.shape[1]\n",
    "        if n_features <= 1 or k >= n_features:\n",
    "            return X_tr, X_te\n",
    "        k = int(max(1, min(k, n_features)))\n",
    "        try:\n",
    "            if method == \"mi\":\n",
    "                if is_classification:\n",
    "                    sel = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "                else:\n",
    "                    sel = SelectKBest(score_func=f_regression, k=k)\n",
    "                sel.fit(X_tr, y_tr)\n",
    "                kept_cols = X_tr.columns[sel.get_support()].tolist()\n",
    "            elif method == \"rf\":\n",
    "                if is_classification:\n",
    "                    rf = RandomForestClassifier(\n",
    "                        n_estimators=300, random_state=random_state, n_jobs=-1,\n",
    "                        class_weight=\"balanced_subsample\"\n",
    "                    )\n",
    "                else:\n",
    "                    rf = RandomForestRegressor(\n",
    "                        n_estimators=300, random_state=random_state, n_jobs=-1\n",
    "                    )\n",
    "                rf.fit(X_tr, y_tr)\n",
    "                order = np.argsort(rf.feature_importances_)[::-1][:k]\n",
    "                kept_cols = X_tr.columns[order].tolist()\n",
    "            else:\n",
    "                print(\"feature selection: unknown method, skipping\")\n",
    "                return X_tr, X_te\n",
    "            return X_tr[kept_cols].copy(), X_te[kept_cols].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"feature selection error ({method}): {e}. using all features.\")\n",
    "            return X_tr, X_te\n",
    "\n",
    "    # pick continuous columns, but always keep forced bases (even if low variance)\n",
    "    def pick_continuous_columns(X, max_cols, must_have=None):\n",
    "        sample = X.iloc[: min(10000, len(X))].copy()\n",
    "        num_cols = sample.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        def ok(col):\n",
    "            return (not str(X[col].dtype).startswith(\"uint8\")) and (sample[col].nunique(dropna=False) > 2)\n",
    "\n",
    "        must_have = [c for c in (must_have or []) if c in X.columns and not str(X[c].dtype).startswith(\"uint8\")]\n",
    "        rest = [c for c in num_cols if ok(c) and c not in must_have]\n",
    "        if rest:\n",
    "            var_series = sample[rest].var().sort_values(ascending=False)\n",
    "            rest_sorted = var_series.index.tolist()\n",
    "        else:\n",
    "            rest_sorted = []\n",
    "        cols = (must_have + [c for c in rest_sorted if c not in must_have])[: max_cols]\n",
    "        return cols\n",
    "\n",
    "    def apply_scaling_and_poly(X_tr, X_te):\n",
    "        # bases needed by your forward polys on Steam\n",
    "        steam_poly_must_have = [\n",
    "            \"price_final_log1p\", \"price_original_log1p\",\n",
    "            \"days_since_release\", \"review_year\",\n",
    "            \"hours_log1p\", \"products_log1p\", \"reviews_log1p\",\n",
    "            \"title_len\", \"desc_len\"\n",
    "        ]\n",
    "        cont_cols = pick_continuous_columns(X_tr, poly_feature_limit, must_have=steam_poly_must_have)\n",
    "        X_tr = X_tr.copy()\n",
    "        X_te = X_te.copy()\n",
    "\n",
    "        do_scale = (scale_method in {\"standard\", \"minmax\"}) and bool(cont_cols)\n",
    "        do_poly = (poly_degree is not None) and (int(poly_degree) >= 2 or bool(poly_interaction_only)) and bool(cont_cols)\n",
    "\n",
    "        if not cont_cols:\n",
    "            return X_tr, X_te\n",
    "\n",
    "        tr_cont = X_tr[cont_cols].astype(\"float32\").values\n",
    "        te_cont = X_te[cont_cols].astype(\"float32\").values\n",
    "\n",
    "        if do_scale:\n",
    "            if scale_method == \"standard\":\n",
    "                scaler = StandardScaler()\n",
    "            elif scale_method == \"minmax\":\n",
    "                scaler = MinMaxScaler()\n",
    "            else:\n",
    "                scaler = None\n",
    "            if scaler is not None:\n",
    "                tr_cont = scaler.fit_transform(tr_cont).astype(\"float32\")\n",
    "                te_cont = scaler.transform(te_cont).astype(\"float32\")\n",
    "                print(f\"scaled columns: {len(cont_cols)} with {scale_method}\")\n",
    "\n",
    "        if do_poly:\n",
    "            poly = PolynomialFeatures(\n",
    "                degree=int(poly_degree),\n",
    "                include_bias=bool(poly_include_bias),\n",
    "                interaction_only=bool(poly_interaction_only)\n",
    "            )\n",
    "            tr_poly = poly.fit_transform(tr_cont).astype(\"float32\")\n",
    "            te_poly = poly.transform(te_cont).astype(\"float32\")\n",
    "            poly_names = [f\"poly_{n}\" for n in poly.get_feature_names_out(cont_cols)]\n",
    "            tr_poly_df = pd.DataFrame(tr_poly, columns=poly_names, index=X_tr.index)\n",
    "            te_poly_df = pd.DataFrame(te_poly, columns=poly_names, index=X_te.index)\n",
    "            X_tr = X_tr.drop(columns=cont_cols).join(tr_poly_df)\n",
    "            X_te = X_te.drop(columns=cont_cols).join(te_poly_df)\n",
    "            print(f\"polynomial features added: {len(poly_names)} (from {len(cont_cols)} columns)\")\n",
    "            return X_tr, X_te\n",
    "\n",
    "        if do_scale:\n",
    "            tr_scaled_df = pd.DataFrame(tr_cont, columns=cont_cols, index=X_tr.index).astype(\"float32\")\n",
    "            te_scaled_df = pd.DataFrame(te_cont, columns=cont_cols, index=X_te.index).astype(\"float32\")\n",
    "            X_tr = X_tr.drop(columns=cont_cols).join(tr_scaled_df)\n",
    "            X_te = X_te.drop(columns=cont_cols).join(te_scaled_df)\n",
    "\n",
    "        return X_tr, X_te\n",
    "\n",
    "    def prepare_steam_df(steam_in):\n",
    "        print(\"prep: steam\")\n",
    "        df = steam_in.copy()\n",
    "        for col in [\"title\", \"description\", \"tags\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(\"string\")\n",
    "\n",
    "        def safe_to_datetime(series):\n",
    "            return pd.to_datetime(series.astype(\"string\"), errors=\"coerce\")\n",
    "        for c in [\"date\", \"date_release\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = safe_to_datetime(df[c])\n",
    "            else:\n",
    "                df[c] = pd.NaT\n",
    "\n",
    "        df[\"days_since_release\"] = (df[\"date\"] - df[\"date_release\"]).dt.days\n",
    "        df[\"days_since_release\"] = df[\"days_since_release\"].clip(lower=0).fillna(0).astype(\"int32\")\n",
    "        df[\"review_year\"] = df[\"date\"].dt.year.astype(\"float64\")\n",
    "        df[\"review_month\"] = df[\"date\"].dt.month.astype(\"float64\")\n",
    "        df[\"review_dow\"] = df[\"date\"].dt.dayofweek.astype(\"float64\")\n",
    "        df[\"review_year\"] = df[\"review_year\"].fillna(-1).astype(\"int16\")\n",
    "        df[\"review_month\"] = df[\"review_month\"].fillna(-1).astype(\"int8\")\n",
    "        df[\"review_dow\"] = df[\"review_dow\"].fillna(-1).astype(\"int8\")\n",
    "\n",
    "        df[\"title_len\"] = df[\"title\"].astype(\"string\").str.len().fillna(0).astype(\"int32\")\n",
    "        df[\"desc_len\"]  = df[\"description\"].astype(\"string\").str.len().fillna(0).astype(\"int32\")\n",
    "\n",
    "        for col in [\"hours\",\"products\",\"reviews\",\"price_final\",\"price_original\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df[col + \"_log1p\"] = np.log1p(df[col])\n",
    "\n",
    "        df[\"is_free\"] = (df[\"price_final\"] == 0).astype(\"int8\")\n",
    "        df[\"discount_ratio\"] = np.where(\n",
    "            pd.to_numeric(df[\"price_original\"], errors=\"coerce\") > 0,\n",
    "            1.0 - (pd.to_numeric(df[\"price_final\"], errors=\"coerce\") /\n",
    "                   pd.to_numeric(df[\"price_original\"], errors=\"coerce\")),\n",
    "            0.0\n",
    "        )\n",
    "        df[\"discount_ratio\"] = pd.Series(df[\"discount_ratio\"]).clip(0, 1).fillna(0.0)\n",
    "\n",
    "        for b in [\"win\",\"mac\",\"linux\",\"steam_deck\"]:\n",
    "            if b in df.columns:\n",
    "                df[b] = (df[b] == True).astype(\"int8\")\n",
    "            else:\n",
    "                df[b] = 0\n",
    "\n",
    "        pos = pd.to_numeric(df.get(\"positive_ratio\"), errors=\"coerce\")\n",
    "        if is_classification:\n",
    "            y = (pos >= 80).astype(\"Int64\")\n",
    "        else:\n",
    "            y = pos.round().astype(\"Int64\")\n",
    "\n",
    "        keep_dense = [\n",
    "            \"win\",\"mac\",\"linux\",\"steam_deck\",\n",
    "            \"days_since_release\",\"review_year\",\"review_month\",\"review_dow\",\n",
    "            \"title_len\",\"desc_len\",\n",
    "            \"hours_log1p\",\"products_log1p\",\"reviews_log1p\",\n",
    "            \"price_final_log1p\",\"price_original_log1p\",\n",
    "            \"discount_ratio\",\"is_free\"\n",
    "        ]\n",
    "        X = df[keep_dense].copy()\n",
    "\n",
    "        base_cols_count = X.shape[1]\n",
    "        allowed_tag_cols = max(0, min(top_k_tags, max_total_features - base_cols_count))\n",
    "        if allowed_tag_cols > 0 and \"tags\" in df.columns:\n",
    "            print(\"prep: steam build sparse tag matrix\")\n",
    "            tags_clean = (\n",
    "                df[\"tags\"].astype(\"string\").fillna(\"\").str.lower()\n",
    "                  .str.replace(r\"[\\[\\]\\\"]\", \"\", regex=True)\n",
    "                  .str.replace(\";\", \",\").str.replace(\"/\", \",\")\n",
    "            )\n",
    "            vec = CountVectorizer(\n",
    "                tokenizer=lambda s: [t.strip() for t in s.split(\",\") if t.strip()],\n",
    "                lowercase=False, binary=True, max_features=allowed_tag_cols\n",
    "            )\n",
    "            tag_sparse = vec.fit_transform(tags_clean.values)\n",
    "            tag_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "                tag_sparse,\n",
    "                columns=[f\"tag_{t}\" for t in vec.get_feature_names_out()],\n",
    "                index=df.index\n",
    "            ).astype(pd.SparseDtype(\"uint8\", 0))\n",
    "            X = pd.concat([X, tag_df], axis=1)\n",
    "\n",
    "            # force-create chosen tags even if not in top-K\n",
    "            force_tags = [\n",
    "                \"early access\",\"great soundtrack\",\"2d\",\"massively multiplayer\",\"free to play\",\"cute\",\n",
    "                \"action rpg\",\"first-person\",\"fast-paced\",\"mmorpg\",\"puzzle\",\"pvp\",\"management\",\"memes\",\n",
    "                \"relaxing\",\"visual novel\",\"sexual content\",\"difficult\",\"emotional\"\n",
    "            ]\n",
    "            def has_tag(series, t):\n",
    "                pattern = rf\"(?:^|,)\\s*{re.escape(t)}\\s*(?:,|$)\"\n",
    "                return series.str.contains(pattern, regex=True)\n",
    "\n",
    "            for t in force_tags:\n",
    "                col = f\"tag_{t}\"\n",
    "                if col not in X.columns:\n",
    "                    X[col] = has_tag(tags_clean, t).astype(\"uint8\")\n",
    "\n",
    "            # alias variants -> OR into canonical columns\n",
    "            tag_aliases = {\n",
    "                \"first-person\": [\"first person\"],\n",
    "                \"action rpg\": [\"action-rpg\"],\n",
    "                \"mmorpg\": [\"mmo rpg\", \"mmo-rpg\"]\n",
    "            }\n",
    "            for t, alts in tag_aliases.items():\n",
    "                col = f\"tag_{t}\"\n",
    "                if col not in X.columns:\n",
    "                    X[col] = has_tag(tags_clean, t).astype(\"uint8\")\n",
    "                for a in alts:\n",
    "                    alias_hits = has_tag(tags_clean, a).astype(\"uint8\")\n",
    "                    if alias_hits.any():\n",
    "                        X[col] = (X[col].astype(\"uint8\") | alias_hits).astype(\"uint8\")\n",
    "\n",
    "        if any(is_sparse_dtype(X[c].dtype) for c in X.columns):\n",
    "            for c in X.columns:\n",
    "                if is_sparse_dtype(X[c].dtype):\n",
    "                    X[c] = X[c].sparse.to_dense().astype(\"uint8\")\n",
    "\n",
    "        dense_num_cols = [c for c in X.select_dtypes(include=[np.number]).columns\n",
    "                          if not is_sparse_dtype(X[c].dtype)]\n",
    "        for c in dense_num_cols:\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(pd.to_numeric(X[c], errors=\"coerce\").median())\n",
    "\n",
    "        raw = df[[\"date\",\"app_id\"]].copy()\n",
    "        out_df = X.copy()\n",
    "        out_df[\"target_steam\"] = y\n",
    "        out_df = out_df[out_df[\"target_steam\"].notna()].copy()\n",
    "        if is_classification:\n",
    "            out_df[\"target_steam\"] = out_df[\"target_steam\"].astype(\"int8\")\n",
    "        else:\n",
    "            out_df[\"target_steam\"] = out_df[\"target_steam\"].astype(\"int16\")\n",
    "        return out_df, raw\n",
    "\n",
    "    def prepare_olist_df(olist_in):\n",
    "        print(\"prep: olist\")\n",
    "        o = olist_in.copy()\n",
    "\n",
    "        def safe_to_datetime(series):\n",
    "            return pd.to_datetime(series.astype(\"string\"), errors=\"coerce\")\n",
    "        date_cols = [\n",
    "            \"order_purchase_timestamp\",\"order_approved_at\",\"order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\",\"order_estimated_delivery_date\",\"shipping_limit_date\",\n",
    "        ]\n",
    "        for c in date_cols:\n",
    "            if c in o.columns:\n",
    "                o[c] = safe_to_datetime(o[c])\n",
    "            else:\n",
    "                o[c] = pd.NaT\n",
    "\n",
    "        o[\"purchase_dayofweek\"] = o[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "        o[\"purchase_month\"] = o[\"order_purchase_timestamp\"].dt.month\n",
    "        o[\"purchase_hour\"] = o[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "        def to_hours(td):\n",
    "            return td.dt.total_seconds() / 3600.0\n",
    "\n",
    "        for c in [\"product_length_cm\",\"product_width_cm\",\"product_height_cm\",\"product_weight_g\",\n",
    "                  \"payment_installments_max\",\"payment_value_total\",\"payment_count\",\"freight_value\",\"order_item_id\"]:\n",
    "            if c in o.columns:\n",
    "                o[c] = pd.to_numeric(o[c], errors=\"coerce\")\n",
    "\n",
    "        o[\"approval_delay_h\"] = to_hours(o[\"order_approved_at\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_carrier_h\"] = to_hours(o[\"order_delivered_carrier_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_customer_h\"] = to_hours(o[\"order_delivered_customer_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"est_delivery_h\"] = to_hours(o[\"order_estimated_delivery_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"limit_from_purchase_h\"] = to_hours(o[\"shipping_limit_date\"] - o[\"order_purchase_timestamp\"])\n",
    "\n",
    "        o[\"delivered_late\"] = (o[\"order_delivered_customer_date\"] > o[\"order_estimated_delivery_date\"]).astype(\"Int64\")\n",
    "\n",
    "        for c in [\"product_length_cm\",\"product_width_cm\",\"product_height_cm\",\"product_weight_g\"]:\n",
    "            if c not in o.columns:\n",
    "                o[c] = np.nan\n",
    "        o[\"product_volume_cm3\"] = o[\"product_length_cm\"] * o[\"product_width_cm\"] * o[\"product_height_cm\"]\n",
    "        o[\"density_g_per_cm3\"] = np.where(\n",
    "            (o[\"product_volume_cm3\"] > 0) & o[\"product_weight_g\"].notna(),\n",
    "            o[\"product_weight_g\"] / o[\"product_volume_cm3\"],\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        for c in [\"payment_installments_max\",\"payment_value_total\",\"payment_count\"]:\n",
    "            if c not in o.columns:\n",
    "                o[c] = np.nan\n",
    "        o[\"avg_installment_value\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_installments_max\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_installments_max\"],\n",
    "            np.nan,\n",
    "        )\n",
    "        o[\"payment_value_per_payment\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_count\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_count\"],\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        if \"freight_value\" not in o.columns:\n",
    "            o[\"freight_value\"] = np.nan\n",
    "        o[\"freight_per_kg\"] = np.where(\n",
    "            pd.to_numeric(o[\"product_weight_g\"], errors=\"coerce\") > 0,\n",
    "            o[\"freight_value\"] / (o[\"product_weight_g\"] / 1000.0),\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        if \"order_item_id\" not in o.columns:\n",
    "            o[\"order_item_id\"] = 1\n",
    "        o[\"is_multi_item_order\"] = (pd.to_numeric(o[\"order_item_id\"], errors=\"coerce\") > 1).astype(\"Int64\")\n",
    "\n",
    "        if \"product_category_name\" in o.columns:\n",
    "            o[\"product_category_name\"] = o[\"product_category_name\"].astype(\"string\")\n",
    "        if \"product_category_name_english\" in o.columns:\n",
    "            o[\"product_category_name_english\"] = o[\"product_category_name_english\"].astype(\"string\")\n",
    "        if \"product_category_name\" in o.columns and \"product_category_name_english\" in o.columns:\n",
    "            s1 = o[\"product_category_name_english\"]\n",
    "            s2 = o[\"product_category_name\"]\n",
    "            o[\"product_category\"] = s1.mask(s1.isna() | (s1 == \"\"), s2)\n",
    "            o = o.drop(columns=[\"product_category_name\",\"product_category_name_english\"])\n",
    "        elif \"product_category_name_english\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name_english\"]\n",
    "            o = o.drop(columns=[\"product_category_name_english\"])\n",
    "        elif \"product_category_name\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name\"]\n",
    "            o = o.drop(columns=[\"product_category_name\"])\n",
    "        else:\n",
    "            o[\"product_category\"] = \"Unknown\"\n",
    "\n",
    "        for col in [\"customer_city\",\"seller_city\"]:\n",
    "            if col in o.columns:\n",
    "                s_ = o[col].astype(\"string\").fillna(\"Unknown\")\n",
    "                freq = s_.map(s_.value_counts(normalize=True))\n",
    "                o[col + \"_freq\"] = freq.astype(float)\n",
    "                o = o.drop(columns=[col])\n",
    "\n",
    "        cat_cols = []\n",
    "        for c in [\"order_status\",\"customer_state\",\"seller_state\",\"product_category\"]:\n",
    "            if c in o.columns:\n",
    "                cat_cols.append(c)\n",
    "                o[c] = o[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        if cat_cols:\n",
    "            o = pd.get_dummies(o, columns=cat_cols, dtype=np.uint8)\n",
    "\n",
    "        if \"review_score_mean_product\" in olist_in.columns:\n",
    "            base = pd.to_numeric(olist_in[\"review_score_mean_product\"], errors=\"coerce\")\n",
    "            if is_classification:\n",
    "                y = (base >= 4.0).astype(\"Int64\")\n",
    "            else:\n",
    "                y = base.astype(\"Float64\")\n",
    "        else:\n",
    "            print(\"olist: 'review_score_mean_product' missing; falling back to delivered_late.\")\n",
    "            y = o[\"delivered_late\"].copy()\n",
    "\n",
    "        o[\"target_olist\"] = y\n",
    "        o = o[o[\"target_olist\"].notna()].copy()\n",
    "        if is_classification:\n",
    "            o[\"target_olist\"] = o[\"target_olist\"].astype(\"int8\")\n",
    "        else:\n",
    "            o[\"target_olist\"] = o[\"target_olist\"].astype(\"float32\")\n",
    "\n",
    "        for leak_col in [\"review_score_mean_product\",\"review_count_product\",\"review_score_mean\",\"delivered_late\"]:\n",
    "            if leak_col in o.columns:\n",
    "                o = o.drop(columns=[leak_col])\n",
    "\n",
    "        drop_ids = [\"order_id\",\"customer_id\",\"customer_unique_id\",\"product_id\",\"seller_id\"]\n",
    "        o = o.drop(columns=[c for c in drop_ids if c in o.columns], errors=\"ignore\")\n",
    "        o = o.drop(columns=[c for c in date_cols if c in o.columns], errors=\"ignore\")\n",
    "\n",
    "        num_cols = o.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        num_cols_no_target = [c for c in num_cols if c != \"target_olist\"]\n",
    "        for c in num_cols_no_target:\n",
    "            o[c] = pd.to_numeric(o[c], errors=\"coerce\").fillna(pd.to_numeric(o[c], errors=\"coerce\").median())\n",
    "        obj_cols = o.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        for c in obj_cols:\n",
    "            o[c] = o[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        return o.copy()\n",
    "    \n",
    "    def prepare_sales_df(sales_in):\n",
    "        print(\"prep: sales\")\n",
    "        s = sales_in.copy()\n",
    "\n",
    "        # target\n",
    "        if \"Critic_Score\" in s.columns:\n",
    "            cs = pd.to_numeric(s[\"Critic_Score\"], errors=\"coerce\")\n",
    "            y = (cs > 8.0).astype(\"Int64\") if is_classification else cs.astype(\"Float64\")\n",
    "        else:\n",
    "            y = pd.Series(np.nan, index=s.index, dtype=\"Float64\")\n",
    "\n",
    "        s[\"target_sales\"] = y\n",
    "        s = s[s[\"target_sales\"].notna()].copy()\n",
    "        s[\"target_sales\"] = s[\"target_sales\"].astype(\"int8\" if is_classification else \"float32\")\n",
    "\n",
    "        # remove obvious leaks and junk\n",
    "        leak_cols = [\"NA_Sales\",\"PAL_Sales\",\"JP_Sales\",\"Other_Sales\",\"Total_Shipped\",\"Rank\",\"Global_Sales\"]\n",
    "        junk_cols = [\"VGChartz_Score\",\"Vgchartzscore\",\"url\",\"img_url\",\"status\",\"Last_Update\",\"basename\",\"User_Score\"]\n",
    "        s = s.drop(columns=[c for c in leak_cols + junk_cols if c in s.columns], errors=\"ignore\")\n",
    "        if \"Critic_Score\" in s.columns:\n",
    "            s = s.drop(columns=[\"Critic_Score\"], errors=\"ignore\")\n",
    "\n",
    "        # simple remaster flag, then drop Name\n",
    "        if \"Name\" in s.columns:\n",
    "            terms = [\"remaster\",\"remastered\",\"hd\",\"definitive\",\"collection\",\"trilogy\",\"anniversary\"]\n",
    "            s[\"is_remaster\"] = s[\"Name\"].astype(\"string\").str.lower().str.contains(\"|\".join(terms), na=False).astype(\"Int64\")\n",
    "            s = s.drop(columns=[\"Name\"])\n",
    "        else:\n",
    "            s[\"is_remaster\"] = pd.Series(0, index=s.index, dtype=\"Int64\")\n",
    "\n",
    "        # platform family + handheld flag\n",
    "        def platform_family(p):\n",
    "            p = \"\" if pd.isna(p) else str(p).upper()\n",
    "            if p.startswith(\"PS\") or p in {\"PSP\",\"PSV\"}: return \"PlayStation\"\n",
    "            if p.startswith(\"X\") or p in {\"XB\",\"XBLA\"}: return \"Xbox\"\n",
    "            if p in {\"SWITCH\",\"WII\",\"WIIU\",\"GC\",\"N64\",\"SNES\",\"NES\",\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\"}: return \"Nintendo\"\n",
    "            if p == \"PC\": return \"PC\"\n",
    "            if p in {\"DC\",\"DREAMCAST\",\"SAT\",\"GEN\",\"MD\",\"MEGADRIVE\",\"GG\"}: return \"Sega\"\n",
    "            if \"ATARI\" in p: return \"Atari\"\n",
    "            return \"Other\"\n",
    "\n",
    "        if \"Platform\" in s.columns:\n",
    "            s[\"Platform_Family\"] = s[\"Platform\"].apply(platform_family)\n",
    "            handhelds = {\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\",\"PSP\",\"PSV\"}\n",
    "            s[\"is_portable\"] = s[\"Platform\"].astype(\"string\").str.upper().isin(handhelds).astype(\"Int64\")\n",
    "            s = s.drop(columns=[\"Platform\"])\n",
    "        else:\n",
    "            s[\"Platform_Family\"] = \"Other\"\n",
    "            s[\"is_portable\"] = pd.Series(0, index=s.index, dtype=\"Int64\")\n",
    "\n",
    "        # year / decade\n",
    "        if \"Year\" in s.columns:\n",
    "            s[\"Year\"] = pd.to_numeric(s[\"Year\"], errors=\"coerce\")\n",
    "            s.loc[~s[\"Year\"].between(1970, 2025, inclusive=\"both\"), \"Year\"] = np.nan\n",
    "            s[\"Decade\"] = (s[\"Year\"] // 10 * 10).astype(\"Int64\").astype(str)\n",
    "        else:\n",
    "            s[\"Year\"] = np.nan\n",
    "            s[\"Decade\"] = \"<NA>\"\n",
    "\n",
    "        # frequency encodes then drop raw strings\n",
    "        for col in [\"Publisher\",\"Developer\"]:\n",
    "            if col in s.columns:\n",
    "                series = s[col].astype(\"string\").fillna(\"Unknown\")\n",
    "                freq = series.map(series.value_counts(normalize=True))\n",
    "                s[col + \"_freq\"] = freq.astype(float)\n",
    "                s = s.drop(columns=[col])\n",
    "\n",
    "        # one-hot encode once, after building the list\n",
    "        cat_cols = [c for c in [\"Genre\",\"ESRB_Rating\",\"Platform_Family\",\"Decade\"] if c in s.columns]\n",
    "        for c in cat_cols:\n",
    "            s[c] = s[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        if len(cat_cols) > 0:\n",
    "            s = pd.get_dummies(s, columns=cat_cols, dtype=np.uint8)\n",
    "\n",
    "        # numeric cleanup\n",
    "        num_cols = s.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        num_cols_no_target = [c for c in num_cols if c != \"target_sales\"]\n",
    "        for c in num_cols_no_target:\n",
    "            s[c] = pd.to_numeric(s[c], errors=\"coerce\").fillna(pd.to_numeric(s[c], errors=\"coerce\").median())\n",
    "\n",
    "        # object cleanup\n",
    "        obj_cols = s.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        for c in obj_cols:\n",
    "            s[c] = s[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        return s.copy()\n",
    "\n",
    "\n",
    "    steam_df, steam_raw = prepare_steam_df(steam)\n",
    "    olist_df = prepare_olist_df(olist)\n",
    "    sales_df = prepare_sales_df(sales)\n",
    "\n",
    "    raw_for_split = steam_raw.loc[steam_df.index].copy()\n",
    "    raw_for_split[\"date\"] = pd.to_datetime(raw_for_split[\"date\"].astype(\"string\"), errors=\"coerce\")\n",
    "    cutoff_q = 1.0 - float(test_size)\n",
    "    cutoff_date = raw_for_split[\"date\"].sort_values().quantile(cutoff_q)\n",
    "\n",
    "    grp_first = raw_for_split.groupby(\"app_id\")[\"date\"].min()\n",
    "    app_train = set(grp_first[grp_first <= cutoff_date].index.tolist())\n",
    "    app_test = set(grp_first[grp_first > cutoff_date].index.tolist())\n",
    "\n",
    "    is_train_mask = raw_for_split[\"app_id\"].isin(app_train)\n",
    "    is_test_mask = raw_for_split[\"app_id\"].isin(app_test)\n",
    "    is_train_mask = is_train_mask | (~(is_train_mask | is_test_mask))\n",
    "\n",
    "    Xs = steam_df.drop(columns=[\"target_steam\"])\n",
    "    ys = steam_df[\"target_steam\"]\n",
    "\n",
    "    X_train_steam = Xs[is_train_mask].reset_index(drop=True)\n",
    "    X_test_steam = Xs[is_test_mask].reset_index(drop=True)\n",
    "    y_train_steam = ys[is_train_mask].reset_index(drop=True)\n",
    "    y_test_steam = ys[is_test_mask].reset_index(drop=True)\n",
    "\n",
    "    print(\"steam cutoff_date:\", pd.to_datetime(cutoff_date))\n",
    "    print(\"steam apps in test:\", len(app_test), \"of\", len(grp_first))\n",
    "    print(\"steam train rows:\", int(X_train_steam.shape[0]), \"| test rows:\", int(X_test_steam.shape[0]))\n",
    "    if is_classification:\n",
    "        print(\"steam y_train counts:\", y_train_steam.value_counts().to_dict())\n",
    "        print(\"steam y_test counts:\", y_test_steam.value_counts().to_dict())\n",
    "    else:\n",
    "        def sstats(s):\n",
    "            return {\"n\": int(s.shape[0]),\n",
    "                    \"mean\": float(np.nanmean(s)),\n",
    "                    \"std\": float(np.nanstd(s)),\n",
    "                    \"min\": float(np.nanmin(s)),\n",
    "                    \"max\": float(np.nanmax(s))}\n",
    "        print(\"steam y_train stats:\", sstats(y_train_steam))\n",
    "        print(\"steam y_test stats:\", sstats(y_test_steam))\n",
    "\n",
    "    if is_classification:\n",
    "        if y_train_steam.nunique() == 2:\n",
    "            vc = y_train_steam.value_counts()\n",
    "            ratio = float(vc.min()) / float(vc.max()) if vc.max() > 0 else 1.0\n",
    "            if balance_method != \"none\" and ratio < min_class_ratio:\n",
    "                method = \"oversample\" if (balance_method == \"auto\" and len(X_train_steam) <= 200000) else \\\n",
    "                         (\"undersample\" if balance_method == \"auto\" else balance_method)\n",
    "                X_train_steam, y_train_steam = resample_binary(X_train_steam, y_train_steam, method=method, random_state=random_state)\n",
    "                print(\"steam balanced train counts:\", y_train_steam.value_counts().to_dict())\n",
    "\n",
    "    steam_split = (X_train_steam, X_test_steam, y_train_steam, y_test_steam)\n",
    "\n",
    "    olist_split = split_and_balance(olist_df, \"target_olist\", balance_method, min_class_ratio, random_state)\n",
    "    sales_split = split_and_balance(sales_df, \"target_sales\", balance_method, min_class_ratio, random_state)\n",
    "\n",
    "    def scale_poly_wrapper(split):\n",
    "        Xtr, Xte, ytr, yte = split\n",
    "        Xtr2, Xte2 = apply_scaling_and_poly(Xtr, Xte)\n",
    "        return (Xtr2, Xte2, ytr, yte)\n",
    "\n",
    "    steam_split = scale_poly_wrapper(steam_split)\n",
    "    olist_split = scale_poly_wrapper(olist_split)\n",
    "    sales_split = scale_poly_wrapper(sales_split)\n",
    "\n",
    "    if feature_select_method != \"none\":\n",
    "        Xtr, Xte, ytr, yte = steam_split\n",
    "        Xtr_s, Xte_s = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"steam\"\n",
    "        )\n",
    "        steam_split = (Xtr_s, Xte_s, ytr, yte)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = olist_split\n",
    "        Xtr_o, Xte_o = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"olist\"\n",
    "        )\n",
    "        olist_split = (Xtr_o, Xte_o, ytr, yte)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = sales_split\n",
    "        Xtr_v, Xte_v = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"sales\"\n",
    "        )\n",
    "        sales_split = (Xtr_v, Xte_v, ytr, yte)\n",
    "\n",
    "    # show exactly which saved features are missing (after selection)\n",
    "    _ = debug_forward_missing(steam_split[0], \"steam\", best_forward_features)\n",
    "    _ = debug_forward_missing(olist_split[0], \"olist\", best_forward_features)\n",
    "    _ = debug_forward_missing(sales_split[0], \"sales\", best_forward_features)\n",
    "\n",
    "    print(\"\\nsteam selected features (train, test):\", steam_split[0].shape[1], steam_split[1].shape[1])\n",
    "    print(\"olist selected features (train, test):\", olist_split[0].shape[1], olist_split[1].shape[1])\n",
    "    print(\"sales selected features (train, test):\", sales_split[0].shape[1], sales_split[1].shape[1])\n",
    "\n",
    "    return {\n",
    "        \"steam\": steam_split,\n",
    "        \"olist\": olist_split,\n",
    "        \"sales\": sales_split,\n",
    "        \"feature_names\": {\n",
    "            \"steam\": steam_split[0].columns.tolist(),\n",
    "            \"olist\": olist_split[0].columns.tolist(),\n",
    "            \"sales\": sales_split[0].columns.tolist(),\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18a3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: start downloads\n",
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.731 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.174 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.206 sec\n",
      "main: downloads finished\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 10000 of 41154794 rows in 5.922 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(10000, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: picked 10000 of 99441 rows in 0.001 sec\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (11444, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: picked 10000 of 55792 rows in 0.021 sec\n",
      "vg2019: done shape=(10000, 16)\n",
      "main: load all done in 16.925 sec (00:00:16)\n",
      "download: shapes summary\n",
      "download: steam shape = (10000, 24)\n",
      "download: olist shape = (11444, 38)\n",
      "download: sales shape = (10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Download Paths\n",
    "# =============================\n",
    "print(\"main: start downloads\")\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "print(\"main: downloads finished\")\n",
    "\n",
    "# =============================\n",
    "# Load All\n",
    "# =============================\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# =============================\n",
    "# Download Shapes\n",
    "# =============================\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d126424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep: steam\n",
      "prep: steam build sparse tag matrix\n",
      "prep: olist\n",
      "prep: sales\n",
      "steam cutoff_date: 2022-02-12 00:00:00\n",
      "steam apps in test: 426 of 2879\n",
      "steam train rows: 9282 | test rows: 718\n",
      "steam y_train counts: {1: 7647, 0: 1635}\n",
      "steam y_test counts: {1: 498, 0: 220}\n",
      "target_olist train class counts: {1: 6217, 0: 2938}\n",
      "target_olist test class counts: {1: 1554, 0: 735}\n",
      "target_sales train class counts: {0: 7722, 1: 278}\n",
      "target_sales test class counts: {0: 1930, 1: 70}\n",
      "polynomial features added: 90 (from 12 columns)\n",
      "polynomial features added: 350 (from 25 columns)\n",
      "polynomial features added: 9 (from 3 columns)\n",
      "[debug] olist missing: ['product_name_lenght']\n",
      "\n",
      "steam selected features (train, test): 314 314\n",
      "olist selected features (train, test): 483 483\n",
      "sales selected features (train, test): 53 53\n",
      "Steam Best threshold: 0.6\n",
      "Steam CV F1 macro: 0.8287592697673509\n",
      "Olist Best threshold: 0.49\n",
      "Olist CV F1 macro: 0.712107175457085\n",
      "Sales Best threshold: 0.24000000000000002\n",
      "Sales CV F1 macro: 0.640298372851678\n",
      "prep: steam\n",
      "prep: steam build sparse tag matrix\n",
      "prep: olist\n",
      "prep: sales\n",
      "steam cutoff_date: 2022-02-12 00:00:00\n",
      "steam apps in test: 426 of 2879\n",
      "steam train rows: 9282 | test rows: 718\n",
      "steam y_train stats: {'n': 9282, 'mean': 86.45798319327731, 'std': 10.755786030983437, 'min': 13.0, 'max': 100.0}\n",
      "steam y_test stats: {'n': 718, 'mean': 82.983286908078, 'std': 14.709148586810915, 'min': 16.0, 'max': 100.0}\n",
      "target_olist train stats: {'n': 9052, 'mean': 3.9933888912200928, 'std': 1.185034990310669, 'min': 1.0, 'max': 5.0}\n",
      "target_olist test stats: {'n': 2264, 'mean': 4.002050399780273, 'std': 1.187378168106079, 'min': 1.0, 'max': 5.0}\n",
      "target_sales train stats: {'n': 905, 'mean': 7.161657333374023, 'std': 1.4508978128433228, 'min': 2.0, 'max': 10.0}\n",
      "target_sales test stats: {'n': 227, 'mean': 7.184581756591797, 'std': 1.5064852237701416, 'min': 2.4000000953674316, 'max': 9.800000190734863}\n",
      "polynomial features added: 90 (from 12 columns)\n",
      "polynomial features added: 350 (from 25 columns)\n",
      "polynomial features added: 9 (from 3 columns)\n",
      "[debug] olist missing: ['product_name_lenght']\n",
      "[debug] sales missing: ['Decade_1970', 'Decade_1980', 'Decade_<NA>', 'Genre_Board Game']\n",
      "\n",
      "steam selected features (train, test): 314 314\n",
      "olist selected features (train, test): 483 483\n",
      "sales selected features (train, test): 43 43\n",
      "Steam CV MAE: 4.2615375725582085\n",
      "Olist CV MAE: 0.7399954428327625\n",
      "Sales CV MAE: 1.080623426295783\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Classification (F1 macro)\n",
    "# ===============================\n",
    "\n",
    "# make classification splits\n",
    "splits_cls = prepare_all(steam, olist, sales, task_type=\"classification\", test_size=0.2)\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits_cls[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits_cls[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits_cls[\"sales\"]\n",
    "\n",
    "# --- Steam ---\n",
    "k_features_steam = min(300, X_train_steam.shape[1])\n",
    "clf_pipe_steam = build_classification_pipeline(\n",
    "    k_features=k_features_steam,\n",
    "    random_state=42\n",
    ")\n",
    "model_steam, thresh_steam, f1_steam = tune_threshold_oof(\n",
    "    clf_pipe_steam,\n",
    "    X_train_steam,\n",
    "    y_train_steam,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Steam Best threshold:\", thresh_steam)\n",
    "print(\"Steam CV F1 macro:\", f1_steam)\n",
    "\n",
    "# --- Olist ---\n",
    "k_features_olist = min(300, X_train_olist.shape[1])\n",
    "clf_pipe_olist = build_classification_pipeline(\n",
    "    k_features=k_features_olist,\n",
    "    random_state=42\n",
    ")\n",
    "model_olist, thresh_olist, f1_olist = tune_threshold_oof(\n",
    "    clf_pipe_olist,\n",
    "    X_train_olist,\n",
    "    y_train_olist,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Olist Best threshold:\", thresh_olist)\n",
    "print(\"Olist CV F1 macro:\", f1_olist)\n",
    "\n",
    "# --- Sales (very imbalanced: optional calibration helps) ---\n",
    "k_features_sales = min(200, X_train_sales.shape[1])\n",
    "\n",
    "clf_pipe_sales = build_classification_pipeline(\n",
    "    k_features=k_features_sales,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# optional calibration\n",
    "try:\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    clf_pipe_sales = CalibratedClassifierCV(\n",
    "        base_estimator=clf_pipe_sales,\n",
    "        cv=3,\n",
    "        method=\"sigmoid\"\n",
    "    )\n",
    "except Exception as _:\n",
    "    pass  # if calibration not available, continue with base pipeline\n",
    "\n",
    "model_sales, thresh_sales, f1_sales = tune_threshold_oof(\n",
    "    clf_pipe_sales,\n",
    "    X_train_sales,\n",
    "    y_train_sales,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Sales Best threshold:\", thresh_sales)\n",
    "print(\"Sales CV F1 macro:\", f1_sales)\n",
    "\n",
    "# ===============================\n",
    "# Regression (MAE)\n",
    "# ===============================\n",
    "\n",
    "# make regression splits (do not reuse classification y)\n",
    "splits_reg = prepare_all(steam, olist, sales, task_type=\"regression\", test_size=0.2)\n",
    "\n",
    "Xr_train_steam, Xr_test_steam, yr_train_steam, yr_test_steam = splits_reg[\"steam\"]\n",
    "Xr_train_olist, Xr_test_olist, yr_train_olist, yr_test_olist = splits_reg[\"olist\"]\n",
    "Xr_train_sales, Xr_test_sales, yr_train_sales, yr_test_sales = splits_reg[\"sales\"]\n",
    "\n",
    "# this line avoids a future warning in some pandas versions\n",
    "import pandas as pd\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "\n",
    "# --- Steam ---\n",
    "Xr_clean_steam = clip_outliers_dataframe(\n",
    "    Xr_train_steam,\n",
    "    lower_q=0.01,\n",
    "    upper_q=0.99\n",
    ")\n",
    "k_features_reg_steam = min(300, Xr_clean_steam.shape[1])\n",
    "reg_pipe_steam = build_regression_pipeline(\n",
    "    k_features=k_features_reg_steam,\n",
    "    random_state=42\n",
    ")\n",
    "cv_mae_steam = cv_mae(\n",
    "    reg_pipe_steam,\n",
    "    Xr_clean_steam,\n",
    "    yr_train_steam,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Steam CV MAE:\", cv_mae_steam)\n",
    "\n",
    "# --- Olist ---\n",
    "Xr_clean_olist = clip_outliers_dataframe(\n",
    "    Xr_train_olist,\n",
    "    lower_q=0.01,\n",
    "    upper_q=0.99\n",
    ")\n",
    "k_features_reg_olist = min(300, Xr_clean_olist.shape[1])\n",
    "reg_pipe_olist = build_regression_pipeline(\n",
    "    k_features=k_features_reg_olist,\n",
    "    random_state=42\n",
    ")\n",
    "cv_mae_olist = cv_mae(\n",
    "    reg_pipe_olist,\n",
    "    Xr_clean_olist,\n",
    "    yr_train_olist,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Olist CV MAE:\", cv_mae_olist)\n",
    "\n",
    "# --- Sales ---\n",
    "Xr_clean_sales = clip_outliers_dataframe(\n",
    "    Xr_train_sales,\n",
    "    lower_q=0.01,\n",
    "    upper_q=0.99\n",
    ")\n",
    "k_features_reg_sales = min(300, Xr_clean_sales.shape[1])\n",
    "reg_pipe_sales = build_regression_pipeline(\n",
    "    k_features=k_features_reg_sales,\n",
    "    random_state=42\n",
    ")\n",
    "cv_mae_sales = cv_mae(\n",
    "    reg_pipe_sales,\n",
    "    Xr_clean_sales,\n",
    "    yr_train_sales,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Sales CV MAE:\", cv_mae_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3177e9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e30164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep: steam\n",
      "prep: steam build sparse tag matrix\n",
      "prep: olist\n",
      "prep: sales\n",
      "steam cutoff_date: 2022-02-12 00:00:00\n",
      "steam apps in test: 426 of 2879\n",
      "steam train rows: 9282 | test rows: 718\n",
      "steam y_train counts: {1: 7647, 0: 1635}\n",
      "steam y_test counts: {1: 498, 0: 220}\n",
      "target_olist train class counts: {1: 6217, 0: 2938}\n",
      "target_olist test class counts: {1: 1554, 0: 735}\n",
      "target_sales train class counts: {0: 7722, 1: 278}\n",
      "target_sales test class counts: {0: 1930, 1: 70}\n",
      "polynomial features added: 90 (from 12 columns)\n",
      "polynomial features added: 350 (from 25 columns)\n",
      "polynomial features added: 9 (from 3 columns)\n",
      "[debug] olist missing: ['product_name_lenght']\n",
      "\n",
      "steam selected features (train, test): 314 314\n",
      "olist selected features (train, test): 483 483\n",
      "sales selected features (train, test): 53 53\n",
      "\n",
      "=== STEAM Dataset ===\n",
      "Removed 19 constant feature(s).\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/40] GBT | k=30 | F1_macro=0.727690 ± 0.012191\n",
      "[2/40] GBT | k=74 | F1_macro=0.727690 ± 0.012191\n",
      "[3/40] GBT | k=148 | F1_macro=0.727690 ± 0.012191\n",
      "[4/40] GBT | k=222 | F1_macro=0.727690 ± 0.012191\n",
      "[5/40] GBT | k=295 | F1_macro=0.727690 ± 0.012191\n",
      "[6/40] RandomForest | k=30 | F1_macro=0.791644 ± 0.009054\n",
      "[7/40] RandomForest | k=74 | F1_macro=0.791644 ± 0.009054\n",
      "[8/40] RandomForest | k=148 | F1_macro=0.791644 ± 0.009054\n",
      "[9/40] RandomForest | k=222 | F1_macro=0.791644 ± 0.009054\n",
      "[10/40] RandomForest | k=295 | F1_macro=0.791644 ± 0.009054\n",
      "[11/40] DecisionTree | k=30 | F1_macro=0.744387 ± 0.010381\n",
      "[12/40] DecisionTree | k=74 | F1_macro=0.744387 ± 0.010381\n",
      "[13/40] DecisionTree | k=148 | F1_macro=0.744387 ± 0.010381\n",
      "[14/40] DecisionTree | k=222 | F1_macro=0.744387 ± 0.010381\n",
      "[15/40] DecisionTree | k=295 | F1_macro=0.744387 ± 0.010381\n",
      "[16/40] LogisticRegression | k=30 | F1_macro=0.610785 ± 0.011664\n",
      "[17/40] LogisticRegression | k=74 | F1_macro=0.645663 ± 0.008790\n",
      "[18/40] LogisticRegression | k=148 | F1_macro=0.673175 ± 0.008672\n",
      "[19/40] LogisticRegression | k=222 | F1_macro=0.690537 ± 0.012682\n",
      "[20/40] LogisticRegression | k=295 | F1_macro=0.697011 ± 0.008063\n",
      "[21/40] LinearSVM | k=30 | F1_macro=0.612555 ± 0.010980\n",
      "[22/40] LinearSVM | k=74 | F1_macro=0.643486 ± 0.007832\n",
      "[23/40] LinearSVM | k=148 | F1_macro=0.668201 ± 0.009770\n",
      "[24/40] LinearSVM | k=222 | F1_macro=0.690896 ± 0.013197\n",
      "[25/40] LinearSVM | k=295 | F1_macro=0.698685 ± 0.012560\n",
      "[26/40] NaiveBayes | k=30 | F1_macro=0.614976 ± 0.006271\n",
      "[27/40] NaiveBayes | k=74 | F1_macro=0.625082 ± 0.021855\n",
      "[28/40] NaiveBayes | k=148 | F1_macro=0.506902 ± 0.079725\n",
      "[29/40] NaiveBayes | k=222 | F1_macro=0.442965 ± 0.022426\n",
      "[30/40] NaiveBayes | k=295 | F1_macro=0.413950 ± 0.007720\n",
      "[31/40] KNN | k=30 | F1_macro=0.698189 ± 0.018809\n",
      "[32/40] KNN | k=74 | F1_macro=0.700277 ± 0.013162\n",
      "[33/40] KNN | k=148 | F1_macro=0.699933 ± 0.003682\n",
      "[34/40] KNN | k=222 | F1_macro=0.680993 ± 0.008381\n",
      "[35/40] KNN | k=295 | F1_macro=0.669753 ± 0.009914\n",
      "[36/40] Dummy | k=30 | F1_macro=0.149766 ± 0.000000\n",
      "[37/40] Dummy | k=74 | F1_macro=0.149766 ± 0.000000\n",
      "[38/40] Dummy | k=148 | F1_macro=0.149766 ± 0.000000\n",
      "[39/40] Dummy | k=222 | F1_macro=0.149766 ± 0.000000\n",
      "[40/40] Dummy | k=295 | F1_macro=0.149766 ± 0.000000\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "                 Model  K_features  MeanScore   StdDev    Metric\n",
      "0         RandomForest          30   0.791644 0.009054  F1_macro\n",
      "1         RandomForest          74   0.791644 0.009054  F1_macro\n",
      "2         RandomForest         148   0.791644 0.009054  F1_macro\n",
      "3         RandomForest         222   0.791644 0.009054  F1_macro\n",
      "4         RandomForest         295   0.791644 0.009054  F1_macro\n",
      "5         DecisionTree          30   0.744387 0.010381  F1_macro\n",
      "6         DecisionTree          74   0.744387 0.010381  F1_macro\n",
      "7         DecisionTree         148   0.744387 0.010381  F1_macro\n",
      "8         DecisionTree         222   0.744387 0.010381  F1_macro\n",
      "9         DecisionTree         295   0.744387 0.010381  F1_macro\n",
      "10                 GBT          30   0.727690 0.012191  F1_macro\n",
      "11                 GBT          74   0.727690 0.012191  F1_macro\n",
      "12                 GBT         148   0.727690 0.012191  F1_macro\n",
      "13                 GBT         222   0.727690 0.012191  F1_macro\n",
      "14                 GBT         295   0.727690 0.012191  F1_macro\n",
      "15                 KNN          74   0.700277 0.013162  F1_macro\n",
      "16                 KNN         148   0.699933 0.003682  F1_macro\n",
      "17           LinearSVM         295   0.698685 0.012560  F1_macro\n",
      "18                 KNN          30   0.698189 0.018809  F1_macro\n",
      "19  LogisticRegression         295   0.697011 0.008063  F1_macro\n",
      "20           LinearSVM         222   0.690896 0.013197  F1_macro\n",
      "21  LogisticRegression         222   0.690537 0.012682  F1_macro\n",
      "22                 KNN         222   0.680993 0.008381  F1_macro\n",
      "23  LogisticRegression         148   0.673175 0.008672  F1_macro\n",
      "24                 KNN         295   0.669753 0.009914  F1_macro\n",
      "25           LinearSVM         148   0.668201 0.009770  F1_macro\n",
      "26  LogisticRegression          74   0.645663 0.008790  F1_macro\n",
      "27           LinearSVM          74   0.643486 0.007832  F1_macro\n",
      "28          NaiveBayes          74   0.625082 0.021855  F1_macro\n",
      "29          NaiveBayes          30   0.614976 0.006271  F1_macro\n",
      "30           LinearSVM          30   0.612555 0.010980  F1_macro\n",
      "31  LogisticRegression          30   0.610785 0.011664  F1_macro\n",
      "32          NaiveBayes         148   0.506902 0.079725  F1_macro\n",
      "33          NaiveBayes         222   0.442965 0.022426  F1_macro\n",
      "34          NaiveBayes         295   0.413950 0.007720  F1_macro\n",
      "35               Dummy          30   0.149766 0.000000  F1_macro\n",
      "36               Dummy          74   0.149766 0.000000  F1_macro\n",
      "37               Dummy         148   0.149766 0.000000  F1_macro\n",
      "38               Dummy         222   0.149766 0.000000  F1_macro\n",
      "39               Dummy         295   0.149766 0.000000  F1_macro\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   5.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   5.5s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   5.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   5.9s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   6.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   6.7s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=  13.3s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=  13.6s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=  13.6s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  13.3s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  13.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   7.7s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=  13.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   7.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   7.2s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=  13.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\u001b[33m\"\u001b[39m\u001b[33msales\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== STEAM Dataset ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m best_steam_model = \u001b[43mbuild_and_tune_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_steam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_steam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclassification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43moversample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43moversample_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msmote\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m score_steam = evaluate_on_holdout(best_steam_model, X_test_steam, y_test_steam, task_type=\u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msteam threshold:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(best_steam_model, \u001b[33m\"\u001b[39m\u001b[33mbest_threshold_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 322\u001b[39m, in \u001b[36mbuild_and_tune_models\u001b[39m\u001b[34m(X_train, y_train, task_type, num_folds, num_iterations, oversample, oversample_method)\u001b[39m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_pipeline\n\u001b[32m    312\u001b[39m search = RandomizedSearchCV(\n\u001b[32m    313\u001b[39m     estimator=best_pipeline,\n\u001b[32m    314\u001b[39m     param_distributions=search_space,\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m    321\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task == \u001b[33m\"\u001b[39m\u001b[33mregression\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    325\u001b[39m     tuned_score_display = -\u001b[38;5;28mfloat\u001b[39m(search.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/default/lib/python3.13/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Classification call\n",
    "splits = prepare_all(steam, olist, sales, task_type=\"classification\", test_size=0.2)\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "print(\"\\n=== STEAM Dataset ===\")\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True,\n",
    "    oversample_method=\"smote\"\n",
    ")\n",
    "\n",
    "score_steam = evaluate_on_holdout(best_steam_model, X_test_steam, y_test_steam, task_type=\"classification\")\n",
    "\n",
    "print(\"steam threshold:\", getattr(best_steam_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== OLIST Dataset ===\")\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True,\n",
    "    oversample_method=\"smote\"\n",
    ")\n",
    "\n",
    "score_olist = evaluate_on_holdout(best_olist_model, X_test_olist, y_test_olist, task_type=\"classification\")\n",
    "\n",
    "print(\"olist threshold:\", getattr(best_olist_model, \"best_threshold_\", None))\n",
    "\n",
    "print(\"\\n=== SALES Dataset ===\")\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"classification\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20,\n",
    "    oversample=True,\n",
    "    oversample_method=\"smote\"\n",
    ") \n",
    "\n",
    "score_sales = evaluate_on_holdout(best_sales_model, X_test_sales, y_test_sales, task_type=\"classification\")\n",
    "\n",
    "print(\"sales threshold:\", getattr(best_sales_model, \"best_threshold_\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec77e4c",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc10f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep: steam\n",
      "prep: steam build sparse tag matrix\n",
      "prep: olist\n",
      "prep: sales\n",
      "steam cutoff_date: 2022-02-12 00:00:00\n",
      "steam apps in test: 426 of 2879\n",
      "steam train rows: 9282 | test rows: 718\n",
      "steam y_train stats: {'n': 9282, 'mean': 86.45798319327731, 'std': 10.755786030983437, 'min': 13.0, 'max': 100.0}\n",
      "steam y_test stats: {'n': 718, 'mean': 82.983286908078, 'std': 14.709148586810915, 'min': 16.0, 'max': 100.0}\n",
      "target_olist train stats: {'n': 9052, 'mean': 3.9933888912200928, 'std': 1.185034990310669, 'min': 1.0, 'max': 5.0}\n",
      "target_olist test stats: {'n': 2264, 'mean': 4.002050399780273, 'std': 1.187378168106079, 'min': 1.0, 'max': 5.0}\n",
      "target_sales train stats: {'n': 905, 'mean': 7.161657333374023, 'std': 1.4508978128433228, 'min': 2.0, 'max': 10.0}\n",
      "target_sales test stats: {'n': 227, 'mean': 7.184581756591797, 'std': 1.5064852237701416, 'min': 2.4000000953674316, 'max': 9.800000190734863}\n",
      "polynomial features added: 90 (from 12 columns)\n",
      "polynomial features added: 350 (from 25 columns)\n",
      "polynomial features added: 9 (from 3 columns)\n",
      "[debug] olist missing: ['product_name_lenght']\n",
      "[debug] sales missing: ['Decade_1970', 'Decade_1980', 'Decade_<NA>', 'Genre_Board Game']\n",
      "\n",
      "steam selected features (train, test): 314 314\n",
      "olist selected features (train, test): 483 483\n",
      "sales selected features (train, test): 43 43\n",
      "Removed 19 constant feature(s).\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/45] GBT | k=30 | CV_MAE=5.747563 ± 0.041857\n",
      "[2/45] GBT | k=74 | CV_MAE=5.747563 ± 0.041857\n",
      "[3/45] GBT | k=148 | CV_MAE=5.747563 ± 0.041857\n",
      "[4/45] GBT | k=222 | CV_MAE=5.747563 ± 0.041857\n",
      "[5/45] GBT | k=295 | CV_MAE=5.747563 ± 0.041857\n",
      "[6/45] RandomForest | k=30 | CV_MAE=4.108884 ± 0.051547\n",
      "[7/45] RandomForest | k=74 | CV_MAE=4.108884 ± 0.051547\n",
      "[8/45] RandomForest | k=148 | CV_MAE=4.108884 ± 0.051547\n",
      "[9/45] RandomForest | k=222 | CV_MAE=4.108884 ± 0.051547\n",
      "[10/45] RandomForest | k=295 | CV_MAE=4.108884 ± 0.051547\n",
      "[11/45] DecisionTree | k=30 | CV_MAE=4.533075 ± 0.150487\n",
      "[12/45] DecisionTree | k=74 | CV_MAE=4.533075 ± 0.150487\n",
      "[13/45] DecisionTree | k=148 | CV_MAE=4.533075 ± 0.150487\n",
      "[14/45] DecisionTree | k=222 | CV_MAE=4.533075 ± 0.150487\n",
      "[15/45] DecisionTree | k=295 | CV_MAE=4.533075 ± 0.150487\n",
      "[16/45] LinearRegression | k=30 | CV_MAE=6.723085 ± 0.032914\n",
      "[17/45] LinearRegression | k=74 | CV_MAE=6.395448 ± 0.087795\n",
      "[18/45] LinearRegression | k=148 | CV_MAE=6.290538 ± 0.056786\n",
      "[19/45] LinearRegression | k=222 | CV_MAE=6.166372 ± 0.080818\n",
      "[20/45] LinearRegression | k=295 | CV_MAE=6.085232 ± 0.079866\n",
      "[21/45] Ridge | k=30 | CV_MAE=6.722912 ± 0.032930\n",
      "[22/45] Ridge | k=74 | CV_MAE=6.394582 ± 0.086792\n",
      "[23/45] Ridge | k=148 | CV_MAE=6.289254 ± 0.055204\n",
      "[24/45] Ridge | k=222 | CV_MAE=6.165423 ± 0.079669\n",
      "[25/45] Ridge | k=295 | CV_MAE=6.087706 ± 0.080089\n",
      "[26/45] Lasso | k=30 | CV_MAE=7.025209 ± 0.086531\n",
      "[27/45] Lasso | k=74 | CV_MAE=7.019777 ± 0.091653\n",
      "[28/45] Lasso | k=148 | CV_MAE=7.019777 ± 0.091653\n",
      "[29/45] Lasso | k=222 | CV_MAE=7.019777 ± 0.091653\n",
      "[30/45] Lasso | k=295 | CV_MAE=7.019777 ± 0.091653\n",
      "[31/45] ElasticNet | k=30 | CV_MAE=6.908964 ± 0.066342\n",
      "[32/45] ElasticNet | k=74 | CV_MAE=6.827483 ± 0.062124\n",
      "[33/45] ElasticNet | k=148 | CV_MAE=6.794003 ± 0.060576\n",
      "[34/45] ElasticNet | k=222 | CV_MAE=6.791965 ± 0.060656\n",
      "[35/45] ElasticNet | k=295 | CV_MAE=6.791965 ± 0.060656\n",
      "[36/45] KNN | k=30 | CV_MAE=5.664921 ± 0.084865\n",
      "[37/45] KNN | k=74 | CV_MAE=5.177591 ± 0.206504\n",
      "[38/45] KNN | k=148 | CV_MAE=5.230101 ± 0.069298\n",
      "[39/45] KNN | k=222 | CV_MAE=5.387503 ± 0.077415\n",
      "[40/45] KNN | k=295 | CV_MAE=5.548675 ± 0.090629\n",
      "[41/45] Dummy | k=30 | CV_MAE=7.807684 ± 0.098351\n",
      "[42/45] Dummy | k=74 | CV_MAE=7.807684 ± 0.098351\n",
      "[43/45] Dummy | k=148 | CV_MAE=7.807684 ± 0.098351\n",
      "[44/45] Dummy | k=222 | CV_MAE=7.807684 ± 0.098351\n",
      "[45/45] Dummy | k=295 | CV_MAE=7.807684 ± 0.098351\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "               Model  K_features  MeanScore   StdDev  Metric\n",
      "0       RandomForest          30   4.108884 0.051547  CV_MAE\n",
      "1       RandomForest          74   4.108884 0.051547  CV_MAE\n",
      "2       RandomForest         148   4.108884 0.051547  CV_MAE\n",
      "3       RandomForest         222   4.108884 0.051547  CV_MAE\n",
      "4       RandomForest         295   4.108884 0.051547  CV_MAE\n",
      "5       DecisionTree          30   4.533075 0.150487  CV_MAE\n",
      "6       DecisionTree          74   4.533075 0.150487  CV_MAE\n",
      "7       DecisionTree         148   4.533075 0.150487  CV_MAE\n",
      "8       DecisionTree         222   4.533075 0.150487  CV_MAE\n",
      "9       DecisionTree         295   4.533075 0.150487  CV_MAE\n",
      "10               KNN          74   5.177591 0.206504  CV_MAE\n",
      "11               KNN         148   5.230101 0.069298  CV_MAE\n",
      "12               KNN         222   5.387503 0.077415  CV_MAE\n",
      "13               KNN         295   5.548675 0.090629  CV_MAE\n",
      "14               KNN          30   5.664921 0.084865  CV_MAE\n",
      "15               GBT          30   5.747563 0.041857  CV_MAE\n",
      "16               GBT          74   5.747563 0.041857  CV_MAE\n",
      "17               GBT         148   5.747563 0.041857  CV_MAE\n",
      "18               GBT         222   5.747563 0.041857  CV_MAE\n",
      "19               GBT         295   5.747563 0.041857  CV_MAE\n",
      "20  LinearRegression         295   6.085232 0.079866  CV_MAE\n",
      "21             Ridge         295   6.087706 0.080089  CV_MAE\n",
      "22             Ridge         222   6.165423 0.079669  CV_MAE\n",
      "23  LinearRegression         222   6.166372 0.080818  CV_MAE\n",
      "24             Ridge         148   6.289254 0.055204  CV_MAE\n",
      "25  LinearRegression         148   6.290538 0.056786  CV_MAE\n",
      "26             Ridge          74   6.394582 0.086792  CV_MAE\n",
      "27  LinearRegression          74   6.395448 0.087795  CV_MAE\n",
      "28             Ridge          30   6.722912 0.032930  CV_MAE\n",
      "29  LinearRegression          30   6.723085 0.032914  CV_MAE\n",
      "30        ElasticNet         222   6.791965 0.060656  CV_MAE\n",
      "31        ElasticNet         295   6.791965 0.060656  CV_MAE\n",
      "32        ElasticNet         148   6.794003 0.060576  CV_MAE\n",
      "33        ElasticNet          74   6.827483 0.062124  CV_MAE\n",
      "34        ElasticNet          30   6.908964 0.066342  CV_MAE\n",
      "35             Lasso          74   7.019777 0.091653  CV_MAE\n",
      "36             Lasso         148   7.019777 0.091653  CV_MAE\n",
      "37             Lasso         222   7.019777 0.091653  CV_MAE\n",
      "38             Lasso         295   7.019777 0.091653  CV_MAE\n",
      "39             Lasso          30   7.025209 0.086531  CV_MAE\n",
      "40             Dummy          30   7.807684 0.098351  CV_MAE\n",
      "41             Dummy          74   7.807684 0.098351  CV_MAE\n",
      "42             Dummy         148   7.807684 0.098351  CV_MAE\n",
      "43             Dummy         222   7.807684 0.098351  CV_MAE\n",
      "44             Dummy         295   7.807684 0.098351  CV_MAE\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   2.4s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   2.6s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   2.8s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   3.0s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   3.0s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   3.0s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   6.1s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   6.4s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   7.2s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   7.2s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   7.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   4.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   6.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   4.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   4.2s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   8.3s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   8.1s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   2.6s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   2.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   7.6s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   2.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   6.4s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   6.5s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   6.9s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  40.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  40.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=  40.8s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  12.2s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  12.5s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   7.8s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=  12.2s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time= 1.3min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time= 1.3min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time= 1.3min\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   7.4s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time= 1.3min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time= 1.3min\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   6.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time= 1.4min\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   4.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   4.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   4.1s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  46.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time= 2.2min\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time= 2.3min\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  45.8s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time= 2.3min\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   3.5s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   3.4s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   3.9s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=  33.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time= 2.3min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time= 2.2min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time= 2.2min\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time= 1.8min\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time= 1.8min\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time= 1.8min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time= 1.7min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time= 1.8min\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time= 1.8min\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 30\n",
      "Best hyperparameters: {'model__n_estimators': 700, 'model__min_samples_split': 2, 'model__min_samples_leaf': 2, 'model__max_features': None, 'model__max_depth': 40}\n",
      "Best CV score (CV MAE): 4.122655\n",
      "Removed 2 constant feature(s).\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/45] GBT | k=49 | CV_MAE=0.811177 ± 0.016875\n",
      "[2/45] GBT | k=121 | CV_MAE=0.811177 ± 0.016875\n",
      "[3/45] GBT | k=241 | CV_MAE=0.811177 ± 0.016875\n",
      "[4/45] GBT | k=361 | CV_MAE=0.811177 ± 0.016875\n",
      "[5/45] GBT | k=481 | CV_MAE=0.811177 ± 0.016875\n",
      "[6/45] RandomForest | k=49 | CV_MAE=0.812595 ± 0.014902\n",
      "[7/45] RandomForest | k=121 | CV_MAE=0.812595 ± 0.014902\n",
      "[8/45] RandomForest | k=241 | CV_MAE=0.812595 ± 0.014902\n",
      "[9/45] RandomForest | k=361 | CV_MAE=0.812595 ± 0.014902\n",
      "[10/45] RandomForest | k=481 | CV_MAE=0.812595 ± 0.014902\n",
      "[11/45] DecisionTree | k=49 | CV_MAE=1.022945 ± 0.011242\n",
      "[12/45] DecisionTree | k=121 | CV_MAE=1.022945 ± 0.011242\n",
      "[13/45] DecisionTree | k=241 | CV_MAE=1.022945 ± 0.011242\n",
      "[14/45] DecisionTree | k=361 | CV_MAE=1.022945 ± 0.011242\n",
      "[15/45] DecisionTree | k=481 | CV_MAE=1.022945 ± 0.011242\n",
      "[16/45] LinearRegression | k=49 | CV_MAE=0.852161 ± 0.014310\n",
      "[17/45] LinearRegression | k=121 | CV_MAE=0.853472 ± 0.012806\n",
      "[18/45] LinearRegression | k=241 | CV_MAE=0.859580 ± 0.019848\n",
      "[19/45] LinearRegression | k=361 | CV_MAE=0.864283 ± 0.024382\n",
      "[20/45] LinearRegression | k=481 | CV_MAE=0.870754 ± 0.022055\n",
      "[21/45] Ridge | k=49 | CV_MAE=0.851749 ± 0.014468\n",
      "[22/45] Ridge | k=121 | CV_MAE=0.851334 ± 0.013084\n",
      "[23/45] Ridge | k=241 | CV_MAE=0.852488 ± 0.022540\n",
      "[24/45] Ridge | k=361 | CV_MAE=0.856792 ± 0.026213\n",
      "[25/45] Ridge | k=481 | CV_MAE=0.862623 ± 0.024560\n",
      "[26/45] Lasso | k=49 | CV_MAE=0.905201 ± 0.011217\n",
      "[27/45] Lasso | k=121 | CV_MAE=0.905201 ± 0.011217\n",
      "[28/45] Lasso | k=241 | CV_MAE=0.905201 ± 0.011217\n",
      "[29/45] Lasso | k=361 | CV_MAE=0.905201 ± 0.011217\n",
      "[30/45] Lasso | k=481 | CV_MAE=0.905201 ± 0.011217\n",
      "[31/45] ElasticNet | k=49 | CV_MAE=0.905201 ± 0.011217\n",
      "[32/45] ElasticNet | k=121 | CV_MAE=0.905201 ± 0.011217\n",
      "[33/45] ElasticNet | k=241 | CV_MAE=0.905201 ± 0.011217\n",
      "[34/45] ElasticNet | k=361 | CV_MAE=0.905201 ± 0.011217\n",
      "[35/45] ElasticNet | k=481 | CV_MAE=0.905201 ± 0.011217\n",
      "[36/45] KNN | k=49 | CV_MAE=0.883423 ± 0.016367\n",
      "[37/45] KNN | k=121 | CV_MAE=0.862451 ± 0.011766\n",
      "[38/45] KNN | k=241 | CV_MAE=0.854203 ± 0.011061\n",
      "[39/45] KNN | k=361 | CV_MAE=0.855736 ± 0.017437\n",
      "[40/45] KNN | k=481 | CV_MAE=0.849528 ± 0.018929\n",
      "[41/45] Dummy | k=49 | CV_MAE=0.905201 ± 0.011217\n",
      "[42/45] Dummy | k=121 | CV_MAE=0.905201 ± 0.011217\n",
      "[43/45] Dummy | k=241 | CV_MAE=0.905201 ± 0.011217\n",
      "[44/45] Dummy | k=361 | CV_MAE=0.905201 ± 0.011217\n",
      "[45/45] Dummy | k=481 | CV_MAE=0.905201 ± 0.011217\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "               Model  K_features  MeanScore   StdDev  Metric\n",
      "0                GBT          49   0.811177 0.016875  CV_MAE\n",
      "1                GBT         121   0.811177 0.016875  CV_MAE\n",
      "2                GBT         241   0.811177 0.016875  CV_MAE\n",
      "3                GBT         361   0.811177 0.016875  CV_MAE\n",
      "4                GBT         481   0.811177 0.016875  CV_MAE\n",
      "5       RandomForest          49   0.812595 0.014902  CV_MAE\n",
      "6       RandomForest         121   0.812595 0.014902  CV_MAE\n",
      "7       RandomForest         241   0.812595 0.014902  CV_MAE\n",
      "8       RandomForest         361   0.812595 0.014902  CV_MAE\n",
      "9       RandomForest         481   0.812595 0.014902  CV_MAE\n",
      "10               KNN         481   0.849528 0.018929  CV_MAE\n",
      "11             Ridge         121   0.851334 0.013084  CV_MAE\n",
      "12             Ridge          49   0.851749 0.014468  CV_MAE\n",
      "13  LinearRegression          49   0.852161 0.014310  CV_MAE\n",
      "14             Ridge         241   0.852488 0.022540  CV_MAE\n",
      "15  LinearRegression         121   0.853472 0.012806  CV_MAE\n",
      "16               KNN         241   0.854203 0.011061  CV_MAE\n",
      "17               KNN         361   0.855736 0.017437  CV_MAE\n",
      "18             Ridge         361   0.856792 0.026213  CV_MAE\n",
      "19  LinearRegression         241   0.859580 0.019848  CV_MAE\n",
      "20               KNN         121   0.862451 0.011766  CV_MAE\n",
      "21             Ridge         481   0.862623 0.024560  CV_MAE\n",
      "22  LinearRegression         361   0.864283 0.024382  CV_MAE\n",
      "23  LinearRegression         481   0.870754 0.022055  CV_MAE\n",
      "24               KNN          49   0.883423 0.016367  CV_MAE\n",
      "25        ElasticNet          49   0.905201 0.011217  CV_MAE\n",
      "26        ElasticNet         121   0.905201 0.011217  CV_MAE\n",
      "27        ElasticNet         241   0.905201 0.011217  CV_MAE\n",
      "28        ElasticNet         361   0.905201 0.011217  CV_MAE\n",
      "29        ElasticNet         481   0.905201 0.011217  CV_MAE\n",
      "30             Lasso          49   0.905201 0.011217  CV_MAE\n",
      "31             Lasso         121   0.905201 0.011217  CV_MAE\n",
      "32             Lasso         241   0.905201 0.011217  CV_MAE\n",
      "33             Lasso         361   0.905201 0.011217  CV_MAE\n",
      "34             Lasso         481   0.905201 0.011217  CV_MAE\n",
      "35             Dummy          49   0.905201 0.011217  CV_MAE\n",
      "36             Dummy         121   0.905201 0.011217  CV_MAE\n",
      "37             Dummy         241   0.905201 0.011217  CV_MAE\n",
      "38             Dummy         361   0.905201 0.011217  CV_MAE\n",
      "39             Dummy         481   0.905201 0.011217  CV_MAE\n",
      "40      DecisionTree          49   1.022945 0.011242  CV_MAE\n",
      "41      DecisionTree         121   1.022945 0.011242  CV_MAE\n",
      "42      DecisionTree         241   1.022945 0.011242  CV_MAE\n",
      "43      DecisionTree         361   1.022945 0.011242  CV_MAE\n",
      "44      DecisionTree         481   1.022945 0.011242  CV_MAE\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=4, model__n_estimators=100, model__subsample=0.9; total time=  53.3s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=4, model__n_estimators=100, model__subsample=0.9; total time=  53.4s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time= 1.3min\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time= 1.3min\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time= 1.3min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=2, model__n_estimators=300, model__subsample=0.9; total time= 1.4min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=2, model__n_estimators=300, model__subsample=0.9; total time= 1.4min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=2, model__n_estimators=300, model__subsample=0.9; total time= 1.4min\n",
      "[CV] END model__learning_rate=0.006579332246575682, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time=  31.5s\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=4, model__n_estimators=100, model__subsample=0.9; total time=  53.4s\n",
      "[CV] END model__learning_rate=0.006579332246575682, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time=  31.3s\n",
      "[CV] END model__learning_rate=0.006579332246575682, model__max_depth=3, model__n_estimators=100, model__subsample=0.7; total time=  32.0s\n",
      "[CV] END model__learning_rate=0.001873817422860383, model__max_depth=2, model__n_estimators=200, model__subsample=0.7; total time=  41.9s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=3, model__n_estimators=500, model__subsample=0.7; total time= 2.6min\n",
      "[CV] END model__learning_rate=0.001873817422860383, model__max_depth=2, model__n_estimators=200, model__subsample=0.7; total time=  42.7s\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=3, model__n_estimators=500, model__subsample=0.7; total time= 2.6min\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=3, model__n_estimators=500, model__subsample=0.7; total time= 2.6min\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=2, model__n_estimators=300, model__subsample=1.0; total time= 1.5min\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=2, model__n_estimators=300, model__subsample=1.0; total time= 1.5min\n",
      "[CV] END model__learning_rate=0.001873817422860383, model__max_depth=2, model__n_estimators=200, model__subsample=0.7; total time=  41.9s\n",
      "[CV] END model__learning_rate=0.012328467394420659, model__max_depth=2, model__n_estimators=300, model__subsample=1.0; total time= 1.5min\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.6; total time= 2.9min\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.6; total time= 2.9min\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.6; total time= 2.9min\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=2, model__n_estimators=500, model__subsample=0.7; total time= 1.7min\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=2, model__n_estimators=500, model__subsample=0.7; total time= 1.7min\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=2, model__n_estimators=500, model__subsample=0.7; total time= 1.7min\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time= 4.9min\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time= 4.9min\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=500, model__subsample=0.8; total time= 4.9min\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 2.0min\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 2.0min\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time=  36.0s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time=  36.3s\n",
      "[CV] END model__learning_rate=0.08111308307896868, model__max_depth=3, model__n_estimators=100, model__subsample=0.8; total time=  36.3s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=5, model__n_estimators=200, model__subsample=0.8; total time= 2.0min\n",
      "[CV] END model__learning_rate=0.04328761281083057, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time=  59.0s\n",
      "[CV] END model__learning_rate=0.04328761281083057, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time=  59.3s\n",
      "[CV] END model__learning_rate=0.04328761281083057, model__max_depth=5, model__n_estimators=100, model__subsample=0.8; total time=  59.1s\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time= 1.3min\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time= 1.3min\n",
      "[CV] END model__learning_rate=0.001, model__max_depth=3, model__n_estimators=300, model__subsample=0.6; total time= 1.4min\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=100, model__subsample=1.0; total time= 1.2min\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=100, model__subsample=1.0; total time= 1.3min\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=5, model__n_estimators=500, model__subsample=0.9; total time= 5.5min\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=5, model__n_estimators=500, model__subsample=0.9; total time= 5.5min\n",
      "[CV] END model__learning_rate=0.5336699231206307, model__max_depth=5, model__n_estimators=500, model__subsample=0.9; total time= 5.5min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=5, model__n_estimators=500, model__subsample=0.6; total time= 3.7min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=5, model__n_estimators=500, model__subsample=0.6; total time= 3.6min\n",
      "[CV] END model__learning_rate=0.02310129700083159, model__max_depth=5, model__n_estimators=100, model__subsample=1.0; total time= 1.2min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=5, model__n_estimators=500, model__subsample=0.6; total time= 3.6min\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.9; total time= 4.4min\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.9; total time= 4.4min\n",
      "[CV] END model__learning_rate=0.003511191734215131, model__max_depth=4, model__n_estimators=500, model__subsample=0.9; total time= 4.4min\n",
      "[CV] END model__learning_rate=0.2848035868435799, model__max_depth=5, model__n_estimators=200, model__subsample=0.6; total time= 1.3min\n",
      "[CV] END model__learning_rate=0.2848035868435799, model__max_depth=5, model__n_estimators=200, model__subsample=0.6; total time= 1.4min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=3, model__n_estimators=200, model__subsample=0.9; total time= 1.3min\n",
      "[CV] END model__learning_rate=0.2848035868435799, model__max_depth=5, model__n_estimators=200, model__subsample=0.6; total time= 1.3min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=3, model__n_estimators=200, model__subsample=0.9; total time= 1.2min\n",
      "[CV] END model__learning_rate=1.0, model__max_depth=3, model__n_estimators=200, model__subsample=0.9; total time= 1.2min\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: GBT\n",
      "Number of features: 49\n",
      "Best hyperparameters: {'model__subsample': np.float64(0.8), 'model__n_estimators': 500, 'model__max_depth': 5, 'model__learning_rate': np.float64(0.02310129700083159)}\n",
      "Best CV score (CV MAE): 0.777403\n",
      "Streaming results (each line is one model × feature count):\n",
      "[1/45] GBT | k=5 | CV_MAE=1.109209 ± 0.038882\n",
      "[2/45] GBT | k=11 | CV_MAE=1.109209 ± 0.038882\n",
      "[3/45] GBT | k=22 | CV_MAE=1.109209 ± 0.038882\n",
      "[4/45] GBT | k=33 | CV_MAE=1.109209 ± 0.038882\n",
      "[5/45] GBT | k=43 | CV_MAE=1.109209 ± 0.038882\n",
      "[6/45] RandomForest | k=5 | CV_MAE=1.068922 ± 0.015495\n",
      "[7/45] RandomForest | k=11 | CV_MAE=1.068922 ± 0.015495\n",
      "[8/45] RandomForest | k=22 | CV_MAE=1.068922 ± 0.015495\n",
      "[9/45] RandomForest | k=33 | CV_MAE=1.068922 ± 0.015495\n",
      "[10/45] RandomForest | k=43 | CV_MAE=1.068922 ± 0.015495\n",
      "[11/45] DecisionTree | k=5 | CV_MAE=1.335389 ± 0.019682\n",
      "[12/45] DecisionTree | k=11 | CV_MAE=1.335389 ± 0.019682\n",
      "[13/45] DecisionTree | k=22 | CV_MAE=1.335389 ± 0.019682\n",
      "[14/45] DecisionTree | k=33 | CV_MAE=1.335389 ± 0.019682\n",
      "[15/45] DecisionTree | k=43 | CV_MAE=1.335389 ± 0.019682\n",
      "[16/45] LinearRegression | k=5 | CV_MAE=1.142055 ± 0.029552\n",
      "[17/45] LinearRegression | k=11 | CV_MAE=1.129479 ± 0.014460\n",
      "[18/45] LinearRegression | k=22 | CV_MAE=1.131009 ± 0.018436\n",
      "[19/45] LinearRegression | k=33 | CV_MAE=1.127515 ± 0.016015\n",
      "[20/45] LinearRegression | k=43 | CV_MAE=1.125022 ± 0.013787\n",
      "[21/45] Ridge | k=5 | CV_MAE=1.136212 ± 0.026442\n",
      "[22/45] Ridge | k=11 | CV_MAE=1.126773 ± 0.013484\n",
      "[23/45] Ridge | k=22 | CV_MAE=1.142463 ± 0.015107\n",
      "[24/45] Ridge | k=33 | CV_MAE=1.143115 ± 0.015143\n",
      "[25/45] Ridge | k=43 | CV_MAE=1.147616 ± 0.021007\n",
      "[26/45] Lasso | k=5 | CV_MAE=1.152468 ± 0.038557\n",
      "[27/45] Lasso | k=11 | CV_MAE=1.152468 ± 0.038557\n",
      "[28/45] Lasso | k=22 | CV_MAE=1.152468 ± 0.038557\n",
      "[29/45] Lasso | k=33 | CV_MAE=1.152468 ± 0.038557\n",
      "[30/45] Lasso | k=43 | CV_MAE=1.152468 ± 0.038557\n",
      "[31/45] ElasticNet | k=5 | CV_MAE=1.152468 ± 0.038557\n",
      "[32/45] ElasticNet | k=11 | CV_MAE=1.152468 ± 0.038557\n",
      "[33/45] ElasticNet | k=22 | CV_MAE=1.152468 ± 0.038557\n",
      "[34/45] ElasticNet | k=33 | CV_MAE=1.152468 ± 0.038557\n",
      "[35/45] ElasticNet | k=43 | CV_MAE=1.152468 ± 0.038557\n",
      "[36/45] KNN | k=5 | CV_MAE=1.194559 ± 0.032393\n",
      "[37/45] KNN | k=11 | CV_MAE=1.167503 ± 0.025481\n",
      "[38/45] KNN | k=22 | CV_MAE=1.211442 ± 0.021625\n",
      "[39/45] KNN | k=33 | CV_MAE=1.204551 ± 0.031609\n",
      "[40/45] KNN | k=43 | CV_MAE=1.208321 ± 0.021137\n",
      "[41/45] Dummy | k=5 | CV_MAE=1.152468 ± 0.038557\n",
      "[42/45] Dummy | k=11 | CV_MAE=1.152468 ± 0.038557\n",
      "[43/45] Dummy | k=22 | CV_MAE=1.152468 ± 0.038557\n",
      "[44/45] Dummy | k=33 | CV_MAE=1.152468 ± 0.038557\n",
      "[45/45] Dummy | k=43 | CV_MAE=1.152468 ± 0.038557\n",
      "\n",
      "=== Baseline results (CV) ===\n",
      "               Model  K_features  MeanScore   StdDev  Metric\n",
      "0       RandomForest           5   1.068922 0.015495  CV_MAE\n",
      "1       RandomForest          11   1.068922 0.015495  CV_MAE\n",
      "2       RandomForest          22   1.068922 0.015495  CV_MAE\n",
      "3       RandomForest          33   1.068922 0.015495  CV_MAE\n",
      "4       RandomForest          43   1.068922 0.015495  CV_MAE\n",
      "5                GBT           5   1.109209 0.038882  CV_MAE\n",
      "6                GBT          11   1.109209 0.038882  CV_MAE\n",
      "7                GBT          22   1.109209 0.038882  CV_MAE\n",
      "8                GBT          33   1.109209 0.038882  CV_MAE\n",
      "9                GBT          43   1.109209 0.038882  CV_MAE\n",
      "10  LinearRegression          43   1.125022 0.013787  CV_MAE\n",
      "11             Ridge          11   1.126773 0.013484  CV_MAE\n",
      "12  LinearRegression          33   1.127515 0.016015  CV_MAE\n",
      "13  LinearRegression          11   1.129479 0.014460  CV_MAE\n",
      "14  LinearRegression          22   1.131009 0.018436  CV_MAE\n",
      "15             Ridge           5   1.136212 0.026442  CV_MAE\n",
      "16  LinearRegression           5   1.142055 0.029552  CV_MAE\n",
      "17             Ridge          22   1.142463 0.015107  CV_MAE\n",
      "18             Ridge          33   1.143115 0.015143  CV_MAE\n",
      "19             Ridge          43   1.147616 0.021007  CV_MAE\n",
      "20             Dummy           5   1.152468 0.038557  CV_MAE\n",
      "21             Dummy          11   1.152468 0.038557  CV_MAE\n",
      "22             Dummy          22   1.152468 0.038557  CV_MAE\n",
      "23             Dummy          33   1.152468 0.038557  CV_MAE\n",
      "24             Dummy          43   1.152468 0.038557  CV_MAE\n",
      "25        ElasticNet           5   1.152468 0.038557  CV_MAE\n",
      "26        ElasticNet          11   1.152468 0.038557  CV_MAE\n",
      "27        ElasticNet          22   1.152468 0.038557  CV_MAE\n",
      "28        ElasticNet          33   1.152468 0.038557  CV_MAE\n",
      "29        ElasticNet          43   1.152468 0.038557  CV_MAE\n",
      "30             Lasso           5   1.152468 0.038557  CV_MAE\n",
      "31             Lasso          11   1.152468 0.038557  CV_MAE\n",
      "32             Lasso          22   1.152468 0.038557  CV_MAE\n",
      "33             Lasso          33   1.152468 0.038557  CV_MAE\n",
      "34             Lasso          43   1.152468 0.038557  CV_MAE\n",
      "35               KNN          11   1.167503 0.025481  CV_MAE\n",
      "36               KNN           5   1.194559 0.032393  CV_MAE\n",
      "37               KNN          33   1.204551 0.031609  CV_MAE\n",
      "38               KNN          43   1.208321 0.021137  CV_MAE\n",
      "39               KNN          22   1.211442 0.021625  CV_MAE\n",
      "40      DecisionTree           5   1.335389 0.019682  CV_MAE\n",
      "41      DecisionTree          11   1.335389 0.019682  CV_MAE\n",
      "42      DecisionTree          22   1.335389 0.019682  CV_MAE\n",
      "43      DecisionTree          33   1.335389 0.019682  CV_MAE\n",
      "44      DecisionTree          43   1.335389 0.019682  CV_MAE\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   0.6s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   0.5s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   0.7s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=400; total time=   0.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   0.8s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   0.7s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   0.6s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=   1.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=   1.0s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=400; total time=   0.7s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=5, model__n_estimators=400; total time=   1.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=700; total time=   1.2s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   1.1s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   1.1s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   0.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   0.7s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   0.8s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=1, model__min_samples_split=5, model__n_estimators=700; total time=   0.7s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   0.7s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   0.7s\n",
      "[CV] END model__max_depth=40, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   0.7s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   0.4s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   0.6s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   0.8s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   0.8s\n",
      "[CV] END model__max_depth=40, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=10, model__n_estimators=700; total time=   0.9s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=   1.1s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=20, model__max_features=sqrt, model__min_samples_leaf=1, model__min_samples_split=2, model__n_estimators=400; total time=   0.7s\n",
      "[CV] END model__max_depth=None, model__max_features=sqrt, model__min_samples_leaf=2, model__min_samples_split=10, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=   1.4s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=2, model__n_estimators=700; total time=   1.4s\n",
      "[CV] END model__max_depth=None, model__max_features=None, model__min_samples_leaf=2, model__min_samples_split=5, model__n_estimators=200; total time=   0.5s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   1.2s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   1.0s\n",
      "[CV] END model__max_depth=20, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=10, model__n_estimators=700; total time=   1.2s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   1.1s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   1.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   0.6s\n",
      "[CV] END model__max_depth=40, model__max_features=None, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=700; total time=   1.1s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   0.6s\n",
      "[CV] END model__max_depth=20, model__max_features=log2, model__min_samples_leaf=4, model__min_samples_split=2, model__n_estimators=400; total time=   0.5s\n",
      "\n",
      "=== Best model after randomized search ===\n",
      "Model name: RandomForest\n",
      "Number of features: 5\n",
      "Best hyperparameters: {'model__n_estimators': 400, 'model__min_samples_split': 2, 'model__min_samples_leaf': 1, 'model__max_features': 'sqrt', 'model__max_depth': 20}\n",
      "Best CV score (CV MAE): 1.065709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regression call\n",
    "splits = prepare_all(\n",
    "    steam, olist, sales,\n",
    "    task_type=\"regression\",\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n",
    "\n",
    "best_steam_model = build_and_tune_models(\n",
    "    X_train_steam, y_train_steam,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_olist_model = build_and_tune_models(\n",
    "    X_train_olist, y_train_olist,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")\n",
    "\n",
    "best_sales_model = build_and_tune_models(\n",
    "    X_train_sales, y_train_sales,\n",
    "    task_type=\"regression\",\n",
    "    num_folds=3,\n",
    "    num_iterations=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
