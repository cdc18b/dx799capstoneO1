{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de57485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Standard Libraries\n",
    "# =============================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# progress / kaggle\n",
    "from tqdm.auto import tqdm\n",
    "import kagglehub\n",
    "from IPython.display import display\n",
    "\n",
    "# =============================\n",
    "# Data Science Libraries\n",
    "# =============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "# =============================\n",
    "# Visualization\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn\n",
    "# =============================\n",
    "from sklearn import clone\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    ParameterGrid,\n",
    "    ParameterSampler,\n",
    "    KFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "import sklearn.model_selection._search as sk_search\n",
    "import sklearn.model_selection._validation as sk_validation\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    MultiLabelBinarizer,\n",
    "    Normalizer,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    get_scorer,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SequentialFeatureSelector,\n",
    "    f_regression,\n",
    "    SelectKBest,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    "    RidgeClassifier,\n",
    "    LogisticRegression,\n",
    "    RidgeCV,\n",
    "    LassoCV,\n",
    "    ElasticNetCV,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# extra joblib tools\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline as SKPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif, VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, mean_absolute_error\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Global Settings\n",
    "# =============================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "random_state = 42\n",
    "N_ROWS = 50_000\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")  # no scientific notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1cdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_eda(df, name):\n",
    "    # simple settings\n",
    "    top_k_categories = 20\n",
    "    max_corr_cols = 30\n",
    "    max_rows_to_show = 25\n",
    "\n",
    "    # make printing wide and avoid scientific notation\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\", max_rows_to_show,\n",
    "        \"display.max_columns\", None,\n",
    "        \"display.width\", 1000,\n",
    "        \"display.max_colwidth\", 200,\n",
    "        \"display.float_format\", lambda x: f\"{x:.6f}\"\n",
    "    ):\n",
    "        report_lines = []\n",
    "\n",
    "        # title\n",
    "        report_lines.append(f\"=== Robust EDA Report: {name} ===\")\n",
    "\n",
    "        # shapes and memory\n",
    "        info_df = pd.DataFrame(\n",
    "            {\n",
    "                \"rows\": [df.shape[0]],\n",
    "                \"columns\": [df.shape[1]],\n",
    "                \"memory_bytes\": [int(df.memory_usage(deep=True).sum())],\n",
    "            }\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Info ===\")\n",
    "        report_lines.append(info_df.to_string(index=False))\n",
    "\n",
    "        # dtypes\n",
    "        dtypes_df = (\n",
    "            df.dtypes.rename(\"dtype\")\n",
    "            .astype(str)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"column\"})\n",
    "            .sort_values(\"column\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Dtypes ===\")\n",
    "        report_lines.append(dtypes_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # missing values\n",
    "        total_rows = len(df)\n",
    "        missing_counts = df.isna().sum()\n",
    "        if total_rows > 0:\n",
    "            missing_percent = (missing_counts / total_rows * 100).round(2)\n",
    "        else:\n",
    "            missing_percent = pd.Series([0] * len(df.columns), index=df.columns)\n",
    "        missing_df = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": df.columns,\n",
    "                    \"missing_count\": missing_counts.values,\n",
    "                    \"missing_percent\": missing_percent.values,\n",
    "                }\n",
    "            )\n",
    "            .sort_values([\"missing_count\", \"missing_percent\"], ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Missing Values ===\")\n",
    "        report_lines.append(missing_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # duplicates (safe fallback for unhashable types)\n",
    "        try:\n",
    "            duplicate_count = int(df.duplicated().sum())\n",
    "            duplicate_index = df.index[df.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "        except TypeError:\n",
    "            df_hashable = df.astype(str)\n",
    "            duplicate_count = int(df_hashable.duplicated().sum())\n",
    "            duplicate_index = df_hashable.index[df_hashable.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "\n",
    "        duplicates_summary_df = pd.DataFrame({\"duplicate_rows\": [duplicate_count]})\n",
    "        report_lines.append(\"\\n=== Duplicates Summary ===\")\n",
    "        report_lines.append(duplicates_summary_df.to_string(index=False))\n",
    "        report_lines.append(\"\\n=== Duplicates Preview (up to 20 rows) ===\")\n",
    "        if len(duplicates_preview_df) > 0:\n",
    "            report_lines.append(duplicates_preview_df.to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"(No duplicate rows found.)\")\n",
    "\n",
    "        # column groups\n",
    "        numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        categorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "        # numeric summary\n",
    "        if len(numeric_columns) > 0:\n",
    "            percentiles = [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "            numeric_summary_df = (\n",
    "                df[numeric_columns]\n",
    "                .describe(percentiles=percentiles)\n",
    "                .T.reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Numeric Summary (5%..95%) ===\")\n",
    "            report_lines.append(numeric_summary_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # skew and kurtosis\n",
    "            skew_kurt_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": numeric_columns,\n",
    "                    \"skew\": df[numeric_columns].skew(numeric_only=True).values,\n",
    "                    \"kurtosis\": df[numeric_columns].kurtosis(numeric_only=True).values,\n",
    "                }\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Skew and Kurtosis ===\")\n",
    "            report_lines.append(skew_kurt_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # IQR outliers per column\n",
    "            q1 = df[numeric_columns].quantile(0.25)\n",
    "            q3 = df[numeric_columns].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outlier_mask = (df[numeric_columns] < (q1 - 1.5 * iqr)) | (df[numeric_columns] > (q3 + 1.5 * iqr))\n",
    "            iqr_outliers_df = (\n",
    "                outlier_mask.sum()\n",
    "                .rename(\"outlier_count\")\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== IQR Outlier Counts ===\")\n",
    "            report_lines.append(iqr_outliers_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # correlation on first N numeric columns\n",
    "            if len(numeric_columns) > 1:\n",
    "                selected_cols = numeric_columns[:max_corr_cols]\n",
    "                correlation_df = df[selected_cols].corr(method=\"pearson\", numeric_only=True)\n",
    "                correlation_df.index.name = \"column\"\n",
    "                report_lines.append(f\"\\n=== Correlation (first {max_corr_cols} numeric columns) ===\")\n",
    "                report_lines.append(correlation_df.to_string())\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No numeric columns found.)\")\n",
    "\n",
    "        # categorical value counts (top K each)\n",
    "        if len(categorical_columns) > 0:\n",
    "            cat_rows = []\n",
    "            for col in categorical_columns:\n",
    "                try:\n",
    "                    vc = df[col].value_counts(dropna=False).head(top_k_categories)\n",
    "                except TypeError:\n",
    "                    vc = df[col].astype(str).value_counts(dropna=False).head(top_k_categories)\n",
    "                for value, count in vc.items():\n",
    "                    percent = (count / total_rows * 100) if total_rows > 0 else 0\n",
    "                    cat_rows.append(\n",
    "                        {\"column\": col, \"value\": value, \"count\": int(count), \"percent\": round(percent, 2)}\n",
    "                    )\n",
    "            categorical_values_df = pd.DataFrame(cat_rows)\n",
    "            report_lines.append(f\"\\n=== Categorical Values (Top {top_k_categories} per column) ===\")\n",
    "            report_lines.append(categorical_values_df.head(max_rows_to_show).to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No categorical columns found.)\")\n",
    "\n",
    "        # unique counts per column\n",
    "        def _safe_nunique(series):\n",
    "            try:\n",
    "                return int(series.nunique(dropna=False))\n",
    "            except TypeError:\n",
    "                return np.nan\n",
    "\n",
    "        unique_counts_df = pd.DataFrame(\n",
    "            {\"column\": df.columns, \"unique_values\": [_safe_nunique(df[c]) for c in df.columns]}\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Unique Counts Per Column ===\")\n",
    "        report_lines.append(unique_counts_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # sample head\n",
    "        report_lines.append(\"\\n=== Head (10 rows) ===\")\n",
    "        report_lines.append(df.head(10).to_string(index=False))\n",
    "\n",
    "        # end\n",
    "        report_lines.append(\"\\n=== End of EDA Report ===\")\n",
    "\n",
    "        # one giant print\n",
    "        print(\"\\n\".join(report_lines))\n",
    "\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "        return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bd5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3082e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Timer + memory helpers\n",
    "# =========================\n",
    "class SimpleTimer:\n",
    "    def __init__(self, enabled=True):\n",
    "        self.enabled = enabled\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "    def tick(self, label):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        t = time.perf_counter() - self.t0\n",
    "        print(f\"[timer] {label}: {t:.2f} s\")\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "def df_mem_gb(df):\n",
    "    try:\n",
    "        return float(df.memory_usage(deep=True).sum()) / (1024**3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def show_shape_mem(label, X_train=None, X_test=None):\n",
    "    parts = [label]\n",
    "    if X_train is not None:\n",
    "        parts.append(f\"X_train shape={tuple(X_train.shape)} mem={df_mem_gb(X_train):.3f} GB\")\n",
    "    if X_test is not None:\n",
    "        parts.append(f\"X_test shape={tuple(X_test.shape)} mem={df_mem_gb(X_test):.3f} GB\")\n",
    "    print(\"[info]\", \" | \".join(parts))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Text feature helpers (fast)\n",
    "# =========================\n",
    "def clean_keyword_name(s):\n",
    "    s = str(s).lower().strip().replace(\" \", \"_\")\n",
    "    keep = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch == \"_\":\n",
    "            keep.append(ch)\n",
    "    return \"\".join(keep)[:60]\n",
    "\n",
    "def text_features_fit(X, keyword_map):\n",
    "    keyword_map = keyword_map or {}\n",
    "    new_cols = []\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        if col not in X.columns:\n",
    "            continue\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"]:\n",
    "            continue\n",
    "        if col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            safe = clean_keyword_name(kw)\n",
    "            name = f\"{col}_has_{safe}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        df_part = pd.DataFrame(part, index=X.index)\n",
    "        new_parts.append(df_part)\n",
    "        new_cols.extend(df_part.columns.tolist())\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    return {\"new_cols\": new_cols, \"keyword_map\": keyword_map}\n",
    "\n",
    "def text_features_apply(X, text_info):\n",
    "    keyword_map = text_info.get(\"keyword_map\") or {}\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        if col not in X.columns:\n",
    "            part = {\n",
    "                len_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "                wc_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "            }\n",
    "            for kw in keywords:\n",
    "                name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "                part[name] = pd.Series(0, index=X.index, dtype=np.uint8)\n",
    "            new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "            continue\n",
    "\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"] or col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    for c in text_info.get(\"new_cols\", []):\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "\n",
    "    return X\n",
    "\n",
    "# length features for text columns like title_length and description_length\n",
    "def add_text_length_features_inplace(X, exclude_cols=None):\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    obj_cols = [c for c in X.columns if str(X[c].dtype) in [\"object\", \"category\"] and c not in exclude_cols]\n",
    "    for c in obj_cols:\n",
    "        X[f\"{c}_length\"] = X[c].fillna(\"\").astype(str).str.len().astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# General helpers\n",
    "# =========================\n",
    "def datetimes_to_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        if np.issubdtype(X[c].dtype, np.datetime64):\n",
    "            mask = X[c].isna()\n",
    "            vals_int = X[c].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "            arr = vals_int.astype(\"float64\") / 1000000000.0\n",
    "            if mask.any():\n",
    "                arr[mask.values] = np.nan\n",
    "            X[c] = arr\n",
    "    return X\n",
    "\n",
    "def downcast_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        dt = X[c].dtype\n",
    "        if np.issubdtype(dt, np.floating):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "        elif np.issubdtype(dt, np.integer) and X[c].nunique(dropna=True) > 2:\n",
    "            X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "def scale_fit(method, X_train_num):\n",
    "    if method == \"standard\":\n",
    "        return StandardScaler().fit(X_train_num)\n",
    "    if method == \"minmax\":\n",
    "        return MinMaxScaler().fit(X_train_num)\n",
    "    return None\n",
    "\n",
    "def scale_numeric_only(X_train, X_test, scale_method):\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return X_train, X_test\n",
    "    scaler = scale_fit(scale_method, X_train[num_cols])\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols]).astype(\"float32\")\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols]).astype(\"float32\")\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Safer OHE with caps (no exclusions)\n",
    "# =========================\n",
    "def _auto_exclude_mask(series, max_unique=500, max_avg_len=25):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    nunq = int(s.nunique(dropna=False))\n",
    "    avg_len = float(s.map(len).mean())\n",
    "    return (nunq > max_unique) or (avg_len > max_avg_len and nunq > 50)\n",
    "\n",
    "def _cap_categories(series, top_k=100, min_freq=1, other_label=\"Other\"):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    vc = s.value_counts()\n",
    "    kept = vc[vc >= min_freq].index.tolist()\n",
    "    if top_k is not None and len(kept) > top_k:\n",
    "        kept = vc.index[:top_k].tolist()\n",
    "    mapped = s.where(s.isin(kept), other_label)\n",
    "    return mapped.astype(\"category\"), kept\n",
    "\n",
    "def ohe_fit(\n",
    "    X,\n",
    "    exclude_cols=None,\n",
    "    top_k_per_col=100,\n",
    "    min_freq_per_col=1,\n",
    "    auto_exclude=False,\n",
    "    high_card_threshold=500,\n",
    "    long_text_avglen=25,\n",
    "):\n",
    "    exclude = set(exclude_cols or [])\n",
    "    value_map = {}\n",
    "\n",
    "    X_tmp = X.copy()\n",
    "\n",
    "    obj_cols = [c for c in X_tmp.select_dtypes(include=[\"object\", \"category\"]).columns]\n",
    "\n",
    "    excluded = list(exclude)\n",
    "\n",
    "    for c in obj_cols:\n",
    "        capped, kept = _cap_categories(X_tmp[c], top_k=top_k_per_col, min_freq=min_freq_per_col)\n",
    "        X_tmp[c] = capped\n",
    "        value_map[c] = kept\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=obj_cols, dummy_na=False)\n",
    "    schema_cols = X_ohe.columns.tolist()\n",
    "\n",
    "    return {\n",
    "        \"obj_cols\": obj_cols,\n",
    "        \"schema_cols\": schema_cols,\n",
    "        \"value_map\": value_map,\n",
    "        \"excluded\": excluded,\n",
    "        \"other_label\": \"Other\",\n",
    "    }\n",
    "\n",
    "def ohe_apply(X, ohe_info):\n",
    "    obj_cols = ohe_info[\"obj_cols\"]\n",
    "    schema_cols = ohe_info[\"schema_cols\"]\n",
    "    value_map = ohe_info[\"value_map\"]\n",
    "    other = ohe_info.get(\"other_label\", \"Other\")\n",
    "    excluded = ohe_info.get(\"excluded\", [])\n",
    "\n",
    "    X_tmp = X.drop(columns=excluded, errors=\"ignore\").copy()\n",
    "\n",
    "    for c in obj_cols:\n",
    "        if c in X_tmp.columns:\n",
    "            s = X_tmp[c].fillna(\"Unknown\").astype(str)\n",
    "            kept = set(value_map.get(c, []))\n",
    "            s = s.where(s.isin(kept), other).astype(\"category\")\n",
    "            X_tmp[c] = s\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=[c for c in obj_cols if c in X_tmp.columns], dummy_na=False)\n",
    "    X_ohe = X_ohe.reindex(columns=schema_cols, fill_value=0)\n",
    "    return X_ohe\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Outliers\n",
    "# =========================\n",
    "def outlier_bounds_fit(X_num, lower_q=0.025, upper_q=0.975, exclude_binary=True, sample_rows=200000):\n",
    "    bounds = {}\n",
    "    if lower_q is None or upper_q is None:\n",
    "        return bounds\n",
    "    X_use = X_num\n",
    "    if len(X_num) > sample_rows:\n",
    "        X_use = X_num.sample(n=sample_rows, random_state=123)\n",
    "    for c in X_use.columns:\n",
    "        vals = X_use[c].astype(\"float32\")\n",
    "        if exclude_binary and X_use[c].nunique(dropna=True) <= 2:\n",
    "            continue\n",
    "        lo = np.nanquantile(vals, lower_q)\n",
    "        hi = np.nanquantile(vals, upper_q)\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and hi >= lo:\n",
    "            bounds[c] = (float(lo), float(hi))\n",
    "    return bounds\n",
    "\n",
    "def outlier_mask(X, bounds):\n",
    "    if not bounds:\n",
    "        return pd.Series(True, index=X.index)\n",
    "    m = pd.Series(True, index=X.index)\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in X.columns:\n",
    "            col = X[c].astype(\"float32\")\n",
    "            m &= (col >= lo) & (col <= hi)\n",
    "    return m\n",
    "\n",
    "def outlier_clip_inplace(X, bounds):\n",
    "    if not bounds:\n",
    "        return X\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].astype(\"float32\").clip(lo, hi)\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature selection\n",
    "# =========================\n",
    "def forward_feature_selection(X, y, model,\n",
    "                              scoring=\"neg_mean_absolute_error\",\n",
    "                              cv=5, tol=None, max_features=None, n_jobs=-1, verbose=False):\n",
    "    try:\n",
    "        feature_names = list(X.columns)\n",
    "        X_arr = X.values\n",
    "    except AttributeError:\n",
    "        X_arr = X\n",
    "        feature_names = [f\"f{i}\" for i in range(X_arr.shape[1])]\n",
    "\n",
    "    selected_idx = []\n",
    "    remaining_idx = list(range(X_arr.shape[1]))\n",
    "    best_scores = []\n",
    "    previous_score = float(\"inf\")\n",
    "    best_feature_set_idx = []\n",
    "    best_score = float(\"inf\")\n",
    "\n",
    "    while remaining_idx:\n",
    "        scores = {}\n",
    "        for idx in remaining_idx:\n",
    "            trial_idx = selected_idx + [idx]\n",
    "            cv_score = -cross_val_score(\n",
    "                model, X_arr[:, trial_idx], y,\n",
    "                scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "            ).mean()\n",
    "            scores[idx] = cv_score\n",
    "\n",
    "        best_idx = min(scores, key=scores.get)\n",
    "        current_score = scores[best_idx]\n",
    "\n",
    "        if tol is not None and previous_score - current_score < tol:\n",
    "            if verbose:\n",
    "                print(\"Stopping early (improvement < tol).\")\n",
    "            break\n",
    "\n",
    "        selected_idx.append(best_idx)\n",
    "        remaining_idx.remove(best_idx)\n",
    "        best_scores.append(current_score)\n",
    "        previous_score = current_score\n",
    "\n",
    "        if verbose:\n",
    "            name = feature_names[best_idx]\n",
    "            print(f\"Added {name} -> CV score = {current_score:.4f}\")\n",
    "\n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_feature_set_idx = selected_idx.copy()\n",
    "\n",
    "        if max_features is not None and len(selected_idx) >= max_features:\n",
    "            break\n",
    "\n",
    "    selected_features = [feature_names[i] for i in selected_idx]\n",
    "    best_feature_set = [feature_names[i] for i in best_feature_set_idx]\n",
    "\n",
    "    if not best_feature_set:\n",
    "        best_feature_set = selected_features[:]\n",
    "        best_score = best_scores[-1] if best_scores else float(\"inf\")\n",
    "\n",
    "    if verbose:\n",
    "        try:\n",
    "            index = np.argmax(np.array(selected_features) == best_feature_set[-1])\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(best_scores) + 1), best_scores, marker=\".\")\n",
    "            plt.plot([index + 1], [best_score], marker=\"x\")\n",
    "            plt.xticks(range(1, len(selected_features) + 1),\n",
    "                       selected_features, rotation=60, ha=\"right\", fontsize=6)\n",
    "            plt.title(\"Forward Feature Selection and CV Scores\")\n",
    "            plt.xlabel(\"Features Added\")\n",
    "            plt.ylabel(\"CV Score (MAE)\")\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"Best Features: {best_feature_set}\")\n",
    "        print(f\"Best CV MAE Score: {best_score:.4f}\")\n",
    "    return selected_features, best_scores, best_feature_set, best_score\n",
    "\n",
    "def select_features(method, max_features, task_type, random_state, X_train, y_train, verbose=False):\n",
    "    if method is None or method == \"none\":\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    k = min(max_features, X_train.shape[1]) if max_features else X_train.shape[1]\n",
    "    if k < 1:\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    if method == \"tree\":\n",
    "        est = RandomForestClassifier(random_state=random_state) if task_type == \"classification\" else RandomForestRegressor(random_state=random_state)\n",
    "        est.fit(X_train, y_train)\n",
    "        imp = pd.Series(est.feature_importances_, index=X_train.columns)\n",
    "        return imp.nlargest(k).index.tolist()\n",
    "\n",
    "    if method == \"forward\":\n",
    "        if task_type != \"regression\":\n",
    "            raise ValueError(\"Forward selection is only supported for regression tasks.\")\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        _, _, best_set, _ = forward_feature_selection(\n",
    "            X=X_train, y=y_train, model=model,\n",
    "            scoring=\"neg_mean_absolute_error\", cv=3,\n",
    "            tol=None, max_features=max_features, n_jobs=-1, verbose=verbose\n",
    "        )\n",
    "        return best_set\n",
    "\n",
    "    if method == \"mutual_info\":\n",
    "        sel = SelectKBest(mutual_info_classif if task_type == \"classification\" else mutual_info_regression, k=k)\n",
    "        sel.fit(X_train, y_train)\n",
    "        return X_train.columns[sel.get_support()].tolist()\n",
    "\n",
    "    return X_train.columns.tolist()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Batched poly features\n",
    "# =========================\n",
    "def add_poly_features_batched(X_train, X_test, squares, pairs):\n",
    "    train_parts = {}\n",
    "    for c in squares:\n",
    "        if c in X_train.columns:\n",
    "            train_parts[f\"{c}_sq\"] = X_train[c].astype(\"float32\") ** 2\n",
    "    for a, b in pairs:\n",
    "        if (a in X_train.columns) and (b in X_train.columns):\n",
    "            name = f\"{a}_x_{b}\"\n",
    "            train_parts[name] = (X_train[a].astype(\"float32\") * X_train[b].astype(\"float32\"))\n",
    "\n",
    "    test_parts = {}\n",
    "    for name in train_parts:\n",
    "        if name.endswith(\"_sq\"):\n",
    "            c = name[:-3]\n",
    "            if c in X_test.columns:\n",
    "                test_parts[name] = X_test[c].astype(\"float32\") ** 2\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "        else:\n",
    "            a, b = name.split(\"_x_\")\n",
    "            if (a in X_test.columns) and (b in X_test.columns):\n",
    "                test_parts[name] = (X_test[a].astype(\"float32\") * X_test[b].astype(\"float32\"))\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "\n",
    "    if train_parts:\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(train_parts, index=X_train.index)], axis=1)\n",
    "    if test_parts:\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(test_parts, index=X_test.index)], axis=1)\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train.copy())\n",
    "    X_test = downcast_numeric_inplace(X_test.copy())\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main prep\n",
    "# =========================\n",
    "\n",
    "def prepare_data(steam_df, olist_df, sales_df, test_size, random_state,\n",
    "                 feature_selection, max_features, task_type, scale_method,\n",
    "                 steam_target_cutoff=50.0, olist_target_cutoff=2.5, sales_target_cutoff=5.0,\n",
    "                 tag_min_count=5, tag_top_k=200,\n",
    "                 outlier_lower_q=0.025, outlier_upper_q=0.975,\n",
    "                 verbose=False,\n",
    "                 ohe_top_k_per_col=100,\n",
    "                 ohe_min_freq_per_col=1,\n",
    "                 ohe_auto_exclude=False,\n",
    "                 ohe_high_card_threshold=500,\n",
    "                 ohe_long_text_avglen=25):\n",
    "    feature_selection = (feature_selection or \"none\").lower()\n",
    "    outputs = {}\n",
    "    timer = SimpleTimer(enabled=verbose)\n",
    "\n",
    "    # ---------- STEAM ----------\n",
    "    steam = steam_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        steam[\"target\"] = (steam[\"positive_ratio\"] >= float(steam_target_cutoff)).astype(int)\n",
    "    else:\n",
    "        steam[\"target\"] = steam[\"positive_ratio\"]\n",
    "    steam.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    steam.drop(columns=[\"app_id\", \"user_id\", \"review_id\", \"positive_ratio\"],\n",
    "               errors=\"ignore\", inplace=True)\n",
    "\n",
    "    if {\"date\", \"date_release\"}.issubset(steam.columns):\n",
    "        steam[\"days_since_release\"] = (steam[\"date\"] - steam[\"date_release\"]).dt.days\n",
    "\n",
    "    for col in [\"is_recommended\", \"mac\", \"linux\", \"win\", \"steam_deck\"]:\n",
    "        if col in steam.columns:\n",
    "            steam[col] = steam[col].astype(int)\n",
    "\n",
    "    if \"hours\" in steam.columns:\n",
    "        steam[\"log_hours\"] = np.log1p(steam[\"hours\"])\n",
    "    if {\"hours\", \"user_reviews\"}.issubset(steam.columns):\n",
    "        steam[\"reviews_per_hour\"] = steam[\"user_reviews\"] / (steam[\"hours\"] + 0.000000001)\n",
    "\n",
    "    X = steam.drop(columns=[\"target\", \"rating\"], errors=\"ignore\")\n",
    "    y = steam[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"steam split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam post split\", X_train, X_test)\n",
    "\n",
    "    # always run these conversions (not tied to verbose)\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"steam datetime to numeric\")\n",
    "\n",
    "    # add text length features\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test,  exclude_cols=[\"tags\"])\n",
    "\n",
    "    steam_kw = {\n",
    "        \"title\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"coop\", \"online\", \"free\", \"demo\", \"survival\"],\n",
    "        \"description\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"open world\", \"story\", \"puzzle\", \"horror\", \"early access\"],\n",
    "    }\n",
    "    steam_text_info = text_features_fit(X_train, steam_kw)\n",
    "    X_test = text_features_apply(X_test, steam_text_info)\n",
    "    if verbose:\n",
    "        timer.tick(\"steam text features\")\n",
    "        show_shape_mem(\"steam after text\", X_train, X_test)\n",
    "\n",
    "    if \"tags\" in X_train.columns:\n",
    "        from collections import Counter\n",
    "\n",
    "        def tag_col_name(t):\n",
    "            s = str(t).lower().strip().replace(\" \", \"_\")\n",
    "            return \"tag_\" + \"\".join(ch for ch in s if ch.isalnum() or ch == \"_\")[:60]\n",
    "\n",
    "        cnt = Counter()\n",
    "        for v in X_train[\"tags\"].fillna(\"\").values:\n",
    "            lst = v if isinstance(v, list) else []\n",
    "            for t in lst:\n",
    "                cnt[t] += 1\n",
    "\n",
    "        items = [(t, n) for t, n in cnt.items() if n >= tag_min_count]\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        vocab = [t for t, _ in items[:tag_top_k]]\n",
    "        tag_cols = [tag_col_name(t) for t in vocab]\n",
    "        if verbose:\n",
    "            print(f\"[info] steam tags unique={len(cnt)}, kept={len(vocab)} (min_count={tag_min_count}, top_k={tag_top_k})\")\n",
    "\n",
    "        def add_tag_cols_fast(df):\n",
    "            if \"tags\" in df.columns:\n",
    "                tag_lists = df[\"tags\"].apply(lambda v: v if isinstance(v, list) else [])\n",
    "            else:\n",
    "                tag_lists = pd.Series([[]] * len(df), index=df.index)\n",
    "            new_data = {}\n",
    "            for tag, col_name in zip(vocab, tag_cols):\n",
    "                new_data[col_name] = np.fromiter(\n",
    "                    (1 if tag in lst else 0 for lst in tag_lists),\n",
    "                    dtype=np.uint8,\n",
    "                    count=len(df)\n",
    "                )\n",
    "            new_df = pd.DataFrame(new_data, index=df.index)\n",
    "            return pd.concat([df.drop(columns=[\"tags\"], errors=\"ignore\"), new_df], axis=1)\n",
    "\n",
    "        X_train = add_tag_cols_fast(X_train)\n",
    "        X_test = add_tag_cols_fast(X_test)\n",
    "        if verbose:\n",
    "            timer.tick(\"steam tags multi-hot\")\n",
    "            show_shape_mem(\"steam after tags\", X_train, X_test)\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"steam OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after OHE\", X_train, X_test)\n",
    "\n",
    "    # numeric-only safety\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "    if verbose:\n",
    "        print(\"steam non-numeric after OHE:\", X_train.select_dtypes(exclude=[\"number\"]).columns.tolist())\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"steam impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    steam_squares = []\n",
    "    if \"log_hours\" in X_train.columns:\n",
    "        steam_squares.append(\"log_hours\")\n",
    "    elif \"hours\" in X_train.columns:\n",
    "        steam_squares.append(\"hours\")\n",
    "    if \"discount\" in X_train.columns:\n",
    "        steam_squares.append(\"discount\")\n",
    "    if \"days_since_release\" in X_train.columns:\n",
    "        steam_squares.append(\"days_since_release\")\n",
    "\n",
    "    steam_pairs = []\n",
    "    if (\"price_final\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_final\", \"discount\"))\n",
    "    if (\"price_original\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_original\", \"discount\"))\n",
    "    if (\"days_since_release\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"days_since_release\", \"discount\"))\n",
    "    if (\"user_reviews\" in X_train.columns) and (\"reviews\" in X_train.columns):\n",
    "        steam_pairs.append((\"user_reviews\", \"reviews\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, steam_squares, steam_pairs)\n",
    "    timer.tick(\"steam poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test  = outlier_clip_inplace(X_test,  bounds)\n",
    "    timer.tick(\"steam outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"steam scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train, verbose=verbose)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"steam select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"steam\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- OLIST ----------\n",
    "    olist = olist_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        olist[\"target\"] = (olist[\"review_score_mean_product\"] >= float(olist_target_cutoff)).astype(int)\n",
    "    else:\n",
    "        olist[\"target\"] = olist[\"review_score_mean_product\"]\n",
    "    olist.dropna(subset=[\"target\"], inplace=True)\n",
    "    olist.drop(columns=[\"order_id\", \"customer_id\", \"customer_unique_id\"],\n",
    "               errors=\"ignore\", inplace=True)\n",
    "\n",
    "    olist[\"delivery_delay\"] = (olist[\"order_estimated_delivery_date\"] - olist[\"order_purchase_timestamp\"]).dt.days\n",
    "    denom = olist[\"payment_installments_max\"].replace(0, 1)\n",
    "    olist[\"avg_installment\"] = olist[\"payment_value_total\"] / denom\n",
    "\n",
    "    if {\"product_length_cm\", \"product_width_cm\", \"product_height_cm\"}.issubset(olist.columns):\n",
    "        olist[\"product_volume_cm3\"] = (\n",
    "            olist[\"product_length_cm\"] * olist[\"product_width_cm\"] * olist[\"product_height_cm\"]\n",
    "        )\n",
    "\n",
    "    X = olist.drop(columns=[\"review_score_mean_product\", \"target\"], errors=\"ignore\")\n",
    "    y = olist[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"olist split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"olist datetime to numeric\")\n",
    "\n",
    "    # add text length features\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test,  exclude_cols=[\"tags\"])\n",
    "\n",
    "    olist_kw = {\n",
    "        \"order_status\": [\"delivered\", \"shipped\", \"canceled\", \"invoiced\", \"processing\"],\n",
    "        \"product_category_name\": [\"moveis\", \"auto\", \"pet\", \"perfumaria\", \"utilidades\", \"brinquedos\"]\n",
    "    }\n",
    "    olist_text_info = text_features_fit(X_train, olist_kw)\n",
    "    X_test = text_features_apply(X_test, olist_text_info)\n",
    "    timer.tick(\"olist text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"olist OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"olist impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    olist_squares = []\n",
    "    if \"delivery_delay\" in X_train.columns:\n",
    "        olist_squares.append(\"delivery_delay\")\n",
    "    if \"price\" in X_train.columns:\n",
    "        olist_squares.append(\"price\")\n",
    "    if \"freight_value\" in X_train.columns:\n",
    "        olist_squares.append(\"freight_value\")\n",
    "\n",
    "    olist_pairs = []\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_weight_g\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_weight_g\"))\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_volume_cm3\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_volume_cm3\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"price\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"freight_value\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"freight_value\"))\n",
    "    if (\"payment_installments_max\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"payment_installments_max\", \"price\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, olist_squares, olist_pairs)\n",
    "    timer.tick(\"olist poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test  = outlier_clip_inplace(X_test,  bounds)\n",
    "    timer.tick(\"olist outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"olist scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train, verbose=verbose)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"olist select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"olist\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- SALES ----------\n",
    "    sales = sales_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        sales[\"target\"] = (sales[\"Critic_Score\"] >= float(sales_target_cutoff)).astype(int)\n",
    "    else:\n",
    "        sales[\"target\"] = sales[\"Critic_Score\"]\n",
    "    sales.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    for c in [\"ESRB_Rating\", \"Genre\", \"Platform\", \"Publisher\", \"Developer\"]:\n",
    "        if c in sales.columns:\n",
    "            sales[c] = sales[c].fillna(\"Unknown\")\n",
    "\n",
    "    X = sales.drop(columns=[\"target\", \"Critic_Score\"], errors=\"ignore\")\n",
    "    y = sales[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"sales split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"sales datetime to numeric\")\n",
    "\n",
    "    # add text length features\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test,  exclude_cols=[\"tags\"])\n",
    "\n",
    "    sales_kw = {\n",
    "        \"Name\": [\"mario\", \"pokemon\", \"zelda\", \"call of duty\", \"fifa\", \"minecraft\", \"final fantasy\"],\n",
    "        \"Genre\": [\"action\", \"sports\", \"shooter\", \"racing\", \"role\", \"adventure\", \"platform\", \"puzzle\"],\n",
    "        \"Publisher\": [\"nintendo\", \"electronic arts\", \"ea\", \"activision\", \"ubisoft\", \"sony\", \"sega\"],\n",
    "        \"ESRB_Rating\": [\"e\", \"t\", \"m\"]\n",
    "    }\n",
    "    sales_text_info = text_features_fit(X_train, sales_kw)\n",
    "    X_test = text_features_apply(X_test, sales_text_info)\n",
    "    timer.tick(\"sales text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=ohe_top_k_per_col,\n",
    "        min_freq_per_col=ohe_min_freq_per_col,\n",
    "        auto_exclude=ohe_auto_exclude,\n",
    "        high_card_threshold=ohe_high_card_threshold,\n",
    "        long_text_avglen=ohe_long_text_avglen,\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"sales OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"sales impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    sales_squares = []\n",
    "    if \"Year\" in X_train.columns:\n",
    "        sales_squares.append(\"Year\")\n",
    "    if \"User_Score\" in X_train.columns:\n",
    "        sales_squares.append(\"User_Score\")\n",
    "\n",
    "    sales_pairs = []\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"PAL_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"PAL_Sales\"))\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"JP_Sales\"))\n",
    "    if (\"PAL_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"PAL_Sales\", \"JP_Sales\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, sales_squares, sales_pairs)\n",
    "    timer.tick(\"sales poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=outlier_lower_q,\n",
    "        upper_q=outlier_upper_q,\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test  = outlier_clip_inplace(X_test,  bounds)\n",
    "    timer.tick(\"sales outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, scale_method)\n",
    "    timer.tick(\"sales scale\")\n",
    "\n",
    "    keep_cols = select_features(feature_selection, max_features, task_type, random_state, X_train, y_train, verbose=verbose)\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"sales select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"sales\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- Shape summary ----------\n",
    "    if verbose:\n",
    "        for name, parts in outputs.items():\n",
    "            Xtr, Xte, ytr, yte = parts\n",
    "            print(f\"[{name}] X_train: {Xtr.shape} | X_test: {Xte.shape} | y_train: {ytr.shape} | y_test: {yte.shape}\")\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18a3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.243 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.174 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.197 sec\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 50000 of 41154794 rows in 5.946 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(50000, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: picked 50000 of 99441 rows in 0.005 sec\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (57112, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: class counts too small for requested size, falling back to simple sample\n",
      "simple_random_sample: picked 50000 of 55792 rows in 0.004 sec\n",
      "vg2019: done shape=(50000, 16)\n",
      "main: load all done in 17.526 sec (00:00:17)\n",
      "download: shapes summary\n",
      "download: steam shape = (50000, 24)\n",
      "download: olist shape = (57112, 38)\n",
      "download: sales shape = (50000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Download Paths\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "\n",
    "# Load All\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# Download Shapes\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3177e9",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31e3f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASELINE (all base cutoffs) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:   0%|          | 1/1503 [00:07<3:09:09,  7.56s/it, ETA ~ 00:15:38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steam baseline: CV F1 Macro = 0.8990 ± 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:   0%|          | 2/1503 [00:18<3:58:09,  9.52s/it, ETA ~ 00:57:26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olist baseline: CV F1 Macro = 0.8151 ± 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall:   0%|          | 3/1503 [00:23<3:02:30,  7.30s/it, ETA ~ 00:19:17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales baseline: CV F1 Macro = 0.6564 ± 0.0147\n",
      "\n",
      "=== STEAM random sweep: 500 trials ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Overall:   0%|          | 4/1503 [00:55<7:09:06, 17.18s/it, ETA ~ 02:53:25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[steam] new best F1=0.7919 at cutoff 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Overall:   0%|          | 5/1503 [01:11<6:59:27, 16.80s/it, ETA ~ 03:04:50]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[steam] new best F1=0.9502 at cutoff 90.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Overall:   0%|          | 7/1503 [02:01<8:44:01, 21.02s/it, ETA ~ 04:21:15]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[steam] new best F1=0.9646 at cutoff 95.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \n",
      "Overall:   1%|          | 9/1503 [03:11<11:52:54, 28.63s/it, ETA ~ 06:00:37]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[steam] new best F1=0.9657 at cutoff 95.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \n",
      "Overall:   4%|▍         | 65/1503 [20:51<10:43:58, 26.87s/it, ETA ~ 05:08:41]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[steam] new best F1=0.9660 at cutoff 95.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \n",
      "Overall:   8%|▊         | 120/1503 [38:20<6:10:09, 16.06s/it, ETA ~ 05:06:32]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[steam] new best F1=0.9667 at cutoff 95.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  22%|██▏       | 334/1503 [1:45:51<5:50:41, 18.00s/it, ETA ~ 05:02:39] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[steam] new best F1=0.9684 at cutoff 95.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STEAM sweep: 100%|██████████| 500/500 [2:38:23<00:00, 19.01s/it, ETA ~ 23:45:05]\n",
      "Overall:  33%|███▎      | 503/1503 [2:38:47<5:04:20, 18.26s/it, ETA ~ 05:00:46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steam: best F1 = 0.9684 ± 0.0031 at cutoff 95.00\n",
      "\n",
      "=== OLIST random sweep: 500 trials ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  34%|███▎      | 504/1503 [2:39:13<5:43:29, 20.63s/it, ETA ~ 05:01:07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.7127 at cutoff 3.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  34%|███▎      | 505/1503 [2:39:39<6:12:54, 22.42s/it, ETA ~ 05:01:30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.7163 at cutoff 3.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  34%|███▎      | 506/1503 [2:39:59<6:01:06, 21.73s/it, ETA ~ 05:01:34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.7631 at cutoff 3.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  34%|███▍      | 510/1503 [2:41:07<5:29:28, 19.91s/it, ETA ~ 05:01:09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.8140 at cutoff 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  34%|███▍      | 512/1503 [2:41:54<5:55:13, 21.51s/it, ETA ~ 05:01:36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.8140 at cutoff 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  37%|███▋      | 549/1503 [2:56:51<7:26:43, 28.10s/it, ETA ~ 05:10:31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.8252 at cutoff 4.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  44%|████▎     | 656/1503 [3:37:20<6:28:40, 27.53s/it, ETA ~ 05:24:17] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.8333 at cutoff 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  45%|████▍     | 676/1503 [3:44:53<5:16:50, 22.99s/it, ETA ~ 05:26:19]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.8845 at cutoff 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  46%|████▋     | 697/1503 [3:53:06<5:14:51, 23.44s/it, ETA ~ 05:28:58]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.8930 at cutoff 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \n",
      "Overall:  58%|█████▊    | 870/1503 [4:59:15<4:13:42, 24.05s/it, ETA ~ 05:43:19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[olist] new best F1=0.8956 at cutoff 4.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OLIST sweep: 100%|██████████| 500/500 [3:11:58<00:00, 23.04s/it, ETA ~ 02:57:04]\n",
      "Overall:  67%|██████▋   | 1003/1503 [5:50:45<3:34:50, 25.78s/it, ETA ~ 05:51:56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olist: best F1 = 0.8956 ± 0.0029 at cutoff 4.00\n",
      "\n",
      "=== SALES random sweep: 500 trials ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Overall:  67%|██████▋   | 1004/1503 [5:51:07<3:24:31, 24.59s/it, ETA ~ 05:51:57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sales] new best F1=0.6533 at cutoff 8.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Overall:  67%|██████▋   | 1007/1503 [5:52:07<3:05:25, 22.43s/it, ETA ~ 05:51:53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sales] new best F1=0.6548 at cutoff 8.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Overall:  67%|██████▋   | 1008/1503 [5:52:22<2:45:21, 20.04s/it, ETA ~ 05:51:43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sales] new best F1=0.7180 at cutoff 5.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Overall:  69%|██████▉   | 1035/1503 [5:59:21<2:03:09, 15.79s/it, ETA ~ 05:48:09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sales] new best F1=0.7188 at cutoff 5.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SALES sweep: 100%|██████████| 500/500 [2:29:05<00:00, 17.89s/it, ETA ~ 05:26:09]\n",
      "Overall: 100%|██████████| 1503/1503 [8:19:51<00:00, 19.95s/it, ETA ~ 05:26:09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales: best F1 = 0.7188 ± 0.0116 at cutoff 5.00\n",
      "\n",
      "=== BEST HYPERPARAMETERS PER DATASET (random sweeps) ===\n",
      "dataset  best_F1   std_F1  cutoff_used feature_selection  max_features scale_method  ohe_auto_exclude  tag_min_count  tag_top_k  ohe_top_k_per_col  ohe_min_freq_per_col  outlier_lower_q  outlier_upper_q  ohe_high_card_threshold  ohe_long_text_avglen\n",
      "  steam 0.968418 0.003113    95.000000       mutual_info     50.000000     standard             False              5        200                400                     5         0.050000         0.950000                      300                    25\n",
      "  olist 0.895588 0.002908     4.000000              tree     10.000000     standard              True              3        400                400                     1         0.010000         0.990000                      800                    35\n",
      "  sales 0.718838 0.011551     5.000000       mutual_info           NaN         None              True             10        100                200                     5         0.010000         0.990000                      800                    20\n",
      "\n",
      "=== TOP 10 TRIALS: STEAM ===\n",
      "dataset  mean_f1   std_f1  steam_cutoff feature_selection  max_features scale_method  ohe_auto_exclude  tag_min_count  tag_top_k  ohe_top_k_per_col  ohe_min_freq_per_col  outlier_lower_q  outlier_upper_q  ohe_high_card_threshold  ohe_long_text_avglen\n",
      "  steam 0.968418 0.003113     95.000000       mutual_info     50.000000     standard             False              5        200                400                     5         0.050000         0.950000                      300                    25\n",
      "  steam 0.966714 0.002311     95.000000       mutual_info     10.000000     standard              True              3        200                200                     1         0.020000         0.980000                      500                    35\n",
      "  steam 0.966628 0.002678     95.000000       mutual_info     10.000000       minmax             False             10        100                100                     5         0.010000         0.990000                      500                    35\n",
      "  steam 0.966526 0.001654     95.000000       mutual_info    100.000000     standard              True              3        400                100                     5         0.010000         0.990000                      300                    35\n",
      "  steam 0.966405 0.002153     95.000000       mutual_info     10.000000         None              True              3        200                100                     5         0.025000         0.975000                      300                    20\n",
      "  steam 0.965986 0.002494     95.000000       mutual_info     10.000000       minmax              True             10        400                100                     5         0.020000         0.980000                     1200                    35\n",
      "  steam 0.965874 0.002141     95.000000       mutual_info     10.000000     standard              True              5        800                100                     2         0.050000         0.950000                      300                    25\n",
      "  steam 0.965840 0.002531     95.000000       mutual_info     10.000000     standard             False             10        400                 50                     5         0.025000         0.975000                     1200                    35\n",
      "  steam 0.965723 0.003487     95.000000       mutual_info     20.000000       minmax             False              5        800                100                     5         0.010000         0.990000                      500                    20\n",
      "  steam 0.964602 0.002160     95.000000       mutual_info     10.000000     standard             False             10        200                400                     5         0.050000         0.950000                      500                    20\n",
      "\n",
      "=== TOP 10 TRIALS: OLIST ===\n",
      "dataset  mean_f1   std_f1  olist_cutoff feature_selection  max_features scale_method  ohe_auto_exclude  tag_min_count  tag_top_k  ohe_top_k_per_col  ohe_min_freq_per_col  outlier_lower_q  outlier_upper_q  ohe_high_card_threshold  ohe_long_text_avglen\n",
      "  olist 0.895588 0.002908      4.000000              tree     10.000000     standard              True              3        400                400                     1         0.010000         0.990000                      800                    35\n",
      "  olist 0.895432 0.002892      4.000000              tree     10.000000       minmax              True              3        800                200                     1         0.010000         0.990000                      800                    25\n",
      "  olist 0.893035 0.003276      4.000000              tree     10.000000         None              True             10        200                200                     2         0.020000         0.980000                      500                    25\n",
      "  olist 0.884797 0.003841      4.000000              tree     10.000000       minmax              True             10        100                 50                     1         0.050000         0.950000                      800                    20\n",
      "  olist 0.884477 0.003808      4.000000              tree     10.000000     standard              True              3        800                 50                     1         0.050000         0.950000                      500                    25\n",
      "  olist 0.884477 0.003808      4.000000              tree     10.000000     standard             False             10        800                100                     5         0.050000         0.950000                      300                    35\n",
      "  olist 0.851065 0.004293      4.000000              tree     20.000000     standard             False             10        400                200                     5         0.050000         0.950000                      300                    20\n",
      "  olist 0.847725 0.004127      4.500000              tree     10.000000         None             False             10        100                200                     5         0.010000         0.990000                      300                    35\n",
      "  olist 0.844622 0.003719      4.500000              tree     10.000000     standard              True             10        800                 50                     5         0.050000         0.950000                      500                    35\n",
      "  olist 0.844474 0.003499      4.500000              tree     10.000000         None              True              5        200                100                     1         0.050000         0.950000                      500                    25\n",
      "\n",
      "=== TOP 10 TRIALS: SALES ===\n",
      "dataset  mean_f1   std_f1  sales_cutoff feature_selection  max_features scale_method  ohe_auto_exclude  tag_min_count  tag_top_k  ohe_top_k_per_col  ohe_min_freq_per_col  outlier_lower_q  outlier_upper_q  ohe_high_card_threshold  ohe_long_text_avglen\n",
      "  sales 0.718838 0.011551      5.000000       mutual_info    200.000000         None              True             10        400                 50                     1         0.010000         0.990000                      500                    25\n",
      "  sales 0.718838 0.011551      5.000000              none    100.000000         None              True              5        800                100                     5         0.010000         0.990000                      500                    35\n",
      "  sales 0.718838 0.011551      5.000000       mutual_info           NaN         None              True             10        100                200                     5         0.010000         0.990000                      800                    20\n",
      "  sales 0.718838 0.011551      5.000000       mutual_info     20.000000         None             False              5        100                 50                     5         0.010000         0.990000                      300                    25\n",
      "  sales 0.718008 0.011361      5.000000       mutual_info    100.000000       minmax              True              5        200                100                     5         0.010000         0.990000                      800                    25\n",
      "  sales 0.718008 0.011361      5.000000              none    100.000000       minmax             False             10        100                200                     2         0.010000         0.990000                      500                    25\n",
      "  sales 0.718008 0.011361      5.000000       mutual_info           NaN       minmax              True             10        100                 50                     5         0.010000         0.990000                      300                    25\n",
      "  sales 0.718008 0.011361      5.000000              none     20.000000       minmax              True             10        800                 50                     2         0.010000         0.990000                      300                    20\n",
      "  sales 0.716862 0.010811      5.000000              none    100.000000     standard             False             10        400                200                     2         0.010000         0.990000                      300                    35\n",
      "  sales 0.716862 0.010811      5.000000       mutual_info           NaN     standard             False              5        200                200                     2         0.010000         0.990000                      300                    25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Add progress bars + ETA ===\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install tqdm: pip install tqdm\")\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "# simple CV helper\n",
    "def run_cv_score(X_train, y_train):\n",
    "    unique_classes = np.unique(y_train)\n",
    "    if unique_classes.shape[0] < 2:\n",
    "        return None, None\n",
    "    rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(\n",
    "        rf_model, X_train, y_train,\n",
    "        scoring=\"f1_macro\", cv=skf, n_jobs=-1\n",
    "    )\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# base cutoffs\n",
    "base_steam_cutoff = 80.0\n",
    "base_olist_cutoff = 4.0\n",
    "base_sales_cutoff = 8.0\n",
    "\n",
    "# cutoff choices\n",
    "steam_cutoff_values = [50.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0]\n",
    "olist_cutoff_values = [2.0, 2.5, 3.0, 3.5, 4.0, 4.5]\n",
    "sales_cutoff_values = [5.0, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5]\n",
    "\n",
    "# cleaning hyperparameter choices\n",
    "feature_selection_values = [\"none\", \"tree\", \"mutual_info\"]\n",
    "max_features_values = [None, 10, 20, 50, 100, 200]\n",
    "scale_method_values = [None, \"standard\", \"minmax\"]\n",
    "\n",
    "ohe_auto_exclude_values = [False, True]\n",
    "ohe_top_k_per_col_values = [50, 100, 200, 400]\n",
    "ohe_min_freq_per_col_values = [1, 2, 5]\n",
    "ohe_high_card_threshold_values = [300, 500, 800, 1200]\n",
    "ohe_long_text_avglen_values = [20, 25, 35]\n",
    "\n",
    "tag_min_count_values = [3, 5, 10]\n",
    "tag_top_k_values = [100, 200, 400, 800]\n",
    "\n",
    "outlier_bounds_values = [(0.010, 0.990), (0.020, 0.980), (0.025, 0.975), (0.050, 0.950)]\n",
    "\n",
    "# trials to run per dataset\n",
    "N_TRIALS_PER_DATASET = 500\n",
    "\n",
    "# RNG\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def rchoice(values):\n",
    "    return values[int(rng.integers(low=0, high=len(values)))]\n",
    "\n",
    "# evaluate one configuration for a specific dataset\n",
    "def evaluate_config(dataset_name,\n",
    "                    steam_cutoff, olist_cutoff, sales_cutoff,\n",
    "                    feature_selection, max_features, scale_method,\n",
    "                    tag_min_count, tag_top_k,\n",
    "                    outlier_lower_q, outlier_upper_q,\n",
    "                    ohe_auto_exclude, ohe_high_card_threshold, ohe_long_text_avglen,\n",
    "                    ohe_top_k_per_col, ohe_min_freq_per_col):\n",
    "\n",
    "    outputs = prepare_data(\n",
    "        steam, olist, sales,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        feature_selection=feature_selection,\n",
    "        max_features=max_features,\n",
    "        task_type=\"classification\",\n",
    "        scale_method=scale_method,\n",
    "        steam_target_cutoff=steam_cutoff,\n",
    "        olist_target_cutoff=olist_cutoff,\n",
    "        sales_target_cutoff=sales_cutoff,\n",
    "        tag_min_count=tag_min_count,\n",
    "        tag_top_k=tag_top_k,\n",
    "        verbose=False,\n",
    "        outlier_lower_q=outlier_lower_q,\n",
    "        outlier_upper_q=outlier_upper_q,\n",
    "        ohe_auto_exclude=ohe_auto_exclude,\n",
    "        ohe_high_card_threshold=ohe_high_card_threshold,\n",
    "        ohe_long_text_avglen=ohe_long_text_avglen,\n",
    "        ohe_top_k_per_col=ohe_top_k_per_col,\n",
    "        ohe_min_freq_per_col=ohe_min_freq_per_col\n",
    "    )\n",
    "\n",
    "    if dataset_name == \"steam\":\n",
    "        X_train, _, y_train, _ = outputs[\"steam\"]\n",
    "    elif dataset_name == \"olist\":\n",
    "        X_train, _, y_train, _ = outputs[\"olist\"]\n",
    "    else:\n",
    "        X_train, _, y_train, _ = outputs[\"sales\"]\n",
    "\n",
    "    return run_cv_score(X_train, y_train)\n",
    "\n",
    "# baseline params for reference\n",
    "baseline_params = {\n",
    "    \"feature_selection\": \"none\",\n",
    "    \"max_features\": None,\n",
    "    \"scale_method\": \"standard\",\n",
    "    \"tag_min_count\": 5,\n",
    "    \"tag_top_k\": 200,\n",
    "    \"outlier_lower_q\": 0.025,\n",
    "    \"outlier_upper_q\": 0.975,\n",
    "    \"ohe_auto_exclude\": True,\n",
    "    \"ohe_high_card_threshold\": 500,\n",
    "    \"ohe_long_text_avglen\": 25,\n",
    "    \"ohe_top_k_per_col\": 100,\n",
    "    \"ohe_min_freq_per_col\": 1\n",
    "}\n",
    "\n",
    "# store all trials\n",
    "all_trials = []\n",
    "\n",
    "# helper to format ETA nicely\n",
    "def set_eta_postfix(bar_start_time, bar, completed, total):\n",
    "    if completed <= 0:\n",
    "        bar.set_postfix_str(\"ETA ~ calculating\")\n",
    "        return\n",
    "    avg_sec = (time.time() - bar_start_time) / completed\n",
    "    remain = max(total - completed, 0)\n",
    "    eta_time = datetime.now() + timedelta(seconds=avg_sec * remain)\n",
    "    bar.set_postfix_str(f\"ETA ~ {eta_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# random sweeps with per-dataset progress bar and ETA\n",
    "def random_sweep(ds_name, n_trials, overall_bar=None, overall_eta_updater=None):\n",
    "    tqdm.write(f\"\\n=== {ds_name.upper()} random sweep: {n_trials} trials ===\")\n",
    "    best = None\n",
    "    seen = set()\n",
    "    valid = 0\n",
    "    attempts = 0\n",
    "    max_attempts = n_trials * 200\n",
    "\n",
    "    dataset_start_time = time.time()\n",
    "    with tqdm(total=n_trials, desc=f\"{ds_name.upper()} sweep\", leave=True) as pbar:\n",
    "        while valid < n_trials and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "\n",
    "            # choose cutoffs (vary only the target dataset)\n",
    "            if ds_name == \"steam\":\n",
    "                steam_cut = rchoice(steam_cutoff_values)\n",
    "                olist_cut = base_olist_cutoff\n",
    "                sales_cut = base_sales_cutoff\n",
    "            elif ds_name == \"olist\":\n",
    "                steam_cut = base_steam_cutoff\n",
    "                olist_cut = rchoice(olist_cutoff_values)\n",
    "                sales_cut = base_sales_cutoff\n",
    "            else:\n",
    "                steam_cut = base_steam_cutoff\n",
    "                olist_cut = base_olist_cutoff\n",
    "                sales_cut = rchoice(sales_cutoff_values)\n",
    "\n",
    "            # choose cleaning hyperparams\n",
    "            feature_selection = rchoice(feature_selection_values)\n",
    "            max_features = rchoice(max_features_values)\n",
    "            scale_method = rchoice(scale_method_values)\n",
    "\n",
    "            ohe_auto_exclude = rchoice(ohe_auto_exclude_values)\n",
    "            ohe_top_k_per_col = rchoice(ohe_top_k_per_col_values)\n",
    "            ohe_min_freq_per_col = rchoice(ohe_min_freq_per_col_values)\n",
    "            ohe_high_card_threshold = rchoice(ohe_high_card_threshold_values)\n",
    "            ohe_long_text_avglen = rchoice(ohe_long_text_avglen_values)\n",
    "\n",
    "            tag_min_count = rchoice(tag_min_count_values)\n",
    "            tag_top_k = rchoice(tag_top_k_values)\n",
    "\n",
    "            outlier_lower_q, outlier_upper_q = rchoice(outlier_bounds_values)\n",
    "\n",
    "            # dedupe by parameter tuple\n",
    "            key = (\n",
    "                ds_name, steam_cut, olist_cut, sales_cut,\n",
    "                feature_selection, max_features, scale_method,\n",
    "                tag_min_count, tag_top_k,\n",
    "                outlier_lower_q, outlier_upper_q,\n",
    "                ohe_auto_exclude, ohe_high_card_threshold, ohe_long_text_avglen,\n",
    "                ohe_top_k_per_col, ohe_min_freq_per_col\n",
    "            )\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            m, s = evaluate_config(\n",
    "                dataset_name=ds_name,\n",
    "                steam_cutoff=steam_cut,\n",
    "                olist_cutoff=olist_cut,\n",
    "                sales_cutoff=sales_cut,\n",
    "                feature_selection=feature_selection,\n",
    "                max_features=max_features,\n",
    "                scale_method=scale_method,\n",
    "                tag_min_count=tag_min_count,\n",
    "                tag_top_k=tag_top_k,\n",
    "                outlier_lower_q=outlier_lower_q,\n",
    "                outlier_upper_q=outlier_upper_q,\n",
    "                ohe_auto_exclude=ohe_auto_exclude,\n",
    "                ohe_high_card_threshold=ohe_high_card_threshold,\n",
    "                ohe_long_text_avglen=ohe_long_text_avglen,\n",
    "                ohe_top_k_per_col=ohe_top_k_per_col,\n",
    "                ohe_min_freq_per_col=ohe_min_freq_per_col\n",
    "            )\n",
    "\n",
    "            if m is None:\n",
    "                continue  # skip collapsed labels\n",
    "\n",
    "            trial = {\n",
    "                \"dataset\": ds_name,\n",
    "                \"mean_f1\": m,\n",
    "                \"std_f1\": s,\n",
    "                \"steam_cutoff\": steam_cut,\n",
    "                \"olist_cutoff\": olist_cut,\n",
    "                \"sales_cutoff\": sales_cut,\n",
    "                \"feature_selection\": feature_selection,\n",
    "                \"max_features\": max_features,\n",
    "                \"scale_method\": scale_method,\n",
    "                \"tag_min_count\": tag_min_count,\n",
    "                \"tag_top_k\": tag_top_k,\n",
    "                \"ohe_top_k_per_col\": ohe_top_k_per_col,\n",
    "                \"ohe_min_freq_per_col\": ohe_min_freq_per_col,\n",
    "                \"outlier_lower_q\": outlier_lower_q,\n",
    "                \"outlier_upper_q\": outlier_upper_q,\n",
    "                \"ohe_auto_exclude\": ohe_auto_exclude,\n",
    "                \"ohe_high_card_threshold\": ohe_high_card_threshold,\n",
    "                \"ohe_long_text_avglen\": ohe_long_text_avglen\n",
    "            }\n",
    "            all_trials.append(trial)\n",
    "\n",
    "            valid += 1\n",
    "            pbar.update(1)\n",
    "            # show live ETA and current best\n",
    "            if (best is None) or (m > best[\"mean_f1\"]):\n",
    "                best = trial\n",
    "                tqdm.write(f\"[{ds_name}] new best F1={m:.4f} at cutoff {trial[f'{ds_name}_cutoff']:.2f}\")\n",
    "            set_eta_postfix(dataset_start_time, pbar, valid, n_trials)\n",
    "\n",
    "            if overall_bar is not None:\n",
    "                overall_bar.update(1)\n",
    "                if overall_eta_updater is not None:\n",
    "                    overall_eta_updater()\n",
    "\n",
    "        if valid < n_trials:\n",
    "            tqdm.write(f\"Note: only {valid} valid trials (others collapsed labels).\")\n",
    "\n",
    "    if best is None:\n",
    "        tqdm.write(f\"{ds_name}: no valid configurations.\")\n",
    "    else:\n",
    "        tqdm.write(f\"{ds_name}: best F1 = {best['mean_f1']:.4f} ± {best['std_f1']:.4f} \"\n",
    "                   f\"at cutoff {best[f'{ds_name}_cutoff']:.2f}\")\n",
    "\n",
    "    return best, valid\n",
    "\n",
    "# === BASELINE + SWEEPS with an overall progress bar and ETA ===\n",
    "TOTAL_BASELINE_STEPS = 3\n",
    "TOTAL_RANDOM_TRIALS = 3 * N_TRIALS_PER_DATASET\n",
    "overall_total = TOTAL_BASELINE_STEPS + TOTAL_RANDOM_TRIALS\n",
    "overall_start_time = time.time()\n",
    "\n",
    "def update_overall_eta(bar):\n",
    "    set_eta_postfix(overall_start_time, bar, bar.n, bar.total)\n",
    "\n",
    "# baseline (all base cutoffs together)\n",
    "print(\"\\n=== BASELINE (all base cutoffs) ===\")\n",
    "with tqdm(total=overall_total, desc=\"Overall\", leave=True) as overall_bar:\n",
    "    for ds_name in [\"steam\", \"olist\", \"sales\"]:\n",
    "        m, s = evaluate_config(\n",
    "            dataset_name=ds_name,\n",
    "            steam_cutoff=base_steam_cutoff,\n",
    "            olist_cutoff=base_olist_cutoff,\n",
    "            sales_cutoff=base_sales_cutoff,\n",
    "            **baseline_params\n",
    "        )\n",
    "        if m is None:\n",
    "            print(f\"{ds_name} baseline: skipped (one class)\")\n",
    "        else:\n",
    "            print(f\"{ds_name} baseline: CV F1 Macro = {m:.4f} ± {s:.4f}\")\n",
    "            all_trials.append({\n",
    "                \"dataset\": ds_name,\n",
    "                \"mean_f1\": m,\n",
    "                \"std_f1\": s,\n",
    "                \"steam_cutoff\": base_steam_cutoff,\n",
    "                \"olist_cutoff\": base_olist_cutoff,\n",
    "                \"sales_cutoff\": base_sales_cutoff,\n",
    "                **baseline_params\n",
    "            })\n",
    "        overall_bar.update(1)\n",
    "        update_overall_eta(overall_bar)\n",
    "\n",
    "    best_steam, valid_steam = random_sweep(\"steam\", N_TRIALS_PER_DATASET,\n",
    "                                           overall_bar=overall_bar,\n",
    "                                           overall_eta_updater=lambda: update_overall_eta(overall_bar))\n",
    "    # if fewer valid than planned, advance overall to keep totals aligned\n",
    "    overall_bar.update(max(N_TRIALS_PER_DATASET - valid_steam, 0))\n",
    "    update_overall_eta(overall_bar)\n",
    "\n",
    "    best_olist, valid_olist = random_sweep(\"olist\", N_TRIALS_PER_DATASET,\n",
    "                                           overall_bar=overall_bar,\n",
    "                                           overall_eta_updater=lambda: update_overall_eta(overall_bar))\n",
    "    overall_bar.update(max(N_TRIALS_PER_DATASET - valid_olist, 0))\n",
    "    update_overall_eta(overall_bar)\n",
    "\n",
    "    best_sales, valid_sales = random_sweep(\"sales\", N_TRIALS_PER_DATASET,\n",
    "                                           overall_bar=overall_bar,\n",
    "                                           overall_eta_updater=lambda: update_overall_eta(overall_bar))\n",
    "    overall_bar.update(max(N_TRIALS_PER_DATASET - valid_sales, 0))\n",
    "    update_overall_eta(overall_bar)\n",
    "\n",
    "# summary table\n",
    "def make_best_table(best_steam, best_olist, best_sales):\n",
    "    rows = []\n",
    "    for ds_name, best in [(\"steam\", best_steam), (\"olist\", best_olist), (\"sales\", best_sales)]:\n",
    "        if best is None:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"dataset\": ds_name,\n",
    "            \"best_F1\": best[\"mean_f1\"],\n",
    "            \"std_F1\": best[\"std_f1\"],\n",
    "            \"cutoff_used\": best[f\"{ds_name}_cutoff\"],\n",
    "            \"feature_selection\": best[\"feature_selection\"],\n",
    "            \"max_features\": best[\"max_features\"],\n",
    "            \"scale_method\": best[\"scale_method\"],\n",
    "            \"ohe_auto_exclude\": best[\"ohe_auto_exclude\"],\n",
    "            \"tag_min_count\": best[\"tag_min_count\"],\n",
    "            \"tag_top_k\": best[\"tag_top_k\"],\n",
    "            \"ohe_top_k_per_col\": best[\"ohe_top_k_per_col\"],\n",
    "            \"ohe_min_freq_per_col\": best[\"ohe_min_freq_per_col\"],\n",
    "            \"outlier_lower_q\": best[\"outlier_lower_q\"],\n",
    "            \"outlier_upper_q\": best[\"outlier_upper_q\"],\n",
    "            \"ohe_high_card_threshold\": best[\"ohe_high_card_threshold\"],\n",
    "            \"ohe_long_text_avglen\": best[\"ohe_long_text_avglen\"]\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "best_df = make_best_table(best_steam, best_olist, best_sales)\n",
    "print(\"\\n=== BEST HYPERPARAMETERS PER DATASET (random sweeps) ===\")\n",
    "if best_df.empty:\n",
    "    print(\"No valid results to show.\")\n",
    "else:\n",
    "    print(best_df.to_string(index=False))\n",
    "\n",
    "# optional: show top 10 trials per dataset\n",
    "trials_df = pd.DataFrame(all_trials)\n",
    "def top_k(df, ds_name, k=10):\n",
    "    sub = df[df[\"dataset\"] == ds_name].sort_values(\"mean_f1\", ascending=False).head(k)\n",
    "    cols = [\n",
    "        \"dataset\",\"mean_f1\",\"std_f1\",\n",
    "        f\"{ds_name}_cutoff\",\"feature_selection\",\"max_features\",\"scale_method\",\n",
    "        \"ohe_auto_exclude\",\"tag_min_count\",\"tag_top_k\",\"ohe_top_k_per_col\",\"ohe_min_freq_per_col\",\n",
    "        \"outlier_lower_q\",\"outlier_upper_q\",\"ohe_high_card_threshold\",\"ohe_long_text_avglen\"\n",
    "    ]\n",
    "    return sub[cols]\n",
    "\n",
    "if not trials_df.empty:\n",
    "    print(\"\\n=== TOP 10 TRIALS: STEAM ===\")\n",
    "    print(top_k(trials_df, \"steam\").to_string(index=False))\n",
    "    print(\"\\n=== TOP 10 TRIALS: OLIST ===\")\n",
    "    print(top_k(trials_df, \"olist\").to_string(index=False))\n",
    "    print(\"\\n=== TOP 10 TRIALS: SALES ===\")\n",
    "    print(top_k(trials_df, \"sales\").to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec77e4c",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
