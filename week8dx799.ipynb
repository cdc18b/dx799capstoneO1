{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f3ed52",
   "metadata": {},
   "source": [
    "# Week 8 - KNN using Euclidean distance, Manhattan distance, Minkowski distance, and Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a1e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: start downloads\n",
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.238 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.22 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.193 sec\n",
      "main: downloads finished\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 1000001 of 41154794 rows in 5.943 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(1000001, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: taking all rows\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (113425, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019.csv with shape (55792, 23)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: taking all rows\n",
      "vg2019: done shape=(55792, 23)\n",
      "main: load all done in 16.979 sec (00:00:16)\n",
      "main: optimize steam dtypes\n",
      "optimize_dtypes: start\n",
      "optimize_dtypes: memory 561.25 MB -> 218.92 MB\n",
      "main: optimize olist dtypes\n",
      "optimize_dtypes: start\n",
      "optimize_dtypes: memory 116.28 MB -> 57.26 MB\n",
      "main: optimize vg2019 dtypes\n",
      "optimize_dtypes: start\n",
      "optimize_dtypes: memory 40.47 MB -> 24.57 MB\n",
      "final: shapes summary\n",
      "final: steam shape = (1000001, 24)\n",
      "final: olist shape = (113425, 38)\n",
      "final: sales shape = (55792, 23)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Standard Libraries\n",
    "# =============================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# progress / kaggle\n",
    "from tqdm.auto import tqdm\n",
    "import kagglehub\n",
    "from IPython.display import display\n",
    "\n",
    "# =============================\n",
    "# Data Science Libraries\n",
    "# =============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "# =============================\n",
    "# Visualization\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn\n",
    "# =============================\n",
    "from sklearn import clone\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    ParameterGrid,\n",
    "    ParameterSampler,\n",
    "    KFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "import sklearn.model_selection._search as sk_search\n",
    "import sklearn.model_selection._validation as sk_validation\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OrdinalEncoder,\n",
    "    MultiLabelBinarizer,\n",
    "    Normalizer,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    get_scorer,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SequentialFeatureSelector,\n",
    "    f_regression,\n",
    "    SelectKBest,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    "    RidgeClassifier,\n",
    "    LogisticRegression,\n",
    "    RidgeCV,\n",
    "    LassoCV,\n",
    "    ElasticNetCV,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# extra joblib tools\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "# =============================\n",
    "# Global Settings\n",
    "# =============================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "random_state = 42\n",
    "N_ROWS = 1_000_000\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")  # no scientific notation\n",
    "\n",
    "# =============================\n",
    "# Small Helpers\n",
    "# =============================\n",
    "def is_sparse_dtype(dtype):\n",
    "    return isinstance(dtype, pd.SparseDtype)\n",
    "\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read csv\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path is not None else None\n",
    "    if full_path is not None and os.path.exists(full_path):\n",
    "        return pd.read_csv(full_path, **kwargs)\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files\n",
    "    if folder_path is None or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return [f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")]\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # simple sample\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    if np.any(counts < 2):\n",
    "        print(\"stratified_sample: some classes < 2, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array, y_array, test_size=test_size, stratify=y_array, random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[test_idx].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # make date/time columns into datetime\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            try:\n",
    "                df[col_name] = pd.to_datetime(df[col_name].astype(\"string\"), errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def optimize_dtypes(data_frame, convert_categoricals=True, category_threshold=0.2):\n",
    "    # shrink memory\n",
    "    print(\"optimize_dtypes: start\")\n",
    "    if data_frame is None:\n",
    "        print(\"optimize_dtypes: skip because data_frame is None\")\n",
    "        return None\n",
    "\n",
    "    start_bytes = int(data_frame.memory_usage(deep=True).sum())\n",
    "\n",
    "    # numeric downsizing\n",
    "    for col in data_frame.select_dtypes(include=[np.floating]).columns:\n",
    "        data_frame[col] = pd.to_numeric(data_frame[col], downcast=\"float\")\n",
    "    for col in data_frame.select_dtypes(include=[np.integer]).columns:\n",
    "        data_frame[col] = pd.to_numeric(data_frame[col], downcast=\"integer\")\n",
    "\n",
    "    # object -> category (skip date/time-like names)\n",
    "    if convert_categoricals:\n",
    "        object_columns = [\n",
    "            c for c in data_frame.select_dtypes(include=[\"object\"]).columns\n",
    "            if (\"date\" not in c.lower() and \"time\" not in c.lower())\n",
    "        ]\n",
    "        total_rows = max(len(data_frame), 1)\n",
    "\n",
    "        def is_unhashable_obj(x):\n",
    "            return isinstance(x, (list, dict, set))\n",
    "\n",
    "        for col in object_columns:\n",
    "            series_now = data_frame[col]\n",
    "            try:\n",
    "                if series_now.apply(is_unhashable_obj).any():\n",
    "                    continue\n",
    "                unique_ratio = series_now.nunique(dropna=False) / total_rows\n",
    "                if unique_ratio <= category_threshold:\n",
    "                    data_frame[col] = data_frame[col].astype(\"category\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    end_bytes = int(data_frame.memory_usage(deep=True).sum())\n",
    "    print(f\"optimize_dtypes: memory {round(start_bytes / (1024**2), 2)} MB -> {round(end_bytes / (1024**2), 2)} MB\")\n",
    "    return data_frame\n",
    "\n",
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(f\"steam: shapes games={None if games is None else games.shape}, users={None if users is None else users.shape}, recs={None if recommendations is None else recommendations.shape}, meta={None if metadata is None else metadata.shape}\")\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if metadata is not None and games is not None and \"app_id\" in metadata.columns and \"app_id\" in games.columns:\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\")\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\")\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\")\n",
    "\n",
    "        # make dates now\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\")\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\")\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\")\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\")\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\")\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\")\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\")\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\")\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\")\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # make dates now\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    target_csv = \"vgsales-12-4-2019.csv\" if \"vgsales-12-4-2019.csv\" in csv_files else (csv_files[0] if csv_files else None)\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    sales = pd.read_csv(full_path, low_memory=False)\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n",
    "\n",
    "# =============================\n",
    "# Download Paths\n",
    "# =============================\n",
    "print(\"main: start downloads\")\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "print(\"main: downloads finished\")\n",
    "\n",
    "# =============================\n",
    "# Load All\n",
    "# =============================\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# =============================\n",
    "# Optimize Dtypes (after making dates)\n",
    "# =============================\n",
    "if steam is not None:\n",
    "    print(\"main: optimize steam dtypes\")\n",
    "    steam = optimize_dtypes(steam, convert_categoricals=True, category_threshold=0.2)\n",
    "\n",
    "if olist is not None:\n",
    "    print(\"main: optimize olist dtypes\")\n",
    "    olist = optimize_dtypes(olist, convert_categoricals=True, category_threshold=0.2)\n",
    "\n",
    "if sales is not None:\n",
    "    print(\"main: optimize vg2019 dtypes\")\n",
    "    sales = optimize_dtypes(sales, convert_categoricals=True, category_threshold=0.2)\n",
    "\n",
    "# =============================\n",
    "# Final Shapes\n",
    "# =============================\n",
    "print(\"final: shapes summary\")\n",
    "print(f\"final: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"final: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"final: sales shape = {None if sales is None else sales.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d28bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep: steam\n",
      "prep: steam build sparse tag matrix\n",
      "prep: olist\n",
      "prep: sales\n",
      "steam cutoff_date: 2022-02-17 00:00:00\n",
      "steam apps in test: 3212 of 22777\n",
      "steam train rows: 955509 | test rows: 44492\n",
      "steam y_train stats: {'n': 955509, 'mean': 86.35820698706135, 'std': 10.978047160242102, 'min': 0.0, 'max': 100.0}\n",
      "steam y_test stats: {'n': 44492, 'mean': 81.25285444574305, 'std': 15.442551141752842, 'min': 0.0, 'max': 100.0}\n",
      "target_olist train stats: {'n': 89954, 'mean': 4.058243751525879, 'std': 0.8512095808982849, 'min': 1.0, 'max': 5.0}\n",
      "target_olist test stats: {'n': 22489, 'mean': 4.053517818450928, 'std': 0.8607538342475891, 'min': 1.0, 'max': 5.0}\n",
      "target_sales train stats: {'n': 5228, 'mean': 7.202467441558838, 'std': 1.45883047580719, 'min': 1.0, 'max': 10.0}\n",
      "target_sales test stats: {'n': 1308, 'mean': 7.258639812469482, 'std': 1.4334877729415894, 'min': 1.5, 'max': 10.0}\n",
      "scaled columns: 12 with minmax\n",
      "scaled columns: 25 with minmax\n",
      "scaled columns: 3 with minmax\n",
      "[debug] steam missing: ['poly_price_final_log1p^2', 'poly_price_original_log1p', 'tag_early access', 'tag_great soundtrack', 'poly_days_since_release', 'poly_hours_log1p^2', 'tag_2d', 'tag_massively multiplayer', 'tag_free to play', 'tag_cute', 'tag_action rpg', 'poly_price_original_log1p^2', 'tag_first-person', 'tag_fast-paced', 'poly_title_len price_final_log1p', 'poly_days_since_release review_year', 'poly_products_log1p reviews_log1p', 'tag_mmorpg', 'tag_puzzle', 'tag_pvp', 'poly_desc_len hours_log1p', 'poly_hours_log1p', 'tag_management', 'tag_memes', 'tag_relaxing', 'tag_visual novel', 'tag_sexual content', 'tag_difficult', 'tag_emotional']\n",
      "[debug] olist missing: ['poly_to_customer_h', 'poly_to_carrier_h', 'poly_customer_zip_code_prefix', 'product_category_watches_gifts', 'product_category_computers_accessories', 'product_category_telephony', 'product_category_home_confort', 'poly_to_customer_h est_delivery_h', 'poly_to_customer_h price', 'seller_state_RS', 'seller_state_PE', 'poly_payment_value_per_payment approval_delay_h', 'product_category_audio', 'poly_product_description_lenght product_height_cm', 'product_name_lenght', 'product_category_stationery', 'product_category_electronics', 'seller_state_MA', 'product_category_Unknown', 'product_category_baby']\n",
      "[debug] sales missing: ['poly_Developer_freq', 'poly_Year^2', 'Decade_1970', 'poly_Developer_freq^2', 'poly_Publisher_freq', 'poly_Year Publisher_freq', 'Decade_1980', 'Genre_Puzzle', 'poly_Year', 'poly_Year Developer_freq', 'poly_Publisher_freq^2', 'Genre_Adventure']\n",
      "\n",
      "steam selected features (train, test): 30 30\n",
      "olist selected features (train, test): 30 30\n",
      "sales selected features (train, test): 30 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_all(\n",
    "    steam, olist, sales,\n",
    "    top_k_tags=200, max_total_features=400,\n",
    "    test_size=0.2, random_state=42,\n",
    "    balance_method=\"auto\",\n",
    "    min_class_ratio=0.5,\n",
    "    feature_select_method=\"none\",\n",
    "    feature_select_k=100,\n",
    "    task_type=\"classification\",\n",
    "    scale_method=\"none\",\n",
    "    poly_degree=2,\n",
    "    poly_interaction_only=False,\n",
    "    poly_include_bias=False,\n",
    "    poly_feature_limit=25\n",
    "):\n",
    "    def is_sparse_dtype(dtype):\n",
    "        return isinstance(dtype, pd.SparseDtype)\n",
    "\n",
    "    is_classification = str(task_type).lower().startswith(\"c\")\n",
    "\n",
    "    # forward-selected feature lists you provided\n",
    "    best_forward_features_raw = {\n",
    "        \"steam\": [\n",
    "            \"poly_price_final_log1p^2\", \"mac\", \"poly_price_original_log1p\",\n",
    "            \"tag_'early access'\", \"tag_'great soundtrack'\", \"poly_days_since_release\",\n",
    "            \"poly_hours_log1p^2\", \"tag_'2d'\", \"tag_'massively multiplayer'\",\n",
    "            \"tag_'free to play'\", \"tag_'cute'\", \"tag_'action rpg'\",\n",
    "            \"poly_price_original_log1p^2\", \"tag_'first-person'\", \"tag_'fast-paced'\",\n",
    "            \"poly_title_len price_final_log1p\", \"poly_days_since_release review_year\",\n",
    "            \"poly_products_log1p reviews_log1p\", \"tag_'mmorpg'\", \"tag_'puzzle'\",\n",
    "            \"tag_'pvp'\", \"poly_desc_len hours_log1p\", \"poly_hours_log1p\",\n",
    "            \"tag_'management'\", \"tag_'memes'\", \"tag_'relaxing'\", \"tag_'visual novel'\",\n",
    "            \"tag_'sexual content'\", \"tag_'difficult'\", \"tag_'emotional'\"\n",
    "        ],\n",
    "        \"olist\": [\n",
    "            \"poly_to_customer_h\", \"order_status_delivered\", \"order_status_shipped\",\n",
    "            \"poly_to_carrier_h\", \"seller_state_SP\", \"order_item_id\",\n",
    "            \"product_category_bed_bath_table\", \"product_category_office_furniture\",\n",
    "            \"product_category_furniture_decor\", \"poly_customer_zip_code_prefix\",\n",
    "            \"product_category_watches_gifts\", \"product_category_computers_accessories\",\n",
    "            \"product_category_telephony\", \"product_category_books_general_interest\",\n",
    "            \"product_category_home_confort\", \"poly_to_customer_h est_delivery_h\",\n",
    "            \"poly_to_customer_h price\", \"order_status_processing\", \"seller_state_RS\",\n",
    "            \"seller_state_PE\", \"poly_payment_value_per_payment approval_delay_h\",\n",
    "            \"product_category_audio\", \"poly_product_description_lenght product_height_cm\",\n",
    "            \"product_name_lenght\", \"product_category_stationery\",\n",
    "            \"product_category_luggage_accessories\", \"product_category_electronics\",\n",
    "            \"seller_state_MA\", \"product_category_Unknown\", \"product_category_baby\"\n",
    "        ],\n",
    "        \"sales\": [\n",
    "            \"poly_Developer_freq\", \"Genre_Action\", \"ESRB_Rating_M\", \"poly_Year^2\",\n",
    "            \"Decade_1970\", \"Platform_Family_PC\", \"poly_Developer_freq^2\",\n",
    "            \"Platform_Family_Nintendo\", \"poly_Publisher_freq\",\n",
    "            \"poly_Year Publisher_freq\", \"is_remaster\", \"Genre_Party\", \"Genre_Misc\",\n",
    "            \"Genre_Simulation\", \"is_portable\", \"Genre_Role-Playing\", \"Decade_1980\",\n",
    "            \"Genre_Strategy\", \"Genre_Fighting\", \"Genre_Puzzle\", \"poly_Year\",\n",
    "            \"Decade_<NA>\", \"poly_Year Developer_freq\", \"poly_Publisher_freq^2\",\n",
    "            \"Platform_Family_PlayStation\", \"Genre_Board Game\", \"Genre_Racing\",\n",
    "            \"Genre_Adventure\", \"Genre_Shooter\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # normalize names like \"tag_'great soundtrack'\" -> \"tag_great soundtrack\"\n",
    "    def _normalize_forward_name(name):\n",
    "        s = str(name).strip()\n",
    "        if s.startswith(\"tag_\"):\n",
    "            rest = s[4:].strip()\n",
    "            if (rest.startswith(\"'\") and rest.endswith(\"'\")) or (rest.startswith('\"') and rest.endswith('\"')):\n",
    "                rest = rest[1:-1]\n",
    "            return \"tag_\" + rest.lower()\n",
    "        return s\n",
    "\n",
    "    best_forward_features = {\n",
    "        k: [_normalize_forward_name(n) for n in v]\n",
    "        for k, v in best_forward_features_raw.items()\n",
    "    }\n",
    "\n",
    "    # make poly interaction names order-agnostic\n",
    "    def canonical_poly_name(name):\n",
    "        s = str(name)\n",
    "        if not s.startswith(\"poly_\"):\n",
    "            return s\n",
    "        body = s[5:]\n",
    "        if \"^\" in body and \" \" not in body:\n",
    "            return s  # keep squares like \"poly_x^2\"\n",
    "        tokens = body.split()\n",
    "        tokens = sorted(tokens)\n",
    "        return \"poly_\" + \" \".join(tokens)\n",
    "\n",
    "    def build_poly_canonical_map(cols):\n",
    "        col_map = {}\n",
    "        for c in cols:\n",
    "            if c.startswith(\"poly_\"):\n",
    "                col_map[canonical_poly_name(c)] = c\n",
    "            else:\n",
    "                col_map[c] = c\n",
    "        return col_map\n",
    "\n",
    "    # small debug print for missing columns (uses canonical poly names)\n",
    "    def debug_forward_missing(X, dataset_name, best_forward_features):\n",
    "        expect = best_forward_features.get(dataset_name, [])\n",
    "        col_map = build_poly_canonical_map(X.columns)\n",
    "        missing = [f for f in expect if canonical_poly_name(f) not in col_map]\n",
    "        if missing:\n",
    "            print(f\"[debug] {dataset_name} missing:\", missing)\n",
    "        return missing\n",
    "\n",
    "    def drop_known_leaks_from_features(X_in, y_in=None):\n",
    "        leak_names = {\"y_is_4_plus\", \"delivered_late\", \"target\", \"label\",\n",
    "                      \"target_olist\", \"target_sales\", \"target_steam\"}\n",
    "        to_drop = [c for c in X_in.columns if (c in leak_names) or c.lower().startswith(\"target\") or c.lower().startswith(\"label\")]\n",
    "        X_in = X_in.drop(columns=to_drop, errors=\"ignore\")\n",
    "        if is_classification and (y_in is not None):\n",
    "            y_series = pd.Series(y_in).reset_index(drop=True)\n",
    "            for c in list(X_in.columns):\n",
    "                xc = pd.Series(X_in[c]).reset_index(drop=True)\n",
    "                try:\n",
    "                    ux = set(pd.unique(xc.dropna()))\n",
    "                    uy = set(pd.unique(y_series.dropna()))\n",
    "                    if ux <= {0, 1} and uy <= {0, 1}:\n",
    "                        if (xc.astype(\"int8\") == y_series.astype(\"int8\")).all():\n",
    "                            X_in = X_in.drop(columns=[c])\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return X_in\n",
    "\n",
    "    def resample_binary(Xb, yb, method=\"oversample\", random_state=42):\n",
    "        counts = yb.value_counts(dropna=False)\n",
    "        if counts.shape[0] != 2:\n",
    "            return Xb, yb\n",
    "        majority_class = counts.idxmax()\n",
    "        minority_class = counts.idxmin()\n",
    "        majority_idx = yb[yb == majority_class].index.values\n",
    "        minority_idx = yb[yb == minority_class].index.values\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        if method == \"undersample\":\n",
    "            keep_majority = rng.choice(majority_idx, size=len(minority_idx), replace=False)\n",
    "            new_index = np.concatenate([minority_idx, keep_majority])\n",
    "        else:\n",
    "            need = int(len(majority_idx) - len(minority_idx))\n",
    "            need = max(0, need)\n",
    "            add_minority = rng.choice(minority_idx, size=need, replace=True)\n",
    "            new_index = np.concatenate([majority_idx, minority_idx, add_minority])\n",
    "        rng.shuffle(new_index)\n",
    "        Xb2 = Xb.loc[new_index].reset_index(drop=True)\n",
    "        yb2 = yb.loc[new_index].reset_index(drop=True)\n",
    "        return Xb2, yb2\n",
    "\n",
    "    def split_and_balance(df_in, target_col, balance_method, min_class_ratio, random_state):\n",
    "        X_in = df_in.drop(columns=[target_col]).copy()\n",
    "        y_in = df_in[target_col].copy()\n",
    "        X_in = drop_known_leaks_from_features(X_in, y_in)\n",
    "\n",
    "        dense_num_cols = [c for c in X_in.select_dtypes(include=[np.number]).columns\n",
    "                          if not is_sparse_dtype(X_in[c].dtype)]\n",
    "        for c in dense_num_cols:\n",
    "            X_in[c] = pd.to_numeric(X_in[c], errors=\"coerce\").fillna(pd.to_numeric(X_in[c], errors=\"coerce\").median())\n",
    "        obj_cols = X_in.select_dtypes(include=[\"object\"]).columns\n",
    "        for c in obj_cols:\n",
    "            X_in[c] = X_in[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        if is_classification:\n",
    "            if pd.Series(y_in).nunique() != 2:\n",
    "                raise ValueError(f\"{target_col}: for classification, target must have 2 classes.\")\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_in, y_in.astype(int), test_size=test_size, random_state=random_state, stratify=y_in\n",
    "            )\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                X_in, y_in, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "\n",
    "        if is_classification:\n",
    "            vc = y_tr.value_counts()\n",
    "            ratio = float(vc.min()) / float(vc.max()) if vc.max() > 0 else 1.0\n",
    "            if balance_method != \"none\" and ratio < min_class_ratio:\n",
    "                method = (\n",
    "                    \"oversample\" if balance_method == \"auto\" and len(X_tr) <= 200000\n",
    "                    else (\"undersample\" if balance_method == \"auto\" else balance_method)\n",
    "                )\n",
    "                X_tr, y_tr = resample_binary(X_tr, y_tr, method=method, random_state=random_state)\n",
    "            print(f\"{target_col} train class counts:\", y_tr.value_counts().to_dict())\n",
    "            print(f\"{target_col} test class counts:\", y_te.value_counts().to_dict())\n",
    "        else:\n",
    "            def sstats(s):\n",
    "                return {\"n\": int(s.shape[0]),\n",
    "                        \"mean\": float(np.nanmean(s)),\n",
    "                        \"std\": float(np.nanstd(s)),\n",
    "                        \"min\": float(np.nanmin(s)),\n",
    "                        \"max\": float(np.nanmax(s))}\n",
    "            print(f\"{target_col} train stats:\", sstats(y_tr))\n",
    "            print(f\"{target_col} test stats:\", sstats(y_te))\n",
    "        return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "    def apply_feature_selection(X_tr, y_tr, X_te, method, k, random_state, dataset_name=None):\n",
    "        if method == \"none\":\n",
    "            return X_tr, X_te\n",
    "\n",
    "        if method in {\"forward_best\", \"forward\", \"best_forward\"}:\n",
    "            feature_list = best_forward_features.get(dataset_name, [])\n",
    "            if not feature_list:\n",
    "                print(f\"forward_best: no saved list for {dataset_name}, using all features.\")\n",
    "                return X_tr, X_te\n",
    "\n",
    "            col_map = build_poly_canonical_map(X_tr.columns)\n",
    "            kept_cols = []\n",
    "            for want in feature_list:\n",
    "                key = canonical_poly_name(want)\n",
    "                if key in col_map:\n",
    "                    kept_cols.append(col_map[key])\n",
    "\n",
    "            missing = [f for f in feature_list if canonical_poly_name(f) not in col_map]\n",
    "            if missing:\n",
    "                print(f\"forward_best: {dataset_name} missing {len(missing)} of {len(feature_list)} saved features.\")\n",
    "\n",
    "            if not kept_cols:\n",
    "                print(f\"forward_best: none of the saved features found for {dataset_name}, using all features.\")\n",
    "                return X_tr, X_te\n",
    "\n",
    "            if isinstance(k, int) and k > 0:\n",
    "                kept_cols = kept_cols[: min(k, len(kept_cols))]\n",
    "\n",
    "            return X_tr[kept_cols].copy(), X_te[kept_cols].copy()\n",
    "\n",
    "        n_features = X_tr.shape[1]\n",
    "        if n_features <= 1 or k >= n_features:\n",
    "            return X_tr, X_te\n",
    "        k = int(max(1, min(k, n_features)))\n",
    "        try:\n",
    "            if method == \"mi\":\n",
    "                if is_classification:\n",
    "                    sel = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "                else:\n",
    "                    sel = SelectKBest(score_func=f_regression, k=k)\n",
    "                sel.fit(X_tr, y_tr)\n",
    "                kept_cols = X_tr.columns[sel.get_support()].tolist()\n",
    "            elif method == \"rf\":\n",
    "                if is_classification:\n",
    "                    rf = RandomForestClassifier(\n",
    "                        n_estimators=300, random_state=random_state, n_jobs=-1,\n",
    "                        class_weight=\"balanced_subsample\"\n",
    "                    )\n",
    "                else:\n",
    "                    rf = RandomForestRegressor(\n",
    "                        n_estimators=300, random_state=random_state, n_jobs=-1\n",
    "                    )\n",
    "                rf.fit(X_tr, y_tr)\n",
    "                order = np.argsort(rf.feature_importances_)[::-1][:k]\n",
    "                kept_cols = X_tr.columns[order].tolist()\n",
    "            else:\n",
    "                print(\"feature selection: unknown method, skipping\")\n",
    "                return X_tr, X_te\n",
    "            return X_tr[kept_cols].copy(), X_te[kept_cols].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"feature selection error ({method}): {e}. using all features.\")\n",
    "            return X_tr, X_te\n",
    "\n",
    "    # pick continuous columns, but always keep forced bases (even if low variance)\n",
    "    def pick_continuous_columns(X, max_cols, must_have=None):\n",
    "        sample = X.iloc[: min(10000, len(X))].copy()\n",
    "        num_cols = sample.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        def ok(col):\n",
    "            return (not str(X[col].dtype).startswith(\"uint8\")) and (sample[col].nunique(dropna=False) > 2)\n",
    "\n",
    "        must_have = [c for c in (must_have or []) if c in X.columns and not str(X[c].dtype).startswith(\"uint8\")]\n",
    "        rest = [c for c in num_cols if ok(c) and c not in must_have]\n",
    "        if rest:\n",
    "            var_series = sample[rest].var().sort_values(ascending=False)\n",
    "            rest_sorted = var_series.index.tolist()\n",
    "        else:\n",
    "            rest_sorted = []\n",
    "        cols = (must_have + [c for c in rest_sorted if c not in must_have])[: max_cols]\n",
    "        return cols\n",
    "\n",
    "    def apply_scaling_and_poly(X_tr, X_te):\n",
    "        # bases needed by your forward polys on Steam\n",
    "        steam_poly_must_have = [\n",
    "            \"price_final_log1p\", \"price_original_log1p\",\n",
    "            \"days_since_release\", \"review_year\",\n",
    "            \"hours_log1p\", \"products_log1p\", \"reviews_log1p\",\n",
    "            \"title_len\", \"desc_len\"\n",
    "        ]\n",
    "        cont_cols = pick_continuous_columns(X_tr, poly_feature_limit, must_have=steam_poly_must_have)\n",
    "        X_tr = X_tr.copy()\n",
    "        X_te = X_te.copy()\n",
    "\n",
    "        do_scale = (scale_method in {\"standard\", \"minmax\"}) and bool(cont_cols)\n",
    "        do_poly = (poly_degree is not None) and (int(poly_degree) >= 2 or bool(poly_interaction_only)) and bool(cont_cols)\n",
    "\n",
    "        if not cont_cols:\n",
    "            return X_tr, X_te\n",
    "\n",
    "        tr_cont = X_tr[cont_cols].astype(\"float32\").values\n",
    "        te_cont = X_te[cont_cols].astype(\"float32\").values\n",
    "\n",
    "        if do_scale:\n",
    "            if scale_method == \"standard\":\n",
    "                scaler = StandardScaler()\n",
    "            elif scale_method == \"minmax\":\n",
    "                scaler = MinMaxScaler()\n",
    "            else:\n",
    "                scaler = None\n",
    "            if scaler is not None:\n",
    "                tr_cont = scaler.fit_transform(tr_cont).astype(\"float32\")\n",
    "                te_cont = scaler.transform(te_cont).astype(\"float32\")\n",
    "                print(f\"scaled columns: {len(cont_cols)} with {scale_method}\")\n",
    "\n",
    "        if do_poly:\n",
    "            poly = PolynomialFeatures(\n",
    "                degree=int(poly_degree),\n",
    "                include_bias=bool(poly_include_bias),\n",
    "                interaction_only=bool(poly_interaction_only)\n",
    "            )\n",
    "            tr_poly = poly.fit_transform(tr_cont).astype(\"float32\")\n",
    "            te_poly = poly.transform(te_cont).astype(\"float32\")\n",
    "            poly_names = [f\"poly_{n}\" for n in poly.get_feature_names_out(cont_cols)]\n",
    "            tr_poly_df = pd.DataFrame(tr_poly, columns=poly_names, index=X_tr.index)\n",
    "            te_poly_df = pd.DataFrame(te_poly, columns=poly_names, index=X_te.index)\n",
    "            X_tr = X_tr.drop(columns=cont_cols).join(tr_poly_df)\n",
    "            X_te = X_te.drop(columns=cont_cols).join(te_poly_df)\n",
    "            print(f\"polynomial features added: {len(poly_names)} (from {len(cont_cols)} columns)\")\n",
    "            return X_tr, X_te\n",
    "\n",
    "        if do_scale:\n",
    "            tr_scaled_df = pd.DataFrame(tr_cont, columns=cont_cols, index=X_tr.index).astype(\"float32\")\n",
    "            te_scaled_df = pd.DataFrame(te_cont, columns=cont_cols, index=X_te.index).astype(\"float32\")\n",
    "            X_tr = X_tr.drop(columns=cont_cols).join(tr_scaled_df)\n",
    "            X_te = X_te.drop(columns=cont_cols).join(te_scaled_df)\n",
    "\n",
    "        return X_tr, X_te\n",
    "\n",
    "    def prepare_steam_df(steam_in):\n",
    "        print(\"prep: steam\")\n",
    "        df = steam_in.copy()\n",
    "        for col in [\"title\", \"description\", \"tags\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(\"string\")\n",
    "\n",
    "        def safe_to_datetime(series):\n",
    "            return pd.to_datetime(series.astype(\"string\"), errors=\"coerce\")\n",
    "        for c in [\"date\", \"date_release\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = safe_to_datetime(df[c])\n",
    "            else:\n",
    "                df[c] = pd.NaT\n",
    "\n",
    "        df[\"days_since_release\"] = (df[\"date\"] - df[\"date_release\"]).dt.days\n",
    "        df[\"days_since_release\"] = df[\"days_since_release\"].clip(lower=0).fillna(0).astype(\"int32\")\n",
    "        df[\"review_year\"] = df[\"date\"].dt.year.astype(\"float64\")\n",
    "        df[\"review_month\"] = df[\"date\"].dt.month.astype(\"float64\")\n",
    "        df[\"review_dow\"] = df[\"date\"].dt.dayofweek.astype(\"float64\")\n",
    "        df[\"review_year\"] = df[\"review_year\"].fillna(-1).astype(\"int16\")\n",
    "        df[\"review_month\"] = df[\"review_month\"].fillna(-1).astype(\"int8\")\n",
    "        df[\"review_dow\"] = df[\"review_dow\"].fillna(-1).astype(\"int8\")\n",
    "\n",
    "        df[\"title_len\"] = df[\"title\"].astype(\"string\").str.len().fillna(0).astype(\"int32\")\n",
    "        df[\"desc_len\"]  = df[\"description\"].astype(\"string\").str.len().fillna(0).astype(\"int32\")\n",
    "\n",
    "        for col in [\"hours\",\"products\",\"reviews\",\"price_final\",\"price_original\"]:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df[col + \"_log1p\"] = np.log1p(df[col])\n",
    "\n",
    "        df[\"is_free\"] = (df[\"price_final\"] == 0).astype(\"int8\")\n",
    "        df[\"discount_ratio\"] = np.where(\n",
    "            pd.to_numeric(df[\"price_original\"], errors=\"coerce\") > 0,\n",
    "            1.0 - (pd.to_numeric(df[\"price_final\"], errors=\"coerce\") /\n",
    "                   pd.to_numeric(df[\"price_original\"], errors=\"coerce\")),\n",
    "            0.0\n",
    "        )\n",
    "        df[\"discount_ratio\"] = pd.Series(df[\"discount_ratio\"]).clip(0, 1).fillna(0.0)\n",
    "\n",
    "        for b in [\"win\",\"mac\",\"linux\",\"steam_deck\"]:\n",
    "            if b in df.columns:\n",
    "                df[b] = (df[b] == True).astype(\"int8\")\n",
    "            else:\n",
    "                df[b] = 0\n",
    "\n",
    "        pos = pd.to_numeric(df.get(\"positive_ratio\"), errors=\"coerce\")\n",
    "        if is_classification:\n",
    "            y = (pos >= 80).astype(\"Int64\")\n",
    "        else:\n",
    "            y = pos.round().astype(\"Int64\")\n",
    "\n",
    "        keep_dense = [\n",
    "            \"win\",\"mac\",\"linux\",\"steam_deck\",\n",
    "            \"days_since_release\",\"review_year\",\"review_month\",\"review_dow\",\n",
    "            \"title_len\",\"desc_len\",\n",
    "            \"hours_log1p\",\"products_log1p\",\"reviews_log1p\",\n",
    "            \"price_final_log1p\",\"price_original_log1p\",\n",
    "            \"discount_ratio\",\"is_free\"\n",
    "        ]\n",
    "        X = df[keep_dense].copy()\n",
    "\n",
    "        base_cols_count = X.shape[1]\n",
    "        allowed_tag_cols = max(0, min(top_k_tags, max_total_features - base_cols_count))\n",
    "        if allowed_tag_cols > 0 and \"tags\" in df.columns:\n",
    "            print(\"prep: steam build sparse tag matrix\")\n",
    "            tags_clean = (\n",
    "                df[\"tags\"].astype(\"string\").fillna(\"\").str.lower()\n",
    "                  .str.replace(r\"[\\[\\]\\\"]\", \"\", regex=True)\n",
    "                  .str.replace(\";\", \",\").str.replace(\"/\", \",\")\n",
    "            )\n",
    "            vec = CountVectorizer(\n",
    "                tokenizer=lambda s: [t.strip() for t in s.split(\",\") if t.strip()],\n",
    "                lowercase=False, binary=True, max_features=allowed_tag_cols\n",
    "            )\n",
    "            tag_sparse = vec.fit_transform(tags_clean.values)\n",
    "            tag_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "                tag_sparse,\n",
    "                columns=[f\"tag_{t}\" for t in vec.get_feature_names_out()],\n",
    "                index=df.index\n",
    "            ).astype(pd.SparseDtype(\"uint8\", 0))\n",
    "            X = pd.concat([X, tag_df], axis=1)\n",
    "\n",
    "            # force-create chosen tags even if not in top-K\n",
    "            force_tags = [\n",
    "                \"early access\",\"great soundtrack\",\"2d\",\"massively multiplayer\",\"free to play\",\"cute\",\n",
    "                \"action rpg\",\"first-person\",\"fast-paced\",\"mmorpg\",\"puzzle\",\"pvp\",\"management\",\"memes\",\n",
    "                \"relaxing\",\"visual novel\",\"sexual content\",\"difficult\",\"emotional\"\n",
    "            ]\n",
    "            def has_tag(series, t):\n",
    "                pattern = rf\"(?:^|,)\\s*{re.escape(t)}\\s*(?:,|$)\"\n",
    "                return series.str.contains(pattern, regex=True)\n",
    "\n",
    "            for t in force_tags:\n",
    "                col = f\"tag_{t}\"\n",
    "                if col not in X.columns:\n",
    "                    X[col] = has_tag(tags_clean, t).astype(\"uint8\")\n",
    "\n",
    "            # alias variants -> OR into canonical columns\n",
    "            tag_aliases = {\n",
    "                \"first-person\": [\"first person\"],\n",
    "                \"action rpg\": [\"action-rpg\"],\n",
    "                \"mmorpg\": [\"mmo rpg\", \"mmo-rpg\"]\n",
    "            }\n",
    "            for t, alts in tag_aliases.items():\n",
    "                col = f\"tag_{t}\"\n",
    "                if col not in X.columns:\n",
    "                    X[col] = has_tag(tags_clean, t).astype(\"uint8\")\n",
    "                for a in alts:\n",
    "                    alias_hits = has_tag(tags_clean, a).astype(\"uint8\")\n",
    "                    if alias_hits.any():\n",
    "                        X[col] = (X[col].astype(\"uint8\") | alias_hits).astype(\"uint8\")\n",
    "\n",
    "        if any(is_sparse_dtype(X[c].dtype) for c in X.columns):\n",
    "            for c in X.columns:\n",
    "                if is_sparse_dtype(X[c].dtype):\n",
    "                    X[c] = X[c].sparse.to_dense().astype(\"uint8\")\n",
    "\n",
    "        dense_num_cols = [c for c in X.select_dtypes(include=[np.number]).columns\n",
    "                          if not is_sparse_dtype(X[c].dtype)]\n",
    "        for c in dense_num_cols:\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(pd.to_numeric(X[c], errors=\"coerce\").median())\n",
    "\n",
    "        raw = df[[\"date\",\"app_id\"]].copy()\n",
    "        out_df = X.copy()\n",
    "        out_df[\"target_steam\"] = y\n",
    "        out_df = out_df[out_df[\"target_steam\"].notna()].copy()\n",
    "        if is_classification:\n",
    "            out_df[\"target_steam\"] = out_df[\"target_steam\"].astype(\"int8\")\n",
    "        else:\n",
    "            out_df[\"target_steam\"] = out_df[\"target_steam\"].astype(\"int16\")\n",
    "        return out_df, raw\n",
    "\n",
    "    def prepare_olist_df(olist_in):\n",
    "        print(\"prep: olist\")\n",
    "        o = olist_in.copy()\n",
    "\n",
    "        def safe_to_datetime(series):\n",
    "            return pd.to_datetime(series.astype(\"string\"), errors=\"coerce\")\n",
    "        date_cols = [\n",
    "            \"order_purchase_timestamp\",\"order_approved_at\",\"order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\",\"order_estimated_delivery_date\",\"shipping_limit_date\",\n",
    "        ]\n",
    "        for c in date_cols:\n",
    "            if c in o.columns:\n",
    "                o[c] = safe_to_datetime(o[c])\n",
    "            else:\n",
    "                o[c] = pd.NaT\n",
    "\n",
    "        o[\"purchase_dayofweek\"] = o[\"order_purchase_timestamp\"].dt.dayofweek\n",
    "        o[\"purchase_month\"] = o[\"order_purchase_timestamp\"].dt.month\n",
    "        o[\"purchase_hour\"] = o[\"order_purchase_timestamp\"].dt.hour\n",
    "\n",
    "        def to_hours(td):\n",
    "            return td.dt.total_seconds() / 3600.0\n",
    "\n",
    "        for c in [\"product_length_cm\",\"product_width_cm\",\"product_height_cm\",\"product_weight_g\",\n",
    "                  \"payment_installments_max\",\"payment_value_total\",\"payment_count\",\"freight_value\",\"order_item_id\"]:\n",
    "            if c in o.columns:\n",
    "                o[c] = pd.to_numeric(o[c], errors=\"coerce\")\n",
    "\n",
    "        o[\"approval_delay_h\"] = to_hours(o[\"order_approved_at\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_carrier_h\"] = to_hours(o[\"order_delivered_carrier_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"to_customer_h\"] = to_hours(o[\"order_delivered_customer_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"est_delivery_h\"] = to_hours(o[\"order_estimated_delivery_date\"] - o[\"order_purchase_timestamp\"])\n",
    "        o[\"limit_from_purchase_h\"] = to_hours(o[\"shipping_limit_date\"] - o[\"order_purchase_timestamp\"])\n",
    "\n",
    "        o[\"delivered_late\"] = (o[\"order_delivered_customer_date\"] > o[\"order_estimated_delivery_date\"]).astype(\"Int64\")\n",
    "\n",
    "        for c in [\"product_length_cm\",\"product_width_cm\",\"product_height_cm\",\"product_weight_g\"]:\n",
    "            if c not in o.columns:\n",
    "                o[c] = np.nan\n",
    "        o[\"product_volume_cm3\"] = o[\"product_length_cm\"] * o[\"product_width_cm\"] * o[\"product_height_cm\"]\n",
    "        o[\"density_g_per_cm3\"] = np.where(\n",
    "            (o[\"product_volume_cm3\"] > 0) & o[\"product_weight_g\"].notna(),\n",
    "            o[\"product_weight_g\"] / o[\"product_volume_cm3\"],\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        for c in [\"payment_installments_max\",\"payment_value_total\",\"payment_count\"]:\n",
    "            if c not in o.columns:\n",
    "                o[c] = np.nan\n",
    "        o[\"avg_installment_value\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_installments_max\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_installments_max\"],\n",
    "            np.nan,\n",
    "        )\n",
    "        o[\"payment_value_per_payment\"] = np.where(\n",
    "            pd.to_numeric(o[\"payment_count\"], errors=\"coerce\") > 0,\n",
    "            o[\"payment_value_total\"] / o[\"payment_count\"],\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        if \"freight_value\" not in o.columns:\n",
    "            o[\"freight_value\"] = np.nan\n",
    "        o[\"freight_per_kg\"] = np.where(\n",
    "            pd.to_numeric(o[\"product_weight_g\"], errors=\"coerce\") > 0,\n",
    "            o[\"freight_value\"] / (o[\"product_weight_g\"] / 1000.0),\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "        if \"order_item_id\" not in o.columns:\n",
    "            o[\"order_item_id\"] = 1\n",
    "        o[\"is_multi_item_order\"] = (pd.to_numeric(o[\"order_item_id\"], errors=\"coerce\") > 1).astype(\"Int64\")\n",
    "\n",
    "        if \"product_category_name\" in o.columns:\n",
    "            o[\"product_category_name\"] = o[\"product_category_name\"].astype(\"string\")\n",
    "        if \"product_category_name_english\" in o.columns:\n",
    "            o[\"product_category_name_english\"] = o[\"product_category_name_english\"].astype(\"string\")\n",
    "        if \"product_category_name\" in o.columns and \"product_category_name_english\" in o.columns:\n",
    "            s1 = o[\"product_category_name_english\"]\n",
    "            s2 = o[\"product_category_name\"]\n",
    "            o[\"product_category\"] = s1.mask(s1.isna() | (s1 == \"\"), s2)\n",
    "            o = o.drop(columns=[\"product_category_name\",\"product_category_name_english\"])\n",
    "        elif \"product_category_name_english\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name_english\"]\n",
    "            o = o.drop(columns=[\"product_category_name_english\"])\n",
    "        elif \"product_category_name\" in o.columns:\n",
    "            o[\"product_category\"] = o[\"product_category_name\"]\n",
    "            o = o.drop(columns=[\"product_category_name\"])\n",
    "        else:\n",
    "            o[\"product_category\"] = \"Unknown\"\n",
    "\n",
    "        for col in [\"customer_city\",\"seller_city\"]:\n",
    "            if col in o.columns:\n",
    "                s_ = o[col].astype(\"string\").fillna(\"Unknown\")\n",
    "                freq = s_.map(s_.value_counts(normalize=True))\n",
    "                o[col + \"_freq\"] = freq.astype(float)\n",
    "                o = o.drop(columns=[col])\n",
    "\n",
    "        cat_cols = []\n",
    "        for c in [\"order_status\",\"customer_state\",\"seller_state\",\"product_category\"]:\n",
    "            if c in o.columns:\n",
    "                cat_cols.append(c)\n",
    "                o[c] = o[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        if cat_cols:\n",
    "            o = pd.get_dummies(o, columns=cat_cols, dtype=np.uint8)\n",
    "\n",
    "        if \"review_score_mean_product\" in olist_in.columns:\n",
    "            base = pd.to_numeric(olist_in[\"review_score_mean_product\"], errors=\"coerce\")\n",
    "            if is_classification:\n",
    "                y = (base >= 4.0).astype(\"Int64\")\n",
    "            else:\n",
    "                y = base.astype(\"Float64\")\n",
    "        else:\n",
    "            print(\"olist: 'review_score_mean_product' missing; falling back to delivered_late.\")\n",
    "            y = o[\"delivered_late\"].copy()\n",
    "\n",
    "        o[\"target_olist\"] = y\n",
    "        o = o[o[\"target_olist\"].notna()].copy()\n",
    "        if is_classification:\n",
    "            o[\"target_olist\"] = o[\"target_olist\"].astype(\"int8\")\n",
    "        else:\n",
    "            o[\"target_olist\"] = o[\"target_olist\"].astype(\"float32\")\n",
    "\n",
    "        for leak_col in [\"review_score_mean_product\",\"review_count_product\",\"review_score_mean\",\"delivered_late\"]:\n",
    "            if leak_col in o.columns:\n",
    "                o = o.drop(columns=[leak_col])\n",
    "\n",
    "        drop_ids = [\"order_id\",\"customer_id\",\"customer_unique_id\",\"product_id\",\"seller_id\"]\n",
    "        o = o.drop(columns=[c for c in drop_ids if c in o.columns], errors=\"ignore\")\n",
    "        o = o.drop(columns=[c for c in date_cols if c in o.columns], errors=\"ignore\")\n",
    "\n",
    "        num_cols = o.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        num_cols_no_target = [c for c in num_cols if c != \"target_olist\"]\n",
    "        for c in num_cols_no_target:\n",
    "            o[c] = pd.to_numeric(o[c], errors=\"coerce\").fillna(pd.to_numeric(o[c], errors=\"coerce\").median())\n",
    "        obj_cols = o.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        for c in obj_cols:\n",
    "            o[c] = o[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        return o.copy()\n",
    "    \n",
    "    def prepare_sales_df(sales_in):\n",
    "        print(\"prep: sales\")\n",
    "        s = sales_in.copy()\n",
    "\n",
    "        # target\n",
    "        if \"Critic_Score\" in s.columns:\n",
    "            cs = pd.to_numeric(s[\"Critic_Score\"], errors=\"coerce\")\n",
    "            y = (cs > 8.0).astype(\"Int64\") if is_classification else cs.astype(\"Float64\")\n",
    "        else:\n",
    "            y = pd.Series(np.nan, index=s.index, dtype=\"Float64\")\n",
    "\n",
    "        s[\"target_sales\"] = y\n",
    "        s = s[s[\"target_sales\"].notna()].copy()\n",
    "        s[\"target_sales\"] = s[\"target_sales\"].astype(\"int8\" if is_classification else \"float32\")\n",
    "\n",
    "        # remove obvious leaks and junk\n",
    "        leak_cols = [\"NA_Sales\",\"PAL_Sales\",\"JP_Sales\",\"Other_Sales\",\"Total_Shipped\",\"Rank\",\"Global_Sales\"]\n",
    "        junk_cols = [\"VGChartz_Score\",\"Vgchartzscore\",\"url\",\"img_url\",\"status\",\"Last_Update\",\"basename\",\"User_Score\"]\n",
    "        s = s.drop(columns=[c for c in leak_cols + junk_cols if c in s.columns], errors=\"ignore\")\n",
    "        if \"Critic_Score\" in s.columns:\n",
    "            s = s.drop(columns=[\"Critic_Score\"], errors=\"ignore\")\n",
    "\n",
    "        # simple remaster flag, then drop Name\n",
    "        if \"Name\" in s.columns:\n",
    "            terms = [\"remaster\",\"remastered\",\"hd\",\"definitive\",\"collection\",\"trilogy\",\"anniversary\"]\n",
    "            s[\"is_remaster\"] = s[\"Name\"].astype(\"string\").str.lower().str.contains(\"|\".join(terms), na=False).astype(\"Int64\")\n",
    "            s = s.drop(columns=[\"Name\"])\n",
    "        else:\n",
    "            s[\"is_remaster\"] = pd.Series(0, index=s.index, dtype=\"Int64\")\n",
    "\n",
    "        # platform family + handheld flag\n",
    "        def platform_family(p):\n",
    "            p = \"\" if pd.isna(p) else str(p).upper()\n",
    "            if p.startswith(\"PS\") or p in {\"PSP\",\"PSV\"}: return \"PlayStation\"\n",
    "            if p.startswith(\"X\") or p in {\"XB\",\"XBLA\"}: return \"Xbox\"\n",
    "            if p in {\"SWITCH\",\"WII\",\"WIIU\",\"GC\",\"N64\",\"SNES\",\"NES\",\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\"}: return \"Nintendo\"\n",
    "            if p == \"PC\": return \"PC\"\n",
    "            if p in {\"DC\",\"DREAMCAST\",\"SAT\",\"GEN\",\"MD\",\"MEGADRIVE\",\"GG\"}: return \"Sega\"\n",
    "            if \"ATARI\" in p: return \"Atari\"\n",
    "            return \"Other\"\n",
    "\n",
    "        if \"Platform\" in s.columns:\n",
    "            s[\"Platform_Family\"] = s[\"Platform\"].apply(platform_family)\n",
    "            handhelds = {\"DS\",\"3DS\",\"GB\",\"GBC\",\"GBA\",\"PSP\",\"PSV\"}\n",
    "            s[\"is_portable\"] = s[\"Platform\"].astype(\"string\").str.upper().isin(handhelds).astype(\"Int64\")\n",
    "            s = s.drop(columns=[\"Platform\"])\n",
    "        else:\n",
    "            s[\"Platform_Family\"] = \"Other\"\n",
    "            s[\"is_portable\"] = pd.Series(0, index=s.index, dtype=\"Int64\")\n",
    "\n",
    "        # year / decade\n",
    "        if \"Year\" in s.columns:\n",
    "            s[\"Year\"] = pd.to_numeric(s[\"Year\"], errors=\"coerce\")\n",
    "            s.loc[~s[\"Year\"].between(1970, 2025, inclusive=\"both\"), \"Year\"] = np.nan\n",
    "            s[\"Decade\"] = (s[\"Year\"] // 10 * 10).astype(\"Int64\").astype(str)\n",
    "        else:\n",
    "            s[\"Year\"] = np.nan\n",
    "            s[\"Decade\"] = \"<NA>\"\n",
    "\n",
    "        # frequency encodes then drop raw strings\n",
    "        for col in [\"Publisher\",\"Developer\"]:\n",
    "            if col in s.columns:\n",
    "                series = s[col].astype(\"string\").fillna(\"Unknown\")\n",
    "                freq = series.map(series.value_counts(normalize=True))\n",
    "                s[col + \"_freq\"] = freq.astype(float)\n",
    "                s = s.drop(columns=[col])\n",
    "\n",
    "        # one-hot encode once, after building the list\n",
    "        cat_cols = [c for c in [\"Genre\",\"ESRB_Rating\",\"Platform_Family\",\"Decade\"] if c in s.columns]\n",
    "        for c in cat_cols:\n",
    "            s[c] = s[c].astype(\"string\").fillna(\"Unknown\")\n",
    "        if len(cat_cols) > 0:\n",
    "            s = pd.get_dummies(s, columns=cat_cols, dtype=np.uint8)\n",
    "\n",
    "        # numeric cleanup\n",
    "        num_cols = s.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        num_cols_no_target = [c for c in num_cols if c != \"target_sales\"]\n",
    "        for c in num_cols_no_target:\n",
    "            s[c] = pd.to_numeric(s[c], errors=\"coerce\").fillna(pd.to_numeric(s[c], errors=\"coerce\").median())\n",
    "\n",
    "        # object cleanup\n",
    "        obj_cols = s.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        for c in obj_cols:\n",
    "            s[c] = s[c].astype(\"string\").fillna(\"Unknown\")\n",
    "\n",
    "        return s.copy()\n",
    "\n",
    "\n",
    "    steam_df, steam_raw = prepare_steam_df(steam)\n",
    "    olist_df = prepare_olist_df(olist)\n",
    "    sales_df = prepare_sales_df(sales)\n",
    "\n",
    "    raw_for_split = steam_raw.loc[steam_df.index].copy()\n",
    "    raw_for_split[\"date\"] = pd.to_datetime(raw_for_split[\"date\"].astype(\"string\"), errors=\"coerce\")\n",
    "    cutoff_q = 1.0 - float(test_size)\n",
    "    cutoff_date = raw_for_split[\"date\"].sort_values().quantile(cutoff_q)\n",
    "\n",
    "    grp_first = raw_for_split.groupby(\"app_id\")[\"date\"].min()\n",
    "    app_train = set(grp_first[grp_first <= cutoff_date].index.tolist())\n",
    "    app_test = set(grp_first[grp_first > cutoff_date].index.tolist())\n",
    "\n",
    "    is_train_mask = raw_for_split[\"app_id\"].isin(app_train)\n",
    "    is_test_mask = raw_for_split[\"app_id\"].isin(app_test)\n",
    "    is_train_mask = is_train_mask | (~(is_train_mask | is_test_mask))\n",
    "\n",
    "    Xs = steam_df.drop(columns=[\"target_steam\"])\n",
    "    ys = steam_df[\"target_steam\"]\n",
    "\n",
    "    X_train_steam = Xs[is_train_mask].reset_index(drop=True)\n",
    "    X_test_steam = Xs[is_test_mask].reset_index(drop=True)\n",
    "    y_train_steam = ys[is_train_mask].reset_index(drop=True)\n",
    "    y_test_steam = ys[is_test_mask].reset_index(drop=True)\n",
    "\n",
    "    print(\"steam cutoff_date:\", pd.to_datetime(cutoff_date))\n",
    "    print(\"steam apps in test:\", len(app_test), \"of\", len(grp_first))\n",
    "    print(\"steam train rows:\", int(X_train_steam.shape[0]), \"| test rows:\", int(X_test_steam.shape[0]))\n",
    "    if is_classification:\n",
    "        print(\"steam y_train counts:\", y_train_steam.value_counts().to_dict())\n",
    "        print(\"steam y_test counts:\", y_test_steam.value_counts().to_dict())\n",
    "    else:\n",
    "        def sstats(s):\n",
    "            return {\"n\": int(s.shape[0]),\n",
    "                    \"mean\": float(np.nanmean(s)),\n",
    "                    \"std\": float(np.nanstd(s)),\n",
    "                    \"min\": float(np.nanmin(s)),\n",
    "                    \"max\": float(np.nanmax(s))}\n",
    "        print(\"steam y_train stats:\", sstats(y_train_steam))\n",
    "        print(\"steam y_test stats:\", sstats(y_test_steam))\n",
    "\n",
    "    if is_classification:\n",
    "        if y_train_steam.nunique() == 2:\n",
    "            vc = y_train_steam.value_counts()\n",
    "            ratio = float(vc.min()) / float(vc.max()) if vc.max() > 0 else 1.0\n",
    "            if balance_method != \"none\" and ratio < min_class_ratio:\n",
    "                method = \"oversample\" if (balance_method == \"auto\" and len(X_train_steam) <= 200000) else \\\n",
    "                         (\"undersample\" if balance_method == \"auto\" else balance_method)\n",
    "                X_train_steam, y_train_steam = resample_binary(X_train_steam, y_train_steam, method=method, random_state=random_state)\n",
    "                print(\"steam balanced train counts:\", y_train_steam.value_counts().to_dict())\n",
    "\n",
    "    steam_split = (X_train_steam, X_test_steam, y_train_steam, y_test_steam)\n",
    "\n",
    "    olist_split = split_and_balance(olist_df, \"target_olist\", balance_method, min_class_ratio, random_state)\n",
    "    sales_split = split_and_balance(sales_df, \"target_sales\", balance_method, min_class_ratio, random_state)\n",
    "\n",
    "    def scale_poly_wrapper(split):\n",
    "        Xtr, Xte, ytr, yte = split\n",
    "        Xtr2, Xte2 = apply_scaling_and_poly(Xtr, Xte)\n",
    "        return (Xtr2, Xte2, ytr, yte)\n",
    "\n",
    "    steam_split = scale_poly_wrapper(steam_split)\n",
    "    olist_split = scale_poly_wrapper(olist_split)\n",
    "    sales_split = scale_poly_wrapper(sales_split)\n",
    "\n",
    "    if feature_select_method != \"none\":\n",
    "        Xtr, Xte, ytr, yte = steam_split\n",
    "        Xtr_s, Xte_s = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"steam\"\n",
    "        )\n",
    "        steam_split = (Xtr_s, Xte_s, ytr, yte)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = olist_split\n",
    "        Xtr_o, Xte_o = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"olist\"\n",
    "        )\n",
    "        olist_split = (Xtr_o, Xte_o, ytr, yte)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = sales_split\n",
    "        Xtr_v, Xte_v = apply_feature_selection(\n",
    "            Xtr, ytr, Xte, feature_select_method, feature_select_k, random_state, dataset_name=\"sales\"\n",
    "        )\n",
    "        sales_split = (Xtr_v, Xte_v, ytr, yte)\n",
    "\n",
    "    # show exactly which saved features are missing (after selection)\n",
    "    _ = debug_forward_missing(steam_split[0], \"steam\", best_forward_features)\n",
    "    _ = debug_forward_missing(olist_split[0], \"olist\", best_forward_features)\n",
    "    _ = debug_forward_missing(sales_split[0], \"sales\", best_forward_features)\n",
    "\n",
    "    print(\"\\nsteam selected features (train, test):\", steam_split[0].shape[1], steam_split[1].shape[1])\n",
    "    print(\"olist selected features (train, test):\", olist_split[0].shape[1], olist_split[1].shape[1])\n",
    "    print(\"sales selected features (train, test):\", sales_split[0].shape[1], sales_split[1].shape[1])\n",
    "\n",
    "    return {\n",
    "        \"steam\": steam_split,\n",
    "        \"olist\": olist_split,\n",
    "        \"sales\": sales_split,\n",
    "        \"feature_names\": {\n",
    "            \"steam\": steam_split[0].columns.tolist(),\n",
    "            \"olist\": olist_split[0].columns.tolist(),\n",
    "            \"sales\": sales_split[0].columns.tolist(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    values = []\n",
    "    value = start\n",
    "    while value <= stop:\n",
    "        values.append(value)\n",
    "        value += step\n",
    "    return values\n",
    "\n",
    "\n",
    "# call\n",
    "splits = prepare_all(\n",
    "    steam, olist, sales,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    balance_method=\"auto\",\n",
    "    min_class_ratio=0.5,\n",
    "    feature_select_method=\"mi\",\n",
    "    feature_select_k=30,\n",
    "    task_type=\"regression\",\n",
    "    scale_method=\"minmax\",\n",
    "    poly_degree=None,\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = splits[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = splits[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = splits[\"sales\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ba1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_knn_search(\n",
    "    param_distributions,\n",
    "    n_iter,\n",
    "    cv,\n",
    "    scoring,\n",
    "    n_jobs,\n",
    "    verbose,\n",
    "    X_train,\n",
    "    y_train\n",
    "\n",
    "):\n",
    "    # Build the search object\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=KNeighborsRegressor(n_jobs=n_jobs),\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=1,\n",
    "        verbose=verbose,\n",
    "        refit=True,\n",
    "        random_state=42,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    # Fit on training data\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Make a results table\n",
    "    results_table = pd.DataFrame(search.cv_results_)\n",
    "    results_table = results_table.sort_values(\"mean_test_score\", ascending=False)\n",
    "\n",
    "    # Show top 5 CV scores\n",
    "    print(\"Top 5 CV scores:\")\n",
    "    top_five = results_table.head(5)[[\"mean_test_score\", \"std_test_score\", \"params\"]]\n",
    "    for rank, row in top_five.reset_index(drop=True).iterrows():\n",
    "        mean_score = row[\"mean_test_score\"]\n",
    "        std_score = row[\"std_test_score\"]\n",
    "        params = row[\"params\"]\n",
    "        print(f\"{rank + 1}. score={mean_score:.6f} (+/- {std_score:.6f}) | params={params}\")\n",
    "\n",
    "    # Show the best model\n",
    "    print(\"\\nBest model found:\")\n",
    "    print(search.best_estimator_)\n",
    "    print(f\"Best CV score: {search.best_score_:.6f}\")\n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92624a5f",
   "metadata": {},
   "source": [
    "# Steam Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19ceed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=  28.7s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=  27.8s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=  27.7s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=  27.8s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=  27.7s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=  27.7s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=  28.2s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=  28.4s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=  28.2s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=  28.4s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=  28.7s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=  28.5s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=  28.6s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=  28.5s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=  28.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=  27.9s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=  28.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=  27.8s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=  27.9s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=  27.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time= 2.0min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time= 1.9min\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=22.5min\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=23.1min\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=24.5min\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=24.8min\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=24.8min\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time= 1.9min\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time= 1.9min\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=22.4min\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=23.1min\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=24.5min\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=24.6min\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=24.5min\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=  28.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=  27.8s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=  27.8s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=  27.8s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=  27.7s\n",
      "Top 5 CV scores:\n",
      "1. score=-2.459382 (+/- 0.009272) | params={'p': 2, 'n_neighbors': 7, 'metric': 'manhattan', 'algorithm': 'auto'}\n",
      "2. score=-2.484813 (+/- 0.012443) | params={'p': 1, 'n_neighbors': 3, 'metric': 'euclidean', 'algorithm': 'auto'}\n",
      "3. score=-2.498203 (+/- 0.009271) | params={'p': 2, 'n_neighbors': 7, 'metric': 'euclidean', 'algorithm': 'auto'}\n",
      "4. score=-2.498203 (+/- 0.009271) | params={'p': 2, 'n_neighbors': 7, 'metric': 'minkowski', 'algorithm': 'auto'}\n",
      "5. score=-2.522065 (+/- 0.010931) | params={'p': 1, 'n_neighbors': 11, 'metric': 'minkowski', 'algorithm': 'auto'}\n",
      "\n",
      "Best model found:\n",
      "KNeighborsRegressor(metric='manhattan', n_jobs=-1, n_neighbors=7)\n",
      "Best CV score: -2.459382\n",
      "Best hyperparameters: {'p': 2, 'n_neighbors': 7, 'metric': 'manhattan', 'algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# Safe KNN search space (keeps valid combos for cosine)\n",
    "param_distributions = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11, 15, 21],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\"],\n",
    "    \"p\": [1, 2], # used only when metric=\"minkowski\"\n",
    "    \"algorithm\": [\"auto\"]\n",
    "}\n",
    "\n",
    "randomized_knn_search(\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    X_train=X_train_steam,\n",
    "    y_train=y_train_steam\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28373b5",
   "metadata": {},
   "source": [
    "# Olist Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "039e93ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   1.0s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=  13.3s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=  13.7s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=  13.6s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=  13.7s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=  11.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   1.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.9s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=  13.5s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=  13.6s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=  13.7s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=  13.7s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=  11.9s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.3s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=  13.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=  13.4s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=  13.6s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=  13.5s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=  11.8s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.3s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.9s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   1.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   1.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.9s\n",
      "Top 5 CV scores:\n",
      "1. score=-0.552754 (+/- 0.003208) | params={'p': 1, 'n_neighbors': 11, 'metric': 'minkowski', 'algorithm': 'auto'}\n",
      "2. score=-0.553138 (+/- 0.002923) | params={'p': 1, 'n_neighbors': 9, 'metric': 'minkowski', 'algorithm': 'auto'}\n",
      "3. score=-0.553588 (+/- 0.002754) | params={'p': 2, 'n_neighbors': 7, 'metric': 'manhattan', 'algorithm': 'auto'}\n",
      "4. score=-0.555023 (+/- 0.003472) | params={'p': 2, 'n_neighbors': 21, 'metric': 'manhattan', 'algorithm': 'auto'}\n",
      "5. score=-0.555023 (+/- 0.003472) | params={'p': 1, 'n_neighbors': 21, 'metric': 'manhattan', 'algorithm': 'auto'}\n",
      "\n",
      "Best model found:\n",
      "KNeighborsRegressor(n_jobs=-1, n_neighbors=11, p=1)\n",
      "Best CV score: -0.552754\n",
      "Best hyperparameters: {'p': 1, 'n_neighbors': 11, 'metric': 'minkowski', 'algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# Safe KNN search space (keeps valid combos for cosine)\n",
    "param_distributions = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11, 15, 21],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\"],\n",
    "    \"p\": [1, 2], # used only when metric=\"minkowski\"\n",
    "    \"algorithm\": [\"auto\"]\n",
    "}\n",
    "\n",
    "randomized_knn_search(\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    X_train=X_train_olist,\n",
    "    y_train=y_train_olist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754814a",
   "metadata": {},
   "source": [
    "# Video Game Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67006764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=3, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=7, p=2; total time=   0.0s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=   0.1s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=   0.1s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=   0.1s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=   0.1s\n",
      "[CV] END ..algorithm=auto, metric=cosine, n_neighbors=5, p=1; total time=   0.1s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=21, p=1; total time=   0.0s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=   0.1s\n",
      "[CV] END .algorithm=auto, metric=cosine, n_neighbors=21, p=1; total time=   0.1s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=5, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=11, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=5, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=9, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=euclidean, n_neighbors=7, p=1; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=minkowski, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.0s\n",
      "[CV] END algorithm=auto, metric=manhattan, n_neighbors=21, p=2; total time=   0.0s\n",
      "Top 5 CV scores:\n",
      "1. score=-1.083811 (+/- 0.011472) | params={'p': 2, 'n_neighbors': 21, 'metric': 'manhattan', 'algorithm': 'auto'}\n",
      "2. score=-1.083811 (+/- 0.011472) | params={'p': 1, 'n_neighbors': 21, 'metric': 'manhattan', 'algorithm': 'auto'}\n",
      "3. score=-1.088982 (+/- 0.012729) | params={'p': 2, 'n_neighbors': 21, 'metric': 'minkowski', 'algorithm': 'auto'}\n",
      "4. score=-1.088982 (+/- 0.012729) | params={'p': 2, 'n_neighbors': 21, 'metric': 'euclidean', 'algorithm': 'auto'}\n",
      "5. score=-1.088982 (+/- 0.012729) | params={'p': 1, 'n_neighbors': 21, 'metric': 'euclidean', 'algorithm': 'auto'}\n",
      "\n",
      "Best model found:\n",
      "KNeighborsRegressor(metric='manhattan', n_jobs=-1, n_neighbors=21, p=1)\n",
      "Best CV score: -1.083811\n",
      "Best hyperparameters: {'p': 1, 'n_neighbors': 21, 'metric': 'manhattan', 'algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# Safe KNN search space (keeps valid combos for cosine)\n",
    "param_distributions = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11, 15, 21],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\", \"cosine\"],\n",
    "    \"p\": [1, 2], # used only when metric=\"minkowski\"\n",
    "    \"algorithm\": [\"auto\"]\n",
    "}\n",
    "\n",
    "randomized_knn_search(\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    X_train=X_train_sales,\n",
    "    y_train=y_train_sales\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
