{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d12f17d",
   "metadata": {},
   "source": [
    "# Week 13 - Naive Bayes, Bernoulli, Multnomial, and Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c63f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: starting antonkozyriev/game-recommendations-on-steam\n",
      "download: done antonkozyriev/game-recommendations-on-steam -> /Users/chandlercampbell/.cache/kagglehub/datasets/antonkozyriev/game-recommendations-on-steam/versions/28 in 0.176 sec\n",
      "download: starting olistbr/brazilian-ecommerce\n",
      "download: done olistbr/brazilian-ecommerce -> /Users/chandlercampbell/.cache/kagglehub/datasets/olistbr/brazilian-ecommerce/versions/2 in 0.193 sec\n",
      "download: starting ashaheedq/video-games-sales-2019\n",
      "download: done ashaheedq/video-games-sales-2019 -> /Users/chandlercampbell/.cache/kagglehub/datasets/ashaheedq/video-games-sales-2019/versions/2 in 0.402 sec\n",
      "steam: start\n",
      "steam: shapes games=(50872, 13), users=(14306064, 3), recs=(41154794, 8), meta=(50872, 3)\n",
      "stratified_sample: picked 50000 of 41154794 rows in 5.816 sec\n",
      "steam: merge games with metadata\n",
      "steam: merge recommendations with games\n",
      "steam: merge with users\n",
      "dates: converting possible date/time columns\n",
      "steam: done shape=(50000, 24)\n",
      "olist: start\n",
      "olist: shapes customers=(99441, 5), geolocation=(1000163, 5), items=(112650, 7), payments=(103886, 5), reviews=(99224, 7), orders=(99441, 8), products=(32951, 9), sellers=(3095, 4), cat_trans=(71, 2)\n",
      "olist: sample orders\n",
      "simple_random_sample: picked 50000 of 99441 rows in 0.005 sec\n",
      "olist: filter items for sampled orders\n",
      "olist: merge category translation\n",
      "olist: build product review stats\n",
      "olist: merge items, products, and sellers\n",
      "olist: build basic zip geo\n",
      "olist: merge customers with geo\n",
      "olist: aggregate payments\n",
      "olist: assemble main table\n",
      "olist: merge payments\n",
      "olist: merge product stats\n",
      "dates: converting possible date/time columns\n",
      "olist: shape after assemble (57112, 38)\n",
      "olist: done\n",
      "vg2019: start\n",
      "vg2019: loaded vgsales-12-4-2019-short.csv with shape (55792, 16)\n",
      "vg2019: stratified sample by Genre\n",
      "stratified_sample: class counts too small for requested size, falling back to simple sample\n",
      "simple_random_sample: picked 50000 of 55792 rows in 0.004 sec\n",
      "vg2019: done shape=(50000, 16)\n",
      "main: load all done in 16.677 sec (00:00:16)\n",
      "download: shapes summary\n",
      "download: steam shape = (50000, 24)\n",
      "download: olist shape = (57112, 38)\n",
      "download: sales shape = (50000, 16)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Standard Library\n",
    "# =============================\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from itertools import chain, combinations\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# =============================\n",
    "# General / Utility (3rd party)\n",
    "# =============================\n",
    "import requests\n",
    "import kagglehub\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from collections import Counter\n",
    "\n",
    "# =============================\n",
    "# Data / Scientific\n",
    "# =============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "from scipy.special import expit, logit\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Visualization\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================\n",
    "# Imbalanced-Learn\n",
    "# =============================\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn (core / model selection)\n",
    "# =============================\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning, NotFittedError\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import (\n",
    "    SequentialFeatureSelector,\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    f_classif,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    "    mutual_info_regression,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import (\n",
    "    ElasticNet,\n",
    "    ElasticNetCV,\n",
    "    Lasso,\n",
    "    LassoCV,\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    "    Ridge,\n",
    "    RidgeCV,\n",
    "    RidgeClassifier,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    get_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    ParameterGrid,\n",
    "    ParameterSampler,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    "    TimeSeriesSplit,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "import sklearn.model_selection._search as sk_search\n",
    "import sklearn.model_selection._validation as sk_validation\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    MultiLabelBinarizer,\n",
    "    Normalizer,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Scikit-learn (ensembles)\n",
    "# =============================\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Global Settings\n",
    "# =============================\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "random_state = 42\n",
    "N_ROWS = 50_000\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")  # no scientific notation\n",
    "\n",
    "def robust_eda(df, name):\n",
    "    # simple settings\n",
    "    top_k_categories = 20\n",
    "    max_corr_cols = 30\n",
    "    max_rows_to_show = 25\n",
    "\n",
    "    # make printing wide and avoid scientific notation\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\", max_rows_to_show,\n",
    "        \"display.max_columns\", None,\n",
    "        \"display.width\", 1000,\n",
    "        \"display.max_colwidth\", 200,\n",
    "        \"display.float_format\", lambda x: f\"{x:.6f}\"\n",
    "    ):\n",
    "        report_lines = []\n",
    "\n",
    "        # title\n",
    "        report_lines.append(f\"=== Robust EDA Report: {name} ===\")\n",
    "\n",
    "        # shapes and memory\n",
    "        info_df = pd.DataFrame(\n",
    "            {\n",
    "                \"rows\": [df.shape[0]],\n",
    "                \"columns\": [df.shape[1]],\n",
    "                \"memory_bytes\": [int(df.memory_usage(deep=True).sum())],\n",
    "            }\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Info ===\")\n",
    "        report_lines.append(info_df.to_string(index=False))\n",
    "\n",
    "        # dtypes\n",
    "        dtypes_df = (\n",
    "            df.dtypes.rename(\"dtype\")\n",
    "            .astype(str)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"column\"})\n",
    "            .sort_values(\"column\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Dtypes ===\")\n",
    "        report_lines.append(dtypes_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # missing values\n",
    "        total_rows = len(df)\n",
    "        missing_counts = df.isna().sum()\n",
    "        if total_rows > 0:\n",
    "            missing_percent = (missing_counts / total_rows * 100).round(2)\n",
    "        else:\n",
    "            missing_percent = pd.Series([0] * len(df.columns), index=df.columns)\n",
    "        missing_df = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": df.columns,\n",
    "                    \"missing_count\": missing_counts.values,\n",
    "                    \"missing_percent\": missing_percent.values,\n",
    "                }\n",
    "            )\n",
    "            .sort_values([\"missing_count\", \"missing_percent\"], ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Missing Values ===\")\n",
    "        report_lines.append(missing_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # duplicates (safe fallback for unhashable types)\n",
    "        try:\n",
    "            duplicate_count = int(df.duplicated().sum())\n",
    "            duplicate_index = df.index[df.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "        except TypeError:\n",
    "            df_hashable = df.astype(str)\n",
    "            duplicate_count = int(df_hashable.duplicated().sum())\n",
    "            duplicate_index = df_hashable.index[df_hashable.duplicated(keep=False)]\n",
    "            duplicates_preview_df = df.loc[duplicate_index].head(20)\n",
    "\n",
    "        duplicates_summary_df = pd.DataFrame({\"duplicate_rows\": [duplicate_count]})\n",
    "        report_lines.append(\"\\n=== Duplicates Summary ===\")\n",
    "        report_lines.append(duplicates_summary_df.to_string(index=False))\n",
    "        report_lines.append(\"\\n=== Duplicates Preview (up to 20 rows) ===\")\n",
    "        if len(duplicates_preview_df) > 0:\n",
    "            report_lines.append(duplicates_preview_df.to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"(No duplicate rows found.)\")\n",
    "\n",
    "        # column groups\n",
    "        numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        categorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "        # numeric summary\n",
    "        if len(numeric_columns) > 0:\n",
    "            percentiles = [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "            numeric_summary_df = (\n",
    "                df[numeric_columns]\n",
    "                .describe(percentiles=percentiles)\n",
    "                .T.reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Numeric Summary (5%..95%) ===\")\n",
    "            report_lines.append(numeric_summary_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # skew and kurtosis\n",
    "            skew_kurt_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"column\": numeric_columns,\n",
    "                    \"skew\": df[numeric_columns].skew(numeric_only=True).values,\n",
    "                    \"kurtosis\": df[numeric_columns].kurtosis(numeric_only=True).values,\n",
    "                }\n",
    "            )\n",
    "            report_lines.append(\"\\n=== Skew and Kurtosis ===\")\n",
    "            report_lines.append(skew_kurt_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # IQR outliers per column\n",
    "            q1 = df[numeric_columns].quantile(0.25)\n",
    "            q3 = df[numeric_columns].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outlier_mask = (df[numeric_columns] < (q1 - 1.5 * iqr)) | (df[numeric_columns] > (q3 + 1.5 * iqr))\n",
    "            iqr_outliers_df = (\n",
    "                outlier_mask.sum()\n",
    "                .rename(\"outlier_count\")\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"column\"})\n",
    "            )\n",
    "            report_lines.append(\"\\n=== IQR Outlier Counts ===\")\n",
    "            report_lines.append(iqr_outliers_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "            # correlation on first N numeric columns\n",
    "            if len(numeric_columns) > 1:\n",
    "                selected_cols = numeric_columns[:max_corr_cols]\n",
    "                correlation_df = df[selected_cols].corr(method=\"pearson\", numeric_only=True)\n",
    "                correlation_df.index.name = \"column\"\n",
    "                report_lines.append(f\"\\n=== Correlation (first {max_corr_cols} numeric columns) ===\")\n",
    "                report_lines.append(correlation_df.to_string())\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No numeric columns found.)\")\n",
    "\n",
    "        # categorical value counts (top K each)\n",
    "        if len(categorical_columns) > 0:\n",
    "            cat_rows = []\n",
    "            for col in categorical_columns:\n",
    "                try:\n",
    "                    vc = df[col].value_counts(dropna=False).head(top_k_categories)\n",
    "                except TypeError:\n",
    "                    vc = df[col].astype(str).value_counts(dropna=False).head(top_k_categories)\n",
    "                for value, count in vc.items():\n",
    "                    percent = (count / total_rows * 100) if total_rows > 0 else 0\n",
    "                    cat_rows.append(\n",
    "                        {\"column\": col, \"value\": value, \"count\": int(count), \"percent\": round(percent, 2)}\n",
    "                    )\n",
    "            categorical_values_df = pd.DataFrame(cat_rows)\n",
    "            report_lines.append(f\"\\n=== Categorical Values (Top {top_k_categories} per column) ===\")\n",
    "            report_lines.append(categorical_values_df.head(max_rows_to_show).to_string(index=False))\n",
    "        else:\n",
    "            report_lines.append(\"\\n(No categorical columns found.)\")\n",
    "\n",
    "        # unique counts per column\n",
    "        def _safe_nunique(series):\n",
    "            try:\n",
    "                return int(series.nunique(dropna=False))\n",
    "            except TypeError:\n",
    "                return np.nan\n",
    "\n",
    "        unique_counts_df = pd.DataFrame(\n",
    "            {\"column\": df.columns, \"unique_values\": [_safe_nunique(df[c]) for c in df.columns]}\n",
    "        )\n",
    "        report_lines.append(\"\\n=== Unique Counts Per Column ===\")\n",
    "        report_lines.append(unique_counts_df.head(max_rows_to_show).to_string(index=False))\n",
    "\n",
    "        # sample head\n",
    "        report_lines.append(\"\\n=== Head (10 rows) ===\")\n",
    "        report_lines.append(df.head(10).to_string(index=False))\n",
    "\n",
    "        # end\n",
    "        report_lines.append(\"\\n=== End of EDA Report ===\")\n",
    "\n",
    "        # one giant print\n",
    "        print(\"\\n\".join(report_lines))\n",
    "\n",
    "\n",
    "def is_sparse_dtype(dtype):\n",
    "    # check if a dtype is pandas sparse\n",
    "        return pd.api.types.is_sparse(dtype)\n",
    "\n",
    "def dollar_format(x, pos=None):\n",
    "    # format money like $12,345\n",
    "    return f\"${x:,.0f}\"\n",
    "\n",
    "def format_hms(seconds):\n",
    "    # format seconds to H:M:S and handle >24 hours\n",
    "    seconds = int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "def try_read_csv(folder_path, file_name, **kwargs):\n",
    "    # try to read a csv; file_name can be a full path\n",
    "    full_path = os.path.join(folder_path, file_name) if folder_path else file_name\n",
    "    if full_path and os.path.exists(full_path):\n",
    "        try:\n",
    "            return pd.read_csv(full_path, **kwargs)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def list_csvs(folder_path):\n",
    "    # list csv files (sorted)\n",
    "    if not folder_path or not os.path.exists(folder_path):\n",
    "        return []\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".csv\")])\n",
    "\n",
    "def simple_random_sample(data_frame, n_rows=None, frac=None, random_state=42):\n",
    "    # sample without replacement\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "    total_rows = len(data_frame)\n",
    "    if (n_rows is None) == (frac is None):\n",
    "        raise ValueError(\"pass exactly one of n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        pick_rows = int(np.floor(frac * total_rows))\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        pick_rows = min(int(n_rows), total_rows)\n",
    "\n",
    "    if pick_rows >= total_rows:\n",
    "        print(\"simple_random_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    pick_index = rng.choice(total_rows, size=pick_rows, replace=False)\n",
    "    pick_index = np.sort(pick_index)  # keep original order\n",
    "    out_df = data_frame.iloc[pick_index].copy()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"simple_random_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def stratified_sample(data_frame, y, n_rows=None, frac=None, random_state=42):\n",
    "    # stratified sample on labels y\n",
    "    if data_frame is None:\n",
    "        raise ValueError(\"data_frame is None\")\n",
    "\n",
    "    y_array = data_frame[y].to_numpy() if isinstance(y, str) else np.asarray(y)\n",
    "    total_rows = len(data_frame)\n",
    "    if len(y_array) != total_rows:\n",
    "        raise ValueError(\"X and y length mismatch\")\n",
    "\n",
    "    # prefer n_rows if both given\n",
    "    if n_rows is not None and frac is not None:\n",
    "        frac = None\n",
    "    if n_rows is None and frac is None:\n",
    "        raise ValueError(\"provide n_rows or frac\")\n",
    "\n",
    "    if frac is not None:\n",
    "        if not (0 < frac <= 1):\n",
    "            raise ValueError(\"frac must be between 0 and 1\")\n",
    "        test_size = float(frac)\n",
    "        use_frac, use_n = frac, None\n",
    "    else:\n",
    "        if int(n_rows) <= 0:\n",
    "            raise ValueError(\"n_rows must be > 0\")\n",
    "        test_size = min(float(n_rows) / total_rows, 1.0)\n",
    "        use_frac, use_n = None, int(n_rows)\n",
    "\n",
    "    if test_size >= 1.0:\n",
    "        print(\"stratified_sample: taking all rows\")\n",
    "        return data_frame.copy()\n",
    "\n",
    "    _, counts = np.unique(y_array, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "\n",
    "    # need at least 1 per class in both splits\n",
    "    if min_count < 2 or (min_count * test_size < 1) or (min_count * (1.0 - test_size) < 1):\n",
    "        print(\"stratified_sample: class counts too small for requested size, falling back to simple sample\")\n",
    "        return simple_random_sample(data_frame, n_rows=use_n, frac=use_frac, random_state=random_state)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    index_array = np.arange(total_rows)\n",
    "    _, test_idx, _, _ = train_test_split(\n",
    "        index_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        stratify=y_array,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    out_df = data_frame.iloc[np.sort(test_idx)].copy()  # keep original order\n",
    "    end = time.perf_counter()\n",
    "    print(f\"stratified_sample: picked {len(out_df)} of {total_rows} rows in {round(end - start, 3)} sec\")\n",
    "    return out_df\n",
    "\n",
    "def safe_kaggle_download(dataset_name):\n",
    "    # download from kaggle with timing and errors\n",
    "    print(f\"download: starting {dataset_name}\")\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(dataset_name)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: done {dataset_name} -> {path} in {round(end - start, 3)} sec\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        end = time.perf_counter()\n",
    "        print(f\"download: error {dataset_name} -> {str(e)} in {round(end - start, 3)} sec\")\n",
    "        return None\n",
    "\n",
    "def coerce_datetime_columns(df):\n",
    "    # convert likely date/time columns if they are strings\n",
    "    if df is None:\n",
    "        return None\n",
    "    print(\"dates: converting possible date/time columns\")\n",
    "    for col_name in df.columns:\n",
    "        lower = col_name.lower()\n",
    "        if (\"date\" in lower) or (\"time\" in lower):\n",
    "            s = df[col_name]\n",
    "            try:\n",
    "                if pd.api.types.is_object_dtype(s) or pd.api.types.is_string_dtype(s):\n",
    "                    df[col_name] = pd.to_datetime(s, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    # float range with guards and tolerance\n",
    "    if step == 0:\n",
    "        raise ValueError(\"step must not be 0\")\n",
    "    values = []\n",
    "    value = float(start)\n",
    "    tolerance = abs(step) / 1_000_000\n",
    "    if step > 0:\n",
    "        while value <= stop + tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    else:\n",
    "        while value >= stop - tolerance:\n",
    "            values.append(round(value, 12))\n",
    "            value += step\n",
    "    return values\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Steam Loader\n",
    "# =============================\n",
    "def load_steam_dataset(base_path, n_rows=100_000, seed=42):\n",
    "    print(\"steam: start\")\n",
    "    if base_path is None:\n",
    "        print(\"steam: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    games = try_read_csv(base_path, \"games.csv\", low_memory=False)\n",
    "    users = try_read_csv(base_path, \"users.csv\", low_memory=False)\n",
    "    recommendations = try_read_csv(base_path, \"recommendations.csv\", low_memory=False)\n",
    "\n",
    "    metadata = None\n",
    "    meta_path = os.path.join(base_path, \"games_metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        try:\n",
    "            metadata = pd.read_json(meta_path, lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"steam: metadata read error -> {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"steam: shapes games={None if games is None else games.shape}, \"\n",
    "        f\"users={None if users is None else users.shape}, \"\n",
    "        f\"recs={None if recommendations is None else recommendations.shape}, \"\n",
    "        f\"meta={None if metadata is None else metadata.shape}\"\n",
    "    )\n",
    "\n",
    "    steam_table = None\n",
    "    if recommendations is not None:\n",
    "        if \"is_recommended\" in recommendations.columns:\n",
    "            recs_sample = stratified_sample(recommendations, y=\"is_recommended\", n_rows=n_rows, random_state=seed)\n",
    "        else:\n",
    "            recs_sample = simple_random_sample(recommendations, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "        games_plus = games\n",
    "        if (\n",
    "            metadata is not None\n",
    "            and games is not None\n",
    "            and \"app_id\" in metadata.columns\n",
    "            and \"app_id\" in games.columns\n",
    "        ):\n",
    "            print(\"steam: merge games with metadata\")\n",
    "            games_plus = games.merge(metadata, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "        steam_table = recs_sample\n",
    "        if games_plus is not None and \"app_id\" in recs_sample.columns and \"app_id\" in games_plus.columns:\n",
    "            print(\"steam: merge recommendations with games\")\n",
    "            steam_table = steam_table.merge(games_plus, on=\"app_id\", how=\"left\", suffixes=(\"\", \"_game\"))\n",
    "\n",
    "        if users is not None and \"user_id\" in steam_table.columns and \"user_id\" in users.columns:\n",
    "            print(\"steam: merge with users\")\n",
    "            steam_table = steam_table.merge(users, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\"))\n",
    "\n",
    "        steam_table = coerce_datetime_columns(steam_table)\n",
    "        print(f\"steam: done shape={None if steam_table is None else steam_table.shape}\")\n",
    "    else:\n",
    "        print(\"steam: skip because recommendations.csv is missing\")\n",
    "\n",
    "    return steam_table\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Olist Loader\n",
    "# =============================\n",
    "def load_olist_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"olist: start\")\n",
    "    if base_path is None:\n",
    "        print(\"olist: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    olist_customers = try_read_csv(base_path, \"olist_customers_dataset.csv\", low_memory=False)\n",
    "    olist_geolocation = try_read_csv(base_path, \"olist_geolocation_dataset.csv\", low_memory=False)\n",
    "    olist_items = try_read_csv(base_path, \"olist_order_items_dataset.csv\", low_memory=False)\n",
    "    olist_payments = try_read_csv(base_path, \"olist_order_payments_dataset.csv\", low_memory=False)\n",
    "    olist_reviews = try_read_csv(base_path, \"olist_order_reviews_dataset.csv\", low_memory=False)\n",
    "    olist_orders = try_read_csv(base_path, \"olist_orders_dataset.csv\", low_memory=False)\n",
    "    olist_products = try_read_csv(base_path, \"olist_products_dataset.csv\", low_memory=False)\n",
    "    olist_sellers = try_read_csv(base_path, \"olist_sellers_dataset.csv\", low_memory=False)\n",
    "    olist_cat_trans = try_read_csv(base_path, \"product_category_name_translation.csv\", low_memory=False)\n",
    "\n",
    "    print(\n",
    "        \"olist: shapes \"\n",
    "        f\"customers={None if olist_customers is None else olist_customers.shape}, \"\n",
    "        f\"geolocation={None if olist_geolocation is None else olist_geolocation.shape}, \"\n",
    "        f\"items={None if olist_items is None else olist_items.shape}, \"\n",
    "        f\"payments={None if olist_payments is None else olist_payments.shape}, \"\n",
    "        f\"reviews={None if olist_reviews is None else olist_reviews.shape}, \"\n",
    "        f\"orders={None if olist_orders is None else olist_orders.shape}, \"\n",
    "        f\"products={None if olist_products is None else olist_products.shape}, \"\n",
    "        f\"sellers={None if olist_sellers is None else olist_sellers.shape}, \"\n",
    "        f\"cat_trans={None if olist_cat_trans is None else olist_cat_trans.shape}\"\n",
    "    )\n",
    "\n",
    "    if not all(x is not None for x in [olist_orders, olist_items, olist_products, olist_sellers, olist_customers]):\n",
    "        print(\"olist: skip because core tables are missing\")\n",
    "        return None\n",
    "\n",
    "    print(\"olist: sample orders\")\n",
    "    orders_small = simple_random_sample(olist_orders, n_rows=min(n_rows, len(olist_orders)), random_state=seed)\n",
    "\n",
    "    print(\"olist: filter items for sampled orders\")\n",
    "    items_small = olist_items[olist_items[\"order_id\"].isin(orders_small[\"order_id\"])].copy()\n",
    "\n",
    "    if olist_cat_trans is not None and \"product_category_name\" in olist_products.columns:\n",
    "        print(\"olist: merge category translation\")\n",
    "        products_en = olist_products.merge(olist_cat_trans, on=\"product_category_name\", how=\"left\")\n",
    "    else:\n",
    "        products_en = olist_products\n",
    "\n",
    "    if olist_reviews is not None:\n",
    "        print(\"olist: build product review stats\")\n",
    "        product_reviews = (\n",
    "            items_small[[\"order_id\", \"product_id\"]]\n",
    "            .merge(olist_reviews[[\"order_id\", \"review_score\"]], on=\"order_id\", how=\"inner\")\n",
    "        )\n",
    "        product_reviews = product_reviews.drop_duplicates([\"order_id\", \"product_id\"])\n",
    "        product_stats = (\n",
    "            product_reviews.groupby(\"product_id\", as_index=False)\n",
    "            .agg(\n",
    "                review_count_product=(\"review_score\", \"count\"),\n",
    "                review_score_mean_product=(\"review_score\", \"mean\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_stats = None\n",
    "\n",
    "    print(\"olist: merge items, products, and sellers\")\n",
    "    items_ext = (\n",
    "        items_small.merge(products_en, on=\"product_id\", how=\"left\")\n",
    "        .merge(olist_sellers, on=\"seller_id\", how=\"left\", suffixes=(\"\", \"_seller\"))\n",
    "    )\n",
    "\n",
    "    if olist_geolocation is not None:\n",
    "        print(\"olist: build basic zip geo\")\n",
    "        geo_zip = (\n",
    "            olist_geolocation.groupby(\"geolocation_zip_code_prefix\", as_index=False).agg(\n",
    "                geolocation_lat=(\"geolocation_lat\", \"mean\"),\n",
    "                geolocation_lng=(\"geolocation_lng\", \"mean\"),\n",
    "                geo_points=(\"geolocation_city\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "        print(\"olist: merge customers with geo\")\n",
    "        customers_geo = (\n",
    "            olist_customers.merge(\n",
    "                geo_zip,\n",
    "                left_on=\"customer_zip_code_prefix\",\n",
    "                right_on=\"geolocation_zip_code_prefix\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(columns=[\"geolocation_zip_code_prefix\"])\n",
    "        )\n",
    "    else:\n",
    "        customers_geo = olist_customers\n",
    "\n",
    "    if olist_payments is not None:\n",
    "        print(\"olist: aggregate payments\")\n",
    "        payments_agg = (\n",
    "            olist_payments.groupby(\"order_id\", as_index=False).agg(\n",
    "                payment_value_total=(\"payment_value\", \"sum\"),\n",
    "                payment_installments_max=(\"payment_installments\", \"max\"),\n",
    "                payment_count=(\"payment_type\", \"count\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        payments_agg = None\n",
    "\n",
    "    print(\"olist: assemble main table\")\n",
    "    olist_full = (\n",
    "        orders_small.merge(customers_geo, on=\"customer_id\", how=\"left\")\n",
    "        .merge(items_ext, on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    if payments_agg is not None:\n",
    "        print(\"olist: merge payments\")\n",
    "        olist_full = olist_full.merge(payments_agg, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    if product_stats is not None:\n",
    "        print(\"olist: merge product stats\")\n",
    "        olist_full = olist_full.merge(product_stats, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    olist_full = coerce_datetime_columns(olist_full)\n",
    "\n",
    "    print(f\"olist: shape after assemble {olist_full.shape}\")\n",
    "    print(\"olist: done\")\n",
    "    return olist_full\n",
    "\n",
    "\n",
    "# =============================\n",
    "# VG2019 Loader\n",
    "# =============================\n",
    "def load_vg2019_dataset(base_path, n_rows=1_000_000, seed=42):\n",
    "    print(\"vg2019: start\")\n",
    "    if base_path is None:\n",
    "        print(\"vg2019: skip because base_path is None\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list_csvs(base_path)\n",
    "    pick = None\n",
    "    for f in csv_files:\n",
    "        if \"vgsales\" in f.lower():\n",
    "            pick = f\n",
    "            break\n",
    "    target_csv = pick if pick else (csv_files[0] if csv_files else None)\n",
    "\n",
    "    if target_csv is None:\n",
    "        print(\"vg2019: skip because no csv found\")\n",
    "        return None\n",
    "\n",
    "    full_path = os.path.join(base_path, target_csv)\n",
    "    try:\n",
    "        sales = pd.read_csv(full_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"vg2019: read error -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"vg2019: loaded {target_csv} with shape {sales.shape}\")\n",
    "\n",
    "    if \"Genre\" in sales.columns:\n",
    "        print(\"vg2019: stratified sample by Genre\")\n",
    "        sales = stratified_sample(sales, y=\"Genre\", n_rows=n_rows, random_state=seed)\n",
    "    else:\n",
    "        print(\"vg2019: simple random sample\")\n",
    "        sales = simple_random_sample(sales, n_rows=n_rows, random_state=seed)\n",
    "\n",
    "    print(f\"vg2019: done shape={sales.shape}\")\n",
    "    return sales\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Timer + memory helpers\n",
    "# =========================\n",
    "class SimpleTimer:\n",
    "    def __init__(self, enabled=True):\n",
    "        self.enabled = enabled\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "    def tick(self, label):\n",
    "        if not self.enabled:\n",
    "            return\n",
    "        t = time.perf_counter() - self.t0\n",
    "        print(f\"[timer] {label}: {t:.2f} s\")\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "\n",
    "def df_mem_gb(df):\n",
    "    try:\n",
    "        return float(df.memory_usage(deep=True).sum()) / (1024 ** 3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "def show_shape_mem(label, X_train=None, X_test=None):\n",
    "    parts = [label]\n",
    "    if X_train is not None:\n",
    "        parts.append(f\"X_train shape={tuple(X_train.shape)} mem={df_mem_gb(X_train):.3f} GB\")\n",
    "    if X_test is not None:\n",
    "        parts.append(f\"X_test shape={tuple(X_test.shape)} mem={df_mem_gb(X_test):.3f} GB\")\n",
    "    print(\"[info]\", \" | \".join(parts))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Text feature helpers (fast)\n",
    "# =========================\n",
    "def clean_keyword_name(s):\n",
    "    s = str(s).lower().strip().replace(\" \", \"_\")\n",
    "    keep = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum() or ch == \"_\":\n",
    "            keep.append(ch)\n",
    "    return \"\".join(keep)[:60]\n",
    "\n",
    "\n",
    "def text_features_fit(X, keyword_map):\n",
    "    keyword_map = keyword_map or {}\n",
    "    new_cols = []\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        if col not in X.columns:\n",
    "            continue\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"]:\n",
    "            continue\n",
    "        if col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            safe = clean_keyword_name(kw)\n",
    "            name = f\"{col}_has_{safe}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        df_part = pd.DataFrame(part, index=X.index)\n",
    "        new_parts.append(df_part)\n",
    "        new_cols.extend(df_part.columns.tolist())\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    return {\"new_cols\": new_cols, \"keyword_map\": keyword_map}\n",
    "\n",
    "\n",
    "def text_features_apply(X, text_info):\n",
    "    keyword_map = text_info.get(\"keyword_map\") or {}\n",
    "    new_parts = []\n",
    "\n",
    "    for col, keywords in keyword_map.items():\n",
    "        len_col = f\"{col}_len\"\n",
    "        wc_col = f\"{col}_wc\"\n",
    "\n",
    "        if col not in X.columns:\n",
    "            part = {\n",
    "                len_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "                wc_col: pd.Series(0, index=X.index, dtype=np.int64),\n",
    "            }\n",
    "            for kw in keywords:\n",
    "                name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "                part[name] = pd.Series(0, index=X.index, dtype=np.uint8)\n",
    "            new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "            continue\n",
    "\n",
    "        if str(X[col].dtype) not in [\"object\", \"category\"] or col == \"tags\":\n",
    "            continue\n",
    "\n",
    "        col_str = X[col].fillna(\"\").astype(str).str.lower()\n",
    "        part = {\n",
    "            len_col: col_str.str.len(),\n",
    "            wc_col: col_str.str.split().apply(len),\n",
    "        }\n",
    "        for kw in keywords:\n",
    "            name = f\"{col}_has_{clean_keyword_name(kw)}\"\n",
    "            part[name] = col_str.str.contains(str(kw).lower(), regex=False).astype(np.uint8)\n",
    "\n",
    "        new_parts.append(pd.DataFrame(part, index=X.index))\n",
    "\n",
    "    if new_parts:\n",
    "        X_new = pd.concat(new_parts, axis=1)\n",
    "        X = pd.concat([X, X_new], axis=1)\n",
    "\n",
    "    for c in text_info.get(\"new_cols\", []):\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_text_length_features_inplace(X, exclude_cols=None):\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    obj_cols = [c for c in X.columns if str(X[c].dtype) in [\"object\", \"category\"] and c not in exclude_cols]\n",
    "    for c in obj_cols:\n",
    "        X[f\"{c}_length\"] = X[c].fillna(\"\").astype(str).str.len().astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# General helpers\n",
    "# =========================\n",
    "def datetimes_to_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        if np.issubdtype(X[c].dtype, np.datetime64):\n",
    "            mask = X[c].isna()\n",
    "            vals_int = X[c].values.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "            arr = vals_int.astype(\"float64\") / 1000000000.0\n",
    "            if mask.any():\n",
    "                arr[mask.values] = np.nan\n",
    "            X[c] = arr\n",
    "    return X\n",
    "\n",
    "\n",
    "def downcast_numeric_inplace(X):\n",
    "    for c in X.columns:\n",
    "        dt = X[c].dtype\n",
    "        if np.issubdtype(dt, np.floating):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "        elif np.issubdtype(dt, np.integer) and X[c].nunique(dropna=True) > 2:\n",
    "            X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def scale_fit(method, X_train_num):\n",
    "    if method == \"standard\":\n",
    "        return StandardScaler().fit(X_train_num)\n",
    "    if method == \"minmax\":\n",
    "        return MinMaxScaler().fit(X_train_num)\n",
    "    return None\n",
    "\n",
    "\n",
    "def scale_numeric_only(X_train, X_test, scale_method):\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        return X_train, X_test\n",
    "    scaler = scale_fit(scale_method, X_train[num_cols])\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols]).astype(\"float32\")\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols]).astype(\"float32\")\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Safer OHE with caps\n",
    "# =========================\n",
    "def _auto_exclude_mask(series, max_unique=500, max_avg_len=25):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    nunq = int(s.nunique(dropna=False))\n",
    "    avg_len = float(s.map(len).mean())\n",
    "    return (nunq > max_unique) or (avg_len > max_avg_len and nunq > 50)\n",
    "\n",
    "\n",
    "def _cap_categories(series, top_k=100, min_freq=1, other_label=\"Other\"):\n",
    "    s = series.fillna(\"Unknown\").astype(str)\n",
    "    vc = s.value_counts()\n",
    "    kept = vc[vc >= min_freq].index.tolist()\n",
    "    if top_k is not None and len(kept) > top_k:\n",
    "        kept = vc.index[:top_k].tolist()\n",
    "    mapped = s.where(s.isin(kept), other_label)\n",
    "    return mapped.astype(\"category\"), kept\n",
    "\n",
    "\n",
    "def ohe_fit(\n",
    "    X,\n",
    "    exclude_cols=None,\n",
    "    top_k_per_col=100,\n",
    "    min_freq_per_col=1,\n",
    "    auto_exclude=False,\n",
    "    high_card_threshold=500,\n",
    "    long_text_avglen=25,\n",
    "):\n",
    "    exclude = set(exclude_cols or [])\n",
    "    value_map = {}\n",
    "\n",
    "    X_tmp = X.copy()\n",
    "    obj_cols = [c for c in X_tmp.select_dtypes(include=[\"object\", \"category\"]).columns]\n",
    "    excluded = list(exclude)\n",
    "\n",
    "    for c in obj_cols:\n",
    "        if c in exclude:\n",
    "            continue\n",
    "        s = X_tmp[c]\n",
    "        if auto_exclude:\n",
    "            if _auto_exclude_mask(s, max_unique=high_card_threshold, max_avg_len=long_text_avglen):\n",
    "                excluded.append(c)\n",
    "                continue\n",
    "        capped, kept = _cap_categories(s, top_k=top_k_per_col, min_freq=min_freq_per_col)\n",
    "        X_tmp[c] = capped\n",
    "        value_map[c] = kept\n",
    "\n",
    "    X_tmp = X_tmp.drop(columns=excluded, errors=\"ignore\")\n",
    "    obj_cols_final = [c for c in obj_cols if c not in excluded]\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=obj_cols_final, dummy_na=False)\n",
    "    schema_cols = X_ohe.columns.tolist()\n",
    "\n",
    "    return {\n",
    "        \"obj_cols\": obj_cols_final,\n",
    "        \"schema_cols\": schema_cols,\n",
    "        \"value_map\": value_map,\n",
    "        \"excluded\": excluded,\n",
    "        \"other_label\": \"Other\",\n",
    "    }\n",
    "\n",
    "\n",
    "def ohe_apply(X, ohe_info):\n",
    "    obj_cols = ohe_info[\"obj_cols\"]\n",
    "    schema_cols = ohe_info[\"schema_cols\"]\n",
    "    value_map = ohe_info[\"value_map\"]\n",
    "    other = ohe_info.get(\"other_label\", \"Other\")\n",
    "    excluded = ohe_info.get(\"excluded\", [])\n",
    "\n",
    "    X_tmp = X.drop(columns=excluded, errors=\"ignore\").copy()\n",
    "\n",
    "    for c in obj_cols:\n",
    "        if c in X_tmp.columns:\n",
    "            s = X_tmp[c].fillna(\"Unknown\").astype(str)\n",
    "            kept = set(value_map.get(c, []))\n",
    "            s = s.where(s.isin(kept), other).astype(\"category\")\n",
    "            X_tmp[c] = s\n",
    "\n",
    "    X_ohe = pd.get_dummies(X_tmp, columns=[c for c in obj_cols if c in X_tmp.columns], dummy_na=False)\n",
    "    X_ohe = X_ohe.reindex(columns=schema_cols, fill_value=0)\n",
    "    return X_ohe\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Outliers\n",
    "# =========================\n",
    "def outlier_bounds_fit(X_num, lower_q=0.025, upper_q=0.975, exclude_binary=True, sample_rows=200000):\n",
    "    bounds = {}\n",
    "    if lower_q is None or upper_q is None:\n",
    "        return bounds\n",
    "    X_use = X_num\n",
    "    if len(X_num) > sample_rows:\n",
    "        X_use = X_num.sample(n=sample_rows, random_state=123)\n",
    "    for c in X_use.columns:\n",
    "        vals = X_use[c].astype(\"float32\")\n",
    "        if exclude_binary and X_use[c].nunique(dropna=True) <= 2:\n",
    "            continue\n",
    "        lo = np.nanquantile(vals, lower_q)\n",
    "        hi = np.nanquantile(vals, upper_q)\n",
    "        if np.isfinite(lo) and np.isfinite(hi) and hi >= lo:\n",
    "            bounds[c] = (float(lo), float(hi))\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def outlier_clip_inplace(X, bounds):\n",
    "    if not bounds:\n",
    "        return X\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].astype(\"float32\").clip(lo, hi)\n",
    "    return X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature selection\n",
    "# =========================\n",
    "def forward_feature_selection(X, y, model,\n",
    "                              scoring=\"neg_mean_absolute_error\",\n",
    "                              cv=5, tol=None, max_features=None, n_jobs=-1, verbose=False):\n",
    "    try:\n",
    "        feature_names = list(X.columns)\n",
    "        X_arr = X.values\n",
    "    except AttributeError:\n",
    "        X_arr = X\n",
    "        feature_names = [f\"f{i}\" for i in range(X_arr.shape[1])]\n",
    "\n",
    "    selected_idx = []\n",
    "    remaining_idx = list(range(X_arr.shape[1]))\n",
    "    best_scores = []\n",
    "    previous_score = float(\"inf\")\n",
    "    best_feature_set_idx = []\n",
    "    best_score = float(\"inf\")\n",
    "\n",
    "    while remaining_idx:\n",
    "        scores = {}\n",
    "        for idx in remaining_idx:\n",
    "            trial_idx = selected_idx + [idx]\n",
    "            cv_score = -cross_val_score(\n",
    "                model, X_arr[:, trial_idx], y,\n",
    "                scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "            ).mean()\n",
    "            scores[idx] = cv_score\n",
    "\n",
    "        best_idx = min(scores, key=scores.get)\n",
    "        current_score = scores[best_idx]\n",
    "\n",
    "        if tol is not None and previous_score - current_score < tol:\n",
    "            if verbose:\n",
    "                print(\"Stopping early (improvement < tol).\")\n",
    "            break\n",
    "\n",
    "        selected_idx.append(best_idx)\n",
    "        remaining_idx.remove(best_idx)\n",
    "        best_scores.append(current_score)\n",
    "        previous_score = current_score\n",
    "\n",
    "        if verbose:\n",
    "            name = feature_names[best_idx]\n",
    "            print(f\"Added {name} -> CV score = {current_score:.4f}\")\n",
    "\n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_feature_set_idx = selected_idx.copy()\n",
    "\n",
    "        if max_features is not None and len(selected_idx) >= max_features:\n",
    "            break\n",
    "\n",
    "    selected_features = [feature_names[i] for i in selected_idx]\n",
    "    best_feature_set = [feature_names[i] for i in best_feature_set_idx]\n",
    "\n",
    "    if not best_feature_set:\n",
    "        best_feature_set = selected_features[:]\n",
    "        best_score = best_scores[-1] if best_scores else float(\"inf\")\n",
    "\n",
    "    if verbose:\n",
    "        try:\n",
    "            index = np.argmax(np.array(selected_features) == best_feature_set[-1])\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(best_scores) + 1), best_scores, marker=\".\")\n",
    "            plt.plot([index + 1], [best_score], marker=\"x\")\n",
    "            plt.xticks(range(1, len(selected_features) + 1),\n",
    "                       selected_features, rotation=60, ha=\"right\", fontsize=6)\n",
    "            plt.title(\"Forward Feature Selection and CV Scores\")\n",
    "            plt.xlabel(\"Features Added\")\n",
    "            plt.ylabel(\"CV Score (MAE)\")\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        print(f\"Best Features: {best_feature_set}\")\n",
    "        print(f\"Best CV MAE Score: {best_score:.4f}\")\n",
    "    return selected_features, best_scores, best_feature_set, best_score\n",
    "\n",
    "\n",
    "def select_features(method, max_features, task_type, random_state, X_train, y_train, verbose=False):\n",
    "    if method is None or method == \"none\":\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    k = min(max_features, X_train.shape[1]) if max_features else X_train.shape[1]\n",
    "    if k < 1:\n",
    "        return X_train.columns.tolist()\n",
    "\n",
    "    if method == \"tree\":\n",
    "        est = RandomForestClassifier(random_state=random_state) if task_type == \"classification\" else RandomForestRegressor(random_state=random_state)\n",
    "        est.fit(X_train, y_train)\n",
    "        imp = pd.Series(est.feature_importances_, index=X_train.columns)\n",
    "        return imp.nlargest(k).index.tolist()\n",
    "\n",
    "    if method == \"forward\":\n",
    "        if task_type != \"regression\":\n",
    "            raise ValueError(\"Forward selection is only supported for regression tasks.\")\n",
    "        model = RandomForestRegressor(random_state=random_state)\n",
    "        _, _, best_set, _ = forward_feature_selection(\n",
    "            X=X_train, y=y_train, model=model,\n",
    "            scoring=\"neg_mean_absolute_error\", cv=3,\n",
    "            tol=None, max_features=max_features, n_jobs=-1, verbose=verbose\n",
    "        )\n",
    "        return best_set\n",
    "\n",
    "    if method == \"mutual_info\":\n",
    "        sel = SelectKBest(mutual_info_classif if task_type == \"classification\" else mutual_info_regression, k=k)\n",
    "        sel.fit(X_train, y_train)\n",
    "        return X_train.columns[sel.get_support()].tolist()\n",
    "\n",
    "    return X_train.columns.tolist()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Batched poly features\n",
    "# =========================\n",
    "def add_poly_features_batched(X_train, X_test, squares, pairs):\n",
    "    train_parts = {}\n",
    "    for c in squares:\n",
    "        if c in X_train.columns:\n",
    "            train_parts[f\"{c}_sq\"] = X_train[c].astype(\"float32\") ** 2\n",
    "    for a, b in pairs:\n",
    "        if (a in X_train.columns) and (b in X_train.columns):\n",
    "            name = f\"{a}_x_{b}\"\n",
    "            train_parts[name] = (X_train[a].astype(\"float32\") * X_train[b].astype(\"float32\"))\n",
    "\n",
    "    test_parts = {}\n",
    "    for name in train_parts:\n",
    "        if name.endswith(\"_sq\"):\n",
    "            c = name[:-3]\n",
    "            if c in X_test.columns:\n",
    "                test_parts[name] = X_test[c].astype(\"float32\") ** 2\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "        else:\n",
    "            a, b = name.split(\"_x_\")\n",
    "            if (a in X_test.columns) and (b in X_test.columns):\n",
    "                test_parts[name] = (X_test[a].astype(\"float32\") * X_test[b].astype(\"float32\"))\n",
    "            else:\n",
    "                test_parts[name] = pd.Series(0.0, index=X_test.index, dtype=\"float32\")\n",
    "\n",
    "    if train_parts:\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(train_parts, index=X_train.index)], axis=1)\n",
    "    if test_parts:\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(test_parts, index=X_test.index)], axis=1)\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train.copy())\n",
    "    X_test = downcast_numeric_inplace(X_test.copy())\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main prep (with best hyperparameters per dataset)\n",
    "# =========================\n",
    "def prepare_data(\n",
    "    steam_df,\n",
    "    olist_df,\n",
    "    sales_df,\n",
    "    test_size,\n",
    "    random_state,\n",
    "    feature_selection=None,\n",
    "    max_features=None,\n",
    "    task_type=\"classification\",\n",
    "    scale_method=None,\n",
    "    use_best_from_sweep=True,\n",
    "    verbose=False,\n",
    "):\n",
    "    outputs = {}\n",
    "    timer = SimpleTimer(enabled=verbose)\n",
    "\n",
    "    if use_best_from_sweep:\n",
    "        steam_hp = {\n",
    "            \"cutoff\": 95.0,\n",
    "            \"feature_selection\": \"mutual_info\",\n",
    "            \"max_features\": 50,\n",
    "            \"scale_method\": None,\n",
    "            \"ohe_auto_exclude\": False,\n",
    "            \"tag_min_count\": 5,\n",
    "            \"tag_top_k\": 200,\n",
    "            \"ohe_top_k_per_col\": 400,\n",
    "            \"ohe_min_freq_per_col\": 5,\n",
    "            \"outlier_lower_q\": 0.05,\n",
    "            \"outlier_upper_q\": 0.95,\n",
    "            \"ohe_high_card_threshold\": 300,\n",
    "            \"ohe_long_text_avglen\": 25,\n",
    "        }\n",
    "        olist_hp = {\n",
    "            \"cutoff\": 4.0,\n",
    "            \"feature_selection\": \"tree\",\n",
    "            \"max_features\": 10,\n",
    "            \"scale_method\": None,\n",
    "            \"ohe_auto_exclude\": True,\n",
    "            \"tag_min_count\": 3,\n",
    "            \"tag_top_k\": 400,\n",
    "            \"ohe_top_k_per_col\": 400,\n",
    "            \"ohe_min_freq_per_col\": 1,\n",
    "            \"outlier_lower_q\": 0.01,\n",
    "            \"outlier_upper_q\": 0.99,\n",
    "            \"ohe_high_card_threshold\": 800,\n",
    "            \"ohe_long_text_avglen\": 35,\n",
    "        }\n",
    "        sales_hp = {\n",
    "            \"cutoff\": 5.0,\n",
    "            \"feature_selection\": \"mutual_info\",\n",
    "            \"max_features\": None,\n",
    "            \"scale_method\": None,\n",
    "            \"ohe_auto_exclude\": True,\n",
    "            \"tag_min_count\": 10,\n",
    "            \"tag_top_k\": 100,\n",
    "            \"ohe_top_k_per_col\": 200,\n",
    "            \"ohe_min_freq_per_col\": 5,\n",
    "            \"outlier_lower_q\": 0.01,\n",
    "            \"outlier_upper_q\": 0.99,\n",
    "            \"ohe_high_card_threshold\": 800,\n",
    "            \"ohe_long_text_avglen\": 20,\n",
    "        }\n",
    "    else:\n",
    "        steam_hp = {\n",
    "            \"cutoff\": 50.0,\n",
    "            \"feature_selection\": (feature_selection or \"none\").lower(),\n",
    "            \"max_features\": max_features,\n",
    "            \"scale_method\": scale_method,\n",
    "            \"ohe_auto_exclude\": False,\n",
    "            \"tag_min_count\": 5,\n",
    "            \"tag_top_k\": 200,\n",
    "            \"ohe_top_k_per_col\": 100,\n",
    "            \"ohe_min_freq_per_col\": 1,\n",
    "            \"outlier_lower_q\": 0.025,\n",
    "            \"outlier_upper_q\": 0.975,\n",
    "            \"ohe_high_card_threshold\": 500,\n",
    "            \"ohe_long_text_avglen\": 25,\n",
    "        }\n",
    "        olist_hp = steam_hp.copy()\n",
    "        olist_hp[\"cutoff\"] = 2.5\n",
    "        sales_hp = steam_hp.copy()\n",
    "        sales_hp[\"cutoff\"] = 5.0\n",
    "\n",
    "    # ---------- STEAM ----------\n",
    "    steam = steam_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        steam[\"target\"] = (steam[\"positive_ratio\"] >= float(steam_hp[\"cutoff\"])).astype(int)\n",
    "    else:\n",
    "        steam[\"target\"] = steam[\"positive_ratio\"]\n",
    "    steam.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    steam.drop(columns=[\"app_id\", \"user_id\", \"review_id\", \"positive_ratio\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    if {\"date\", \"date_release\"}.issubset(steam.columns):\n",
    "        steam[\"days_since_release\"] = (steam[\"date\"] - steam[\"date_release\"]).dt.days\n",
    "\n",
    "    for col in [\"is_recommended\", \"mac\", \"linux\", \"win\", \"steam_deck\"]:\n",
    "        if col in steam.columns:\n",
    "            steam[col] = steam[col].astype(int)\n",
    "\n",
    "    if \"hours\" in steam.columns:\n",
    "        steam[\"log_hours\"] = np.log1p(steam[\"hours\"])\n",
    "    if {\"hours\", \"user_reviews\"}.issubset(steam.columns):\n",
    "        steam[\"reviews_per_hour\"] = steam[\"user_reviews\"] / (steam[\"hours\"] + 0.000000001)\n",
    "\n",
    "    X = steam.drop(columns=[\"target\", \"rating\"], errors=\"ignore\")\n",
    "    y = steam[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"steam split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"steam datetime to numeric\")\n",
    "\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test, exclude_cols=[\"tags\"])\n",
    "\n",
    "    steam_kw = {\n",
    "        \"title\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"coop\", \"online\", \"free\", \"demo\", \"survival\"],\n",
    "        \"description\": [\"vr\", \"dlc\", \"multiplayer\", \"co-op\", \"open world\", \"story\", \"puzzle\", \"horror\", \"early access\"],\n",
    "    }\n",
    "    steam_text_info = text_features_fit(X_train, steam_kw)\n",
    "    X_test = text_features_apply(X_test, steam_text_info)\n",
    "    if verbose:\n",
    "        timer.tick(\"steam text features\")\n",
    "        show_shape_mem(\"steam after text\", X_train, X_test)\n",
    "\n",
    "    if \"tags\" in X_train.columns:\n",
    "        from collections import Counter\n",
    "\n",
    "        def tag_col_name(t):\n",
    "            s = str(t).lower().strip().replace(\" \", \"_\")\n",
    "            return \"tag_\" + \"\".join(ch for ch in s if ch.isalnum() or ch == \"_\")[:60]\n",
    "\n",
    "        cnt = Counter()\n",
    "        for v in X_train[\"tags\"].fillna(\"\").values:\n",
    "            lst = v if isinstance(v, list) else []\n",
    "            for t in lst:\n",
    "                cnt[t] += 1\n",
    "\n",
    "        items = [(t, n) for t, n in cnt.items() if n >= steam_hp[\"tag_min_count\"]]\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        vocab = [t for t, _ in items[:steam_hp[\"tag_top_k\"]]]\n",
    "        tag_cols = [tag_col_name(t) for t in vocab]\n",
    "        if verbose:\n",
    "            print(f\"[info] steam tags unique={len(cnt)}, kept={len(vocab)} (min_count={steam_hp['tag_min_count']}, top_k={steam_hp['tag_top_k']})\")\n",
    "\n",
    "        def add_tag_cols_fast(df):\n",
    "            if \"tags\" in df.columns:\n",
    "                tag_lists = df[\"tags\"].apply(lambda v: v if isinstance(v, list) else [])\n",
    "            else:\n",
    "                tag_lists = pd.Series([[]] * len(df), index=df.index)\n",
    "            new_data = {}\n",
    "            for tag, col_name in zip(vocab, tag_cols):\n",
    "                new_data[col_name] = np.fromiter(\n",
    "                    (1 if tag in lst else 0 for lst in tag_lists),\n",
    "                    dtype=np.uint8,\n",
    "                    count=len(df)\n",
    "                )\n",
    "            new_df = pd.DataFrame(new_data, index=df.index)\n",
    "            return pd.concat([df.drop(columns=[\"tags\"], errors=\"ignore\"), new_df], axis=1)\n",
    "\n",
    "        X_train = add_tag_cols_fast(X_train)\n",
    "        X_test = add_tag_cols_fast(X_test)\n",
    "        if verbose:\n",
    "            timer.tick(\"steam tags multi-hot\")\n",
    "            show_shape_mem(\"steam after tags\", X_train, X_test)\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=steam_hp[\"ohe_top_k_per_col\"],\n",
    "        min_freq_per_col=steam_hp[\"ohe_min_freq_per_col\"],\n",
    "        auto_exclude=steam_hp[\"ohe_auto_exclude\"],\n",
    "        high_card_threshold=steam_hp[\"ohe_high_card_threshold\"],\n",
    "        long_text_avglen=steam_hp[\"ohe_long_text_avglen\"],\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"steam OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "    if verbose:\n",
    "        print(\"steam non-numeric after OHE:\", X_train.select_dtypes(exclude=[\"number\"]).columns.tolist())\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"steam impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    steam_squares = []\n",
    "    if \"log_hours\" in X_train.columns:\n",
    "        steam_squares.append(\"log_hours\")\n",
    "    elif \"hours\" in X_train.columns:\n",
    "        steam_squares.append(\"hours\")\n",
    "    if \"discount\" in X_train.columns:\n",
    "        steam_squares.append(\"discount\")\n",
    "    if \"days_since_release\" in X_train.columns:\n",
    "        steam_squares.append(\"days_since_release\")\n",
    "\n",
    "    steam_pairs = []\n",
    "    if (\"price_final\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_final\", \"discount\"))\n",
    "    if (\"price_original\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"price_original\", \"discount\"))\n",
    "    if (\"days_since_release\" in X_train.columns) and (\"discount\" in X_train.columns):\n",
    "        steam_pairs.append((\"days_since_release\", \"discount\"))\n",
    "    if (\"user_reviews\" in X_train.columns) and (\"reviews\" in X_train.columns):\n",
    "        steam_pairs.append((\"user_reviews\", \"reviews\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, steam_squares, steam_pairs)\n",
    "    timer.tick(\"steam poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=steam_hp[\"outlier_lower_q\"],\n",
    "        upper_q=steam_hp[\"outlier_upper_q\"],\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test = outlier_clip_inplace(X_test, bounds)\n",
    "    timer.tick(\"steam outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, steam_hp[\"scale_method\"])\n",
    "    timer.tick(\"steam scale\")\n",
    "\n",
    "    keep_cols = select_features(\n",
    "        steam_hp[\"feature_selection\"],\n",
    "        steam_hp[\"max_features\"],\n",
    "        task_type,\n",
    "        random_state,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"steam select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"steam after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"steam\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- OLIST ----------\n",
    "    olist = olist_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        olist[\"target\"] = (olist[\"review_score_mean_product\"] >= float(olist_hp[\"cutoff\"])).astype(int)\n",
    "    else:\n",
    "        olist[\"target\"] = olist[\"review_score_mean_product\"]\n",
    "    olist.dropna(subset=[\"target\"], inplace=True)\n",
    "    olist.drop(columns=[\"order_id\", \"customer_id\", \"customer_unique_id\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    olist[\"delivery_delay\"] = (olist[\"order_estimated_delivery_date\"] - olist[\"order_purchase_timestamp\"]).dt.days\n",
    "    denom = olist[\"payment_installments_max\"].replace(0, 1)\n",
    "    olist[\"avg_installment\"] = olist[\"payment_value_total\"] / denom\n",
    "\n",
    "    if {\"product_length_cm\", \"product_width_cm\", \"product_height_cm\"}.issubset(olist.columns):\n",
    "        olist[\"product_volume_cm3\"] = (\n",
    "            olist[\"product_length_cm\"] * olist[\"product_width_cm\"] * olist[\"product_height_cm\"]\n",
    "        )\n",
    "\n",
    "    X = olist.drop(columns=[\"review_score_mean_product\", \"target\"], errors=\"ignore\")\n",
    "    y = olist[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"olist split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"olist datetime to numeric\")\n",
    "\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test, exclude_cols=[\"tags\"])\n",
    "\n",
    "    olist_kw = {\n",
    "        \"order_status\": [\"delivered\", \"shipped\", \"canceled\", \"invoiced\", \"processing\"],\n",
    "        \"product_category_name\": [\"moveis\", \"auto\", \"pet\", \"perfumaria\", \"utilidades\", \"brinquedos\"]\n",
    "    }\n",
    "    olist_text_info = text_features_fit(X_train, olist_kw)\n",
    "    X_test = text_features_apply(X_test, olist_text_info)\n",
    "    timer.tick(\"olist text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=olist_hp[\"ohe_top_k_per_col\"],\n",
    "        min_freq_per_col=olist_hp[\"ohe_min_freq_per_col\"],\n",
    "        auto_exclude=olist_hp[\"ohe_auto_exclude\"],\n",
    "        high_card_threshold=olist_hp[\"ohe_high_card_threshold\"],\n",
    "        long_text_avglen=olist_hp[\"ohe_long_text_avglen\"],\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"olist OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"olist impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    olist_squares = []\n",
    "    if \"delivery_delay\" in X_train.columns:\n",
    "        olist_squares.append(\"delivery_delay\")\n",
    "    if \"price\" in X_train.columns:\n",
    "        olist_squares.append(\"price\")\n",
    "    if \"freight_value\" in X_train.columns:\n",
    "        olist_squares.append(\"freight_value\")\n",
    "\n",
    "    olist_pairs = []\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_weight_g\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_weight_g\"))\n",
    "    if (\"freight_value\" in X_train.columns) and (\"product_volume_cm3\" in X_train.columns):\n",
    "        olist_pairs.append((\"freight_value\", \"product_volume_cm3\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"price\"))\n",
    "    if (\"delivery_delay\" in X_train.columns) and (\"freight_value\" in X_train.columns):\n",
    "        olist_pairs.append((\"delivery_delay\", \"freight_value\"))\n",
    "    if (\"payment_installments_max\" in X_train.columns) and (\"price\" in X_train.columns):\n",
    "        olist_pairs.append((\"payment_installments_max\", \"price\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, olist_squares, olist_pairs)\n",
    "    timer.tick(\"olist poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=olist_hp[\"outlier_lower_q\"],\n",
    "        upper_q=olist_hp[\"outlier_upper_q\"],\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test = outlier_clip_inplace(X_test, bounds)\n",
    "    timer.tick(\"olist outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, olist_hp[\"scale_method\"])\n",
    "    timer.tick(\"olist scale\")\n",
    "\n",
    "    keep_cols = select_features(\n",
    "        olist_hp[\"feature_selection\"],\n",
    "        olist_hp[\"max_features\"],\n",
    "        task_type,\n",
    "        random_state,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"olist select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"olist after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"olist\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ---------- SALES ----------\n",
    "    sales = sales_df.copy()\n",
    "    if task_type == \"classification\":\n",
    "        sales[\"target\"] = (sales[\"Critic_Score\"] >= float(sales_hp[\"cutoff\"])).astype(int)\n",
    "    else:\n",
    "        sales[\"target\"] = sales[\"Critic_Score\"]\n",
    "    sales.dropna(subset=[\"target\"], inplace=True)\n",
    "\n",
    "    for c in [\"ESRB_Rating\", \"Genre\", \"Platform\", \"Publisher\", \"Developer\"]:\n",
    "        if c in sales.columns:\n",
    "            sales[c] = sales[c].fillna(\"Unknown\")\n",
    "\n",
    "    X = sales.drop(columns=[\"target\", \"Critic_Score\"], errors=\"ignore\")\n",
    "    y = sales[\"target\"]\n",
    "    strat = y if task_type == \"classification\" else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=strat\n",
    "    )\n",
    "    timer.tick(\"sales split\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales post split\", X_train, X_test)\n",
    "\n",
    "    datetimes_to_numeric_inplace(X_train)\n",
    "    datetimes_to_numeric_inplace(X_test)\n",
    "    timer.tick(\"sales datetime to numeric\")\n",
    "\n",
    "    add_text_length_features_inplace(X_train, exclude_cols=[\"tags\"])\n",
    "    add_text_length_features_inplace(X_test, exclude_cols=[\"tags\"])\n",
    "\n",
    "    sales_kw = {\n",
    "        \"Name\": [\"mario\", \"pokemon\", \"zelda\", \"call of duty\", \"fifa\", \"minecraft\", \"final fantasy\"],\n",
    "        \"Genre\": [\"action\", \"sports\", \"shooter\", \"racing\", \"role\", \"adventure\", \"platform\", \"puzzle\"],\n",
    "        \"Publisher\": [\"nintendo\", \"electronic arts\", \"ea\", \"activision\", \"ubisoft\", \"sony\", \"sega\"],\n",
    "        \"ESRB_Rating\": [\"e\", \"t\", \"m\"]\n",
    "    }\n",
    "    sales_text_info = text_features_fit(X_train, sales_kw)\n",
    "    X_test = text_features_apply(X_test, sales_text_info)\n",
    "    timer.tick(\"sales text features\")\n",
    "\n",
    "    ohe_info = ohe_fit(\n",
    "        X_train,\n",
    "        exclude_cols=[],\n",
    "        top_k_per_col=sales_hp[\"ohe_top_k_per_col\"],\n",
    "        min_freq_per_col=sales_hp[\"ohe_min_freq_per_col\"],\n",
    "        auto_exclude=sales_hp[\"ohe_auto_exclude\"],\n",
    "        high_card_threshold=sales_hp[\"ohe_high_card_threshold\"],\n",
    "        long_text_avglen=sales_hp[\"ohe_long_text_avglen\"],\n",
    "    )\n",
    "    X_train = ohe_apply(X_train, ohe_info)\n",
    "    X_test = ohe_apply(X_test, ohe_info)\n",
    "    timer.tick(\"sales OHE\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after OHE\", X_train, X_test)\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[\"number\"]).copy()\n",
    "    X_test = X_test.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    if num_cols:\n",
    "        simp = SimpleImputer(strategy=\"median\").fit(X_train[num_cols])\n",
    "        X_train[num_cols] = simp.transform(X_train[num_cols])\n",
    "        X_test[num_cols] = simp.transform(X_test[num_cols])\n",
    "    timer.tick(\"sales impute\")\n",
    "\n",
    "    X_train = downcast_numeric_inplace(X_train)\n",
    "    X_test = downcast_numeric_inplace(X_test)\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after impute+downcast\", X_train, X_test)\n",
    "\n",
    "    sales_squares = []\n",
    "    if \"Year\" in X_train.columns:\n",
    "        sales_squares.append(\"Year\")\n",
    "    if \"User_Score\" in X_train.columns:\n",
    "        sales_squares.append(\"User_Score\")\n",
    "\n",
    "    sales_pairs = []\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"PAL_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"PAL_Sales\"))\n",
    "    if (\"NA_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"NA_Sales\", \"JP_Sales\"))\n",
    "    if (\"PAL_Sales\" in X_train.columns) and (\"JP_Sales\" in X_train.columns):\n",
    "        sales_pairs.append((\"PAL_Sales\", \"JP_Sales\"))\n",
    "\n",
    "    X_train, X_test = add_poly_features_batched(X_train, X_test, sales_squares, sales_pairs)\n",
    "    timer.tick(\"sales poly\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after poly\", X_train, X_test)\n",
    "\n",
    "    num_cols2 = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    bounds = outlier_bounds_fit(\n",
    "        X_train[num_cols2],\n",
    "        lower_q=sales_hp[\"outlier_lower_q\"],\n",
    "        upper_q=sales_hp[\"outlier_upper_q\"],\n",
    "        exclude_binary=True,\n",
    "        sample_rows=200000\n",
    "    )\n",
    "    X_train = outlier_clip_inplace(X_train, bounds)\n",
    "    X_test = outlier_clip_inplace(X_test, bounds)\n",
    "    timer.tick(\"sales outlier clip\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after outlier clip\", X_train, X_test)\n",
    "\n",
    "    X_train, X_test = scale_numeric_only(X_train, X_test, sales_hp[\"scale_method\"])\n",
    "    timer.tick(\"sales scale\")\n",
    "\n",
    "    keep_cols = select_features(\n",
    "        sales_hp[\"feature_selection\"],\n",
    "        sales_hp[\"max_features\"],\n",
    "        task_type,\n",
    "        random_state,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    X_train = X_train[keep_cols]\n",
    "    X_test = X_test[keep_cols]\n",
    "    timer.tick(\"sales select features\")\n",
    "    if verbose:\n",
    "        show_shape_mem(\"sales after select\", X_train, X_test)\n",
    "\n",
    "    outputs[\"sales\"] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    if verbose:\n",
    "        for name, parts in outputs.items():\n",
    "            Xtr, Xte, ytr, yte = parts\n",
    "            print(f\"[{name}] X_train: {Xtr.shape} | X_test: {Xte.shape} | y_train: {ytr.shape} | y_test: {yte.shape}\")\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Download Paths\n",
    "steam_path = safe_kaggle_download(\"antonkozyriev/game-recommendations-on-steam\")\n",
    "olist_path = safe_kaggle_download(\"olistbr/brazilian-ecommerce\")\n",
    "vg2019_path = safe_kaggle_download(\"ashaheedq/video-games-sales-2019\")\n",
    "\n",
    "# Load All\n",
    "start_total = time.perf_counter()\n",
    "steam = load_steam_dataset(steam_path, n_rows=N_ROWS, seed=random_state)\n",
    "olist = load_olist_dataset(olist_path, n_rows=N_ROWS, seed=random_state)\n",
    "sales = load_vg2019_dataset(vg2019_path, n_rows=N_ROWS, seed=random_state)\n",
    "end_total = time.perf_counter()\n",
    "print(f\"main: load all done in {round(end_total - start_total, 3)} sec ({format_hms(end_total - start_total)})\")\n",
    "\n",
    "# Download Shapes\n",
    "print(\"download: shapes summary\")\n",
    "print(f\"download: steam shape = {None if steam is None else steam.shape}\")\n",
    "print(f\"download: olist shape = {None if olist is None else olist.shape}\")\n",
    "print(f\"download: sales shape = {None if sales is None else sales.shape}\")\n",
    "\n",
    "# Prepare Data\n",
    "outputs = prepare_data(\n",
    "    steam_df=steam,\n",
    "    olist_df=olist,\n",
    "    sales_df=sales,\n",
    "    test_size=0.2,\n",
    "    random_state=random_state,\n",
    "    scale_method=None,\n",
    "    use_best_from_sweep=True,\n",
    "    verbose=False,\n",
    "    task_type=\"classification\"\n",
    ")\n",
    "\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = outputs[\"steam\"]\n",
    "X_train_olist, X_test_olist, y_train_olist, y_test_olist = outputs[\"olist\"]\n",
    "X_train_sales, X_test_sales, y_train_sales, y_test_sales = outputs[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c6a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    from scipy import sparse\n",
    "except ImportError:\n",
    "    sparse = None\n",
    "\n",
    "\n",
    "def search_best_naive_bayes(x_train, y_train, n_iter_per_model=10, random_seed=42):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    alpha_values = [0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "    bernoulli_binarize_values = [0.0, 0.5]\n",
    "\n",
    "    gaussian_var_smoothing_values = [\n",
    "        0.000000001,\n",
    "        0.00000001,\n",
    "        0.0000001,\n",
    "        0.000001,\n",
    "        0.00001\n",
    "    ]\n",
    "\n",
    "    is_sparse = False\n",
    "    if sparse is not None:\n",
    "        is_sparse = sparse.issparse(x_train)\n",
    "\n",
    "    models_to_search = [\"bernoulli\", \"multinomial\"]\n",
    "    if not is_sparse:\n",
    "        models_to_search.append(\"gaussian\")\n",
    "\n",
    "    total_iterations = n_iter_per_model * len(models_to_search)\n",
    "\n",
    "    cv_splitter = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "\n",
    "    best_overall_score = -1.0\n",
    "    best_overall_model = None\n",
    "    best_overall_name = \"\"\n",
    "    best_overall_params = {}\n",
    "\n",
    "    bernoulli_best_score = -1.0\n",
    "    bernoulli_best_params = None\n",
    "\n",
    "    multinomial_best_score = -1.0\n",
    "    multinomial_best_params = None\n",
    "\n",
    "    gaussian_best_score = -1.0\n",
    "    gaussian_best_params = None\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"Searching Naive Bayes models\") as progress_bar:\n",
    "\n",
    "        # Bernoulli\n",
    "        if \"bernoulli\" in models_to_search:\n",
    "            for _ in range(n_iter_per_model):\n",
    "                alpha = random.choice(alpha_values)\n",
    "                binarize_value = random.choice(bernoulli_binarize_values)\n",
    "\n",
    "                model = BernoulliNB(alpha=alpha, binarize=binarize_value)\n",
    "\n",
    "                try:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                        scores = cross_val_score(\n",
    "                            model,\n",
    "                            x_train,\n",
    "                            y_train,\n",
    "                            cv=cv_splitter,\n",
    "                            scoring=\"f1_macro\"\n",
    "                        )\n",
    "                    mean_score = scores.mean()\n",
    "                except Exception:\n",
    "                    mean_score = -1.0\n",
    "\n",
    "                if mean_score > bernoulli_best_score:\n",
    "                    bernoulli_best_score = mean_score\n",
    "                    bernoulli_best_params = {\n",
    "                        \"alpha\": alpha,\n",
    "                        \"binarize\": binarize_value\n",
    "                    }\n",
    "\n",
    "                if mean_score > best_overall_score:\n",
    "                    best_overall_score = mean_score\n",
    "                    best_overall_model = BernoulliNB(alpha=alpha, binarize=binarize_value)\n",
    "                    best_overall_name = \"BernoulliNB\"\n",
    "                    best_overall_params = {\n",
    "                        \"alpha\": alpha,\n",
    "                        \"binarize\": binarize_value\n",
    "                    }\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        # Multinomial\n",
    "        if \"multinomial\" in models_to_search:\n",
    "            for _ in range(n_iter_per_model):\n",
    "                alpha = random.choice(alpha_values)\n",
    "\n",
    "                model = MultinomialNB(alpha=alpha)\n",
    "\n",
    "                try:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                        scores = cross_val_score(\n",
    "                            model,\n",
    "                            x_train,\n",
    "                            y_train,\n",
    "                            cv=cv_splitter,\n",
    "                            scoring=\"f1_macro\"\n",
    "                        )\n",
    "                    mean_score = scores.mean()\n",
    "                except Exception:\n",
    "                    mean_score = -1.0\n",
    "\n",
    "                if mean_score > multinomial_best_score:\n",
    "                    multinomial_best_score = mean_score\n",
    "                    multinomial_best_params = {\n",
    "                        \"alpha\": alpha\n",
    "                    }\n",
    "\n",
    "                if mean_score > best_overall_score:\n",
    "                    best_overall_score = mean_score\n",
    "                    best_overall_model = MultinomialNB(alpha=alpha)\n",
    "                    best_overall_name = \"MultinomialNB\"\n",
    "                    best_overall_params = {\n",
    "                        \"alpha\": alpha\n",
    "                    }\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        # Gaussian\n",
    "        if \"gaussian\" in models_to_search:\n",
    "            for _ in range(n_iter_per_model):\n",
    "                var_smoothing_value = random.choice(gaussian_var_smoothing_values)\n",
    "\n",
    "                model = GaussianNB(var_smoothing=var_smoothing_value)\n",
    "\n",
    "                try:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                        scores = cross_val_score(\n",
    "                            model,\n",
    "                            x_train,\n",
    "                            y_train,\n",
    "                            cv=cv_splitter,\n",
    "                            scoring=\"f1_macro\"\n",
    "                        )\n",
    "                    mean_score = scores.mean()\n",
    "                except Exception:\n",
    "                    mean_score = -1.0\n",
    "\n",
    "                if mean_score > gaussian_best_score:\n",
    "                    gaussian_best_score = mean_score\n",
    "                    gaussian_best_params = {\n",
    "                        \"var_smoothing\": var_smoothing_value\n",
    "                    }\n",
    "\n",
    "                if mean_score > best_overall_score:\n",
    "                    best_overall_score = mean_score\n",
    "                    best_overall_model = GaussianNB(var_smoothing=var_smoothing_value)\n",
    "                    best_overall_name = \"GaussianNB\"\n",
    "                    best_overall_params = {\n",
    "                        \"var_smoothing\": var_smoothing_value\n",
    "                    }\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "    if best_overall_model is not None:\n",
    "        best_overall_model.fit(x_train, y_train)\n",
    "\n",
    "    # Per-flavor summary\n",
    "    print(\"Best BernoulliNB F1 macro:\", None if bernoulli_best_score < 0 else round(bernoulli_best_score, 4))\n",
    "    print(\"Best BernoulliNB params:\", bernoulli_best_params)\n",
    "\n",
    "    print(\"Best MultinomialNB F1 macro:\", None if multinomial_best_score < 0 else round(multinomial_best_score, 4))\n",
    "    print(\"Best MultinomialNB params:\", multinomial_best_params)\n",
    "\n",
    "    if \"gaussian\" in models_to_search:\n",
    "        print(\"Best GaussianNB F1 macro:\", None if gaussian_best_score < 0 else round(gaussian_best_score, 4))\n",
    "        print(\"Best GaussianNB params:\", gaussian_best_params)\n",
    "    else:\n",
    "        print(\"GaussianNB was skipped because x_train is sparse.\")\n",
    "\n",
    "    # Overall best summary\n",
    "    print(\"Overall best model type:\", best_overall_name)\n",
    "    print(\"Overall best hyperparameters:\", best_overall_params)\n",
    "    print(\"Overall best mean F1 macro:\", round(best_overall_score, 4))\n",
    "\n",
    "    return best_overall_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6b03b",
   "metadata": {},
   "source": [
    "# Steam Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0186e931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching Naive Bayes models: 100%|| 60/60 [00:02<00:00, 27.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BernoulliNB F1 macro: 0.6534\n",
      "Best BernoulliNB params: {'alpha': 10.0, 'binarize': 0.0}\n",
      "Best MultinomialNB F1 macro: None\n",
      "Best MultinomialNB params: None\n",
      "Best GaussianNB F1 macro: 0.5615\n",
      "Best GaussianNB params: {'var_smoothing': 1e-07}\n",
      "Overall best model type: BernoulliNB\n",
      "Overall best hyperparameters: {'alpha': 10.0, 'binarize': 0.0}\n",
      "Overall best mean F1 macro: 0.6534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_steam = search_best_naive_bayes(X_train_steam, y_train_steam, n_iter_per_model=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef53fb",
   "metadata": {},
   "source": [
    "# Olist Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f66706d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching Naive Bayes models: 100%|| 60/60 [00:01<00:00, 52.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BernoulliNB F1 macro: 0.4003\n",
      "Best BernoulliNB params: {'alpha': 5.0, 'binarize': 0.0}\n",
      "Best MultinomialNB F1 macro: 0.5194\n",
      "Best MultinomialNB params: {'alpha': 0.5}\n",
      "Best GaussianNB F1 macro: 0.4827\n",
      "Best GaussianNB params: {'var_smoothing': 1e-09}\n",
      "Overall best model type: MultinomialNB\n",
      "Overall best hyperparameters: {'alpha': 0.5}\n",
      "Overall best mean F1 macro: 0.5194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_olist = search_best_naive_bayes(X_train_olist, y_train_olist, n_iter_per_model=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f308df8",
   "metadata": {},
   "source": [
    "# Video Game Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d621ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching Naive Bayes models: 100%|| 60/60 [00:01<00:00, 45.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BernoulliNB F1 macro: 0.5852\n",
      "Best BernoulliNB params: {'alpha': 0.001, 'binarize': 0.5}\n",
      "Best MultinomialNB F1 macro: 0.5573\n",
      "Best MultinomialNB params: {'alpha': 0.5}\n",
      "Best GaussianNB F1 macro: 0.6749\n",
      "Best GaussianNB params: {'var_smoothing': 1e-08}\n",
      "Overall best model type: GaussianNB\n",
      "Overall best hyperparameters: {'var_smoothing': 1e-08}\n",
      "Overall best mean F1 macro: 0.6749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_sales = search_best_naive_bayes(X_train_sales, y_train_sales, n_iter_per_model=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
